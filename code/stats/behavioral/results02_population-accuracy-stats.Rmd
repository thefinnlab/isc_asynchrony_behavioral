---
title: "results02_population-accuracy-stats"
author: "Tommy Botch"
date: "2024-11-15"
output: html_document
---

# Setup environment

## Load packages and setup paths for analysis

Set up directories - since we're doing this locally (and mounting the server), 
we're going to use a different base_dir

```{r setup, include=FALSE}

# Source the utils.R file to load the functions
source("../utils/utils.R")

# Load required and additional packages
install_and_load()

# Set the paths for this notebook
set_paths(analysis_name="behavioral", plots_name="behavioral")
```

# Population-level analysis 

## Data loading

Load data and set factors. Our columns are the following:
- **modality:** which modality was the stimulus presented as (video, audio, text) 
- **ground_truth:** ground-truth word to be predicted
- **top_pred:** most predicted word across the population
- **word_index:** the word index within the story
- **task:** which of the three stories was the data from (black, wheretheressmoke, howtodraw)

We create a contrast of modality such that we test the hypothesis that multimodal (video) > spoken (audio) > written (text).

```{r}

results_fn <- file.path(results_dir, "all-task_group-analyzed-behavior_human-lemmatized.csv")

# Load data and convert columns to factors
df_results <- load_and_prepare_data(
  file_path=results_fn,
  factor_columns=c('modality', 'ground_truth', 'top_pred', 'word_index','task')
)

# Set the levels for the modality column
df_results <- set_factor_levels(
  df=df_results,
  column_name="modality",
  levels=c("video", "audio", "text")
)

df_results <- create_column_contrast(
  df=df_results, 
  column_name="modality"
)

colnames(df_results)
```

## Analysis 1A - Population-level binary accuracy

We now aggregate predictions into a distribution across subjects, and find the most commonly predicted word for participants within each condition. We then predicted the binary accuracy (one-shot accuracy) based on the modality of presentation (condition) while controlling for random effects of trial (word_index) within each task (story).

Given that accuracy is a binary measure (0/1), we used a logistic mixed-effects model.

```{r}

binary_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula, 
               data = df_results, 
               family = binomial(link = "logit"))

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)
```

### Analysis 1B - Prosody interaction with binary accuracy

We can also look at whether the average prosody (prominence) leading up to a to-be-predicted word exhibits an interaction with modality. We therefore run an additional model investigating whether the two are related. 

```{r}

binary_accuracy_formula <- 'accuracy ~ modality * prominence_mean_words3 + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula, 
               data = df_results, 
               family = binomial(link = "logit"))

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

### Plot interaction model

```{r}

plot_model(model, type="pred", terms=c("prominence_mean_words3", "modality"))

```


## Analysis 1C - Task-level comparisons (binary accuracy)

```{r}

binary_accuracy_formula <- 'accuracy ~ modality + (1|word_index)'

```

#### Story = Black

```{r}

model <- lmer_task(
  task_name='black', 
  df=df_results, 
  formula=binary_accuracy_formula,
  family=binomial(link="logit")
)

# Output the results
tab_model(model, show.se = TRUE, show.stat = TRUE, show.r2 = TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

#### Story = Smoke

```{r}

model <- lmer_task(
  task_name='wheretheressmoke', 
  df=df_results, 
  formula=binary_accuracy_formula,
  family=binomial(link="logit")
)

# Output the results
tab_model(model, show.se = TRUE, show.stat = TRUE, show.r2 = TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

#### Story = Draw

```{r}

model <- lmer_task(
  task_name='howtodraw', 
  df=df_results, 
  formula=binary_accuracy_formula,
  family=binomial(link="logit")
)

# Output the results
tab_model(model, show.se = TRUE, show.stat = TRUE, show.r2 = TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

## Analysis 2A - Population-level continuous accuracy

We next evaluate the semantic similarity of human predictions to the ground-truth word. We use fasttext as a semantic embedding space and calculate the cosine similarity between each prediction and the ground-truth word. This turns accuracy into a continuous metric (now referred to as "continuous accuracy").

We use a linear mixed-effects model to predict continous accuracy based on the modality of presentation (condition) while controlling for random effects of trial (word_index) within each task (story).

```{r}

continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data=df_results)

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)
```

## Analysis 2B - Prosody interaction with continuous accuracy

We also look at how the average prosody leading up to a word informs the semantic similarity of the predictions.

```{r}

continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality * prominence_mean_words5 + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data=df_results)

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)
summary (model)

plot_model(model, type='pred', terms=c('prominence_mean_words5', 'modality'))
```


## Analysis 2C - Task-level comparisons (continuous accuracy)

We are also interested in understanding how these effects exist at the level of individual tasks. Since task is a random effect in previous models, we therefore run separate models for each tasks.

We first setup our formula to evaluate our model -- we discard task are random-effect as we are inherently testing at the task-level. 

```{r}

continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index)'

```

#### Story = Black

```{r}

model <- lmer_task(
  task_name='black', 
  df=df_results, 
  formula=continuous_accuracy_formula,
)

# Output the results
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

#### Story = Smoke

```{r}

model <- lmer_task(
  task_name='wheretheressmoke', 
  df=df_results, 
  formula=continuous_accuracy_formula,
)

# Output the results
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```

#### Story = Draw

```{r}

model <- lmer_task(
  task_name='howtodraw', 
  df=df_results, 
  formula=continuous_accuracy_formula,
)

# Output the results
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)

```
## Supplementary Analysis - Leakage filtering 

We next ensured that our results were not erroneously due to "accidental" leakage of auditory information (e.g., hearing the first phoneme of the upcoming to-be-predicted word).

We find the moments of potential leakage as where incorrect predictions were advantaged in both the multimodal (video) and spoken (audio) conditions. 

```{r}

df_results_filtered <- subset(df_results, !df_results$filter_for_leakage)

```

### Supplementary Analysis 1A - Binary accuracy

```{r}

binary_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula, 
               data = df_results_filtered, 
               family = binomial(link = "logit"))

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)
```

### Supplementary Analysis 1B - Binary accuracy prosody interaction

Rerun the logistic regression with the mean prosody included as an interaction effect. 

```{r}

binary_accuracy_formula <- 'accuracy ~ modality * prominence_mean_words5 + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula, 
               data = df_results_filtered, 
               family = binomial(link = "logit"))

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)
summary (model)
```

### Supplementary Analysis 2A - Continuous accuracy

```{r}

continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data = df_results_filtered)

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)
```

### Supplementary Analysis 2B - Continuous accuracy w/ prosody interaction

```{r}

continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality * prominence_mean_words5 + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data = df_results_filtered)

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)

# Display results of fixed effects
display_model_summary(model)

# Conduct pairwise comparisons between modalities
pairwise_comparisons <- emmeans(model, pairwise ~ modality)
display_pairwise_comparisons(pairwise_comparisons)
```

#### Plot the model

```{r}
plot_model(model, type='pred', terms=c('prominence_mean_words5', 'modality'))
```
