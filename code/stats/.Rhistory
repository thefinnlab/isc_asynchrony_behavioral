aes(x=log_model_predictability, y=log(human_predictability), color=modality),
stroke=1,
alpha=0.05) +
geom_line(size=1) +
geom_ribbon(aes(ymin=log(conf.low), ymax=log(conf.high), fill=group), alpha=0.3, color=NA) +
geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +
theme(
aspect.ratio = 4/5,
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
scale_color_manual(values=colors) +
scale_fill_manual(values=colors) +
labs(
x = "Model Predictability (Log-Scale)",
y = "Human Predictability",
title = "Relationship of predictability")
# ggsave('human-llm_predictability-relationship.pdf', device = "pdf")
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-model-distributions-lemmatized.csv')
df_results <- read.csv(results_fn)
# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
mutate(model_type = case_when(
model_name %in% c('roberta', 'electra', 'xlm-prophetnet') ~ "mlm",
TRUE ~ "clm"  # Default case
))
# df_results <- df_results[df_results$human_predictability > 0.02,]
df_results[df_results$human_predictability == 0,]$human_predictability <- 0.02
df_results[df_results$human_predictability == 1,]$human_predictability <- 0.99
# Set the variable types
factor_columns <- c('modality', 'model_name', 'model_type', 'word_index', 'task', "entropy_group", "accuracy_group", 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
# set order of variables
df_results$modality <- factor(df_results$modality, levels = c("audio", "text"))
df_results$model_type <- factor(df_results$model_type, levels = c("clm", "mlm"))
# df_results <- df_results[df_results$model_type == 'clm',]
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
df_results$log_model_predictability <- log(df_results$model_predictability)
df_results$log_human_predictability <- log(df_results$human_predictability)
#
df_cleaned <- remove_outliers(df_results, 'log_model_predictability')
# df_cleaned <- df_cleaned[df_cleaned$modality == 'text',]
#
model <- glmmTMB(
formula = human_predictability ~ modality * log_model_predictability + (1|word_index:task) + (1|model_name),
# ziformula = ~ modality * log_model_predictability + (1|word_index:task),
data = df_cleaned,
# family = gaussian()
family = gaussian(link='log')
)
AIC(model)
summary(model)
tab_model(model, show.df=TRUE, show.se=TRUE, show.stat=TRUE)
plot_model(model, type='pred', terms=c('log_model_predictability', 'modality'), show.data=TRUE)
preds <- data.frame(get_model_data(model, type='pred', terms = c("log_model_predictability", "modality")))
head(preds)
colors <- c('#82C564', '#F7CD84')
ggplot(preds, aes(x=x, y=log(predicted), color=group)) +
geom_point(
data = df_cleaned,
aes(x=log_model_predictability, y=log(human_predictability), color=modality),
stroke=1,
alpha=0.05) +
geom_line(size=1) +
geom_ribbon(aes(ymin=log(conf.low), ymax=log(conf.high), fill=group), alpha=0.3, color=NA) +
geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +
theme(
aspect.ratio = 4/5,
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
scale_color_manual(values=colors) +
scale_fill_manual(values=colors) +
labs(
x = "Model Predictability (Log-Scale)",
y = "Human Predictability",
title = "Relationship of predictability")
# ggsave('human-llm_predictability-relationship.pdf', device = "pdf")
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-model-distributions-lemmatized.csv')
df_results <- read.csv(results_fn)
# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
mutate(model_type = case_when(
model_name %in% c('roberta', 'electra', 'xlm-prophetnet') ~ "mlm",
TRUE ~ "clm"  # Default case
))
# df_results <- df_results[df_results$human_predictability > 0.02,]
df_results[df_results$human_predictability == 0,]$human_predictability <- 0.01
df_results[df_results$human_predictability == 1,]$human_predictability <- 0.99
# Set the variable types
factor_columns <- c('modality', 'model_name', 'model_type', 'word_index', 'task', "entropy_group", "accuracy_group", 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
# set order of variables
df_results$modality <- factor(df_results$modality, levels = c("audio", "text"))
df_results$model_type <- factor(df_results$model_type, levels = c("clm", "mlm"))
# df_results <- df_results[df_results$model_type == 'clm',]
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
df_results$log_model_predictability <- log(df_results$model_predictability)
df_results$log_human_predictability <- log(df_results$human_predictability)
#
df_cleaned <- remove_outliers(df_results, 'log_model_predictability')
# df_cleaned <- df_cleaned[df_cleaned$modality == 'text',]
#
model <- glmmTMB(
formula = human_predictability ~ modality * log_model_predictability + (1|word_index:task) + (1|model_name),
# ziformula = ~ modality * log_model_predictability + (1|word_index:task),
data = df_cleaned,
# family = gaussian()
family = gaussian(link='log')
)
AIC(model)
summary(model)
tab_model(model, show.df=TRUE, show.se=TRUE, show.stat=TRUE)
plot_model(model, type='pred', terms=c('log_model_predictability', 'modality'), show.data=TRUE)
preds <- data.frame(get_model_data(model, type='pred', terms = c("log_model_predictability", "modality")))
head(preds)
colors <- c('#82C564', '#F7CD84')
ggplot(preds, aes(x=x, y=log(predicted), color=group)) +
geom_point(
data = df_cleaned,
aes(x=log_model_predictability, y=log(human_predictability), color=modality),
stroke=1,
alpha=0.05) +
geom_line(size=1) +
geom_ribbon(aes(ymin=log(conf.low), ymax=log(conf.high), fill=group), alpha=0.3, color=NA) +
geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +
theme(
aspect.ratio = 4/5,
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
scale_color_manual(values=colors) +
scale_fill_manual(values=colors) +
labs(
x = "Model Predictability (Log-Scale)",
y = "Human Predictability",
title = "Relationship of predictability")
# ggsave('human-llm_predictability-relationship.pdf', device = "pdf")
# remove all variables
rm(list=ls())
# install.packages(c("lmerTest", "emmeans", "glue", "ggeffects", "ggplot2",
#                    "ggsignif", "ggpattern", "viridis", "forcats", "dplyr",
#                    "stringr", "tools", "lmer4", "combinat", "ggpubr", "report",
#                    "rcompanion", "sjPlot", "glmmTMB", "DHARMa", "tidyverse"))
library(lmerTest)
library(emmeans)
library(glue)
library(ggeffects)
library(ggplot2)
library(sjPlot)
library(report)
library(rcompanion)
library(sjPlot)
library(DHARMa)
library(glmmTMB)
library(tidyverse)
library(performance)
library(ordinal)
base_dir<-'/Volumes/FinnLab/tommy/isc_asynchrony_behavior/derivatives/'
results_dir<-glue('{base_dir}results/behavioral/')
plots_dir <- glue('{base_dir}plots/final/')
# Function to convert specified columns to factors
convert_columns_to_factors <- function(df, columns_to_convert) {
# Check if all specified columns exist in the dataframe
if (!all(columns_to_convert %in% names(df))) {
missing_cols <- columns_to_convert[!columns_to_convert %in% names(df)]
stop(paste("The following columns do not exist in the dataframe:",
paste(missing_cols, collapse = ", ")))
}
# Convert each specified column to factor
for (col in columns_to_convert) {
df[[col]] <- factor(df[[col]])
}
return(df)
}
# Function to remove outliers based on IQR of a specific column and print the number of removed values
remove_outliers <- function(df, column) {
if (!column %in% names(df)) {
stop("Column not found in dataframe.")
}
if (!is.numeric(df[[column]])) {
stop("Selected column is not numeric.")
}
initial_rows <- nrow(df)  # Store the initial number of rows
# Calculate IQR bounds for the specified column
Q1 <- quantile(df[[column]], 0.25)
Q3 <- quantile(df[[column]], 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Filter rows based on the specified column's bounds
df_clean <- df[df[[column]] >= lower_bound & df[[column]] <= upper_bound, ]
# Calculate and print number of rows removed
removed_rows <- initial_rows - nrow(df_clean)
cat("Number of rows removed due to outliers in column", column, ":", removed_rows, "\n")
return(df_clean)
}
# Load the results file
results_fn<-glue('{results_dir}all-task_subject-behavior_lemmatized.csv')
df_results <- read.csv(results_fn)
# Set the variable types
factor_columns <- c('modality', 'prolific_id', 'subject', 'word_index', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
df_results
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
subject_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task) + (1|subject:modality:task)'
# Fit the logistic mixed-effects model
model <- glmer(subject_accuracy_formula,
data = df_results,
family = binomial(link = "logit"))
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)
# remove all variables
rm(list=ls())
# install.packages(c("lmerTest", "emmeans", "glue", "ggeffects", "ggplot2",
#                    "ggsignif", "ggpattern", "viridis", "forcats", "dplyr",
#                    "stringr", "tools", "lmer4", "combinat", "ggpubr", "report",
#                    "rcompanion", "sjPlot", "glmmTMB", "DHARMa", "tidyverse"))
library(lmerTest)
library(emmeans)
library(glue)
library(ggeffects)
library(ggplot2)
library(sjPlot)
library(report)
library(rcompanion)
library(sjPlot)
library(DHARMa)
library(glmmTMB)
library(tidyverse)
library(performance)
library(ordinal)
base_dir<-'/Volumes/FinnLab/tommy/isc_asynchrony_behavior/derivatives/'
results_dir<-glue('{base_dir}results/behavioral/')
plots_dir <- glue('{base_dir}plots/final/')
# Function to convert specified columns to factors
convert_columns_to_factors <- function(df, columns_to_convert) {
# Check if all specified columns exist in the dataframe
if (!all(columns_to_convert %in% names(df))) {
missing_cols <- columns_to_convert[!columns_to_convert %in% names(df)]
stop(paste("The following columns do not exist in the dataframe:",
paste(missing_cols, collapse = ", ")))
}
# Convert each specified column to factor
for (col in columns_to_convert) {
df[[col]] <- factor(df[[col]])
}
return(df)
}
# Function to remove outliers based on IQR of a specific column and print the number of removed values
remove_outliers <- function(df, column) {
if (!column %in% names(df)) {
stop("Column not found in dataframe.")
}
if (!is.numeric(df[[column]])) {
stop("Selected column is not numeric.")
}
initial_rows <- nrow(df)  # Store the initial number of rows
# Calculate IQR bounds for the specified column
Q1 <- quantile(df[[column]], 0.25)
Q3 <- quantile(df[[column]], 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
# Filter rows based on the specified column's bounds
df_clean <- df[df[[column]] >= lower_bound & df[[column]] <= upper_bound, ]
# Calculate and print number of rows removed
removed_rows <- initial_rows - nrow(df_clean)
cat("Number of rows removed due to outliers in column", column, ":", removed_rows, "\n")
return(df_clean)
}
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized.csv')
df_results <- read.csv(results_fn)
# Set the variable types
factor_columns <- c('modality', 'ground_truth', 'word_index', 'top_pred', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
df_results
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized.csv')
df_results <- read.csv(results_fn)
# Set the variable types
factor_columns <- c('modality', 'ground_truth', 'word_index', 'top_pred', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
binary_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula,
data = df_results,
family = binomial(link = "logit"))
tab_model(model,show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)
report (model)
binary_accuracy_formula <- 'accuracy ~ modality * prominence_mean + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula,
data = df_results,
family = binomial(link = "logit"))
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)
plot_model(model, type="pred", terms=c("prominence_mean", "modality"))
continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data=df_results)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)
report(model)
continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality * prominence_mean + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- lmer(continuous_accuracy_formula, data=df_results)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)
summary (model)
plot_model(model, type='pred', terms=c('prominence_mean', 'modality'))
# First filter by pvalue less than 0.05 uncorrected --> test absence of effect
df_results_filtered <- subset(df_results, df_results$barnard_pvalue >= 0.05)
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results_filtered$modality) <- cbind(audio_greater_text)
binary_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- glmer(binary_accuracy_formula,
data = df_results_filtered,
family = binomial(link = "logit"))
# remove all variables
rm(list=ls())
# install.packages(c("lmerTest", "emmeans", "glue", "ggeffects", "ggplot2",
#                    "ggsignif", "ggpattern", "viridis", "forcats", "dplyr",
#                    "stringr", "tools", "lmer4", "combinat", "ggpubr", "report",
#                    "rcompanion", "sjPlot", "glmmTMB", "DHARMa", "tidyverse"))
library(lmerTest)
library(emmeans)
library(glue)
library(ggeffects)
library(ggplot2)
library(sjPlot)
library(report)
library(rcompanion)
library(sjPlot)
library(DHARMa)
library(glmmTMB)
library(tidyverse)
library(performance)
library(ordinal)
base_dir<-'/Volumes/FinnLab/tommy/isc_asynchrony_behavior/derivatives/'
results_dir<-glue('{base_dir}results/behavioral/')
plots_dir <- glue('{base_dir}plots/final/')
# Function to convert specified columns to factors
convert_columns_to_factors <- function(df, columns_to_convert) {
# Check if all specified columns exist in the dataframe
if (!all(columns_to_convert %in% names(df))) {
missing_cols <- columns_to_convert[!columns_to_convert %in% names(df)]
stop(paste("The following columns do not exist in the dataframe:",
paste(missing_cols, collapse = ", ")))
}
# Convert each specified column to factor
for (col in columns_to_convert) {
df[[col]] <- factor(df[[col]])
}
return(df)
}
# Function to extract p-values and test statistics
extract_lme_results <- function(task, df_results, formula, include_intercept = FALSE) {
# Subset data for the specific task
df_task <- df_results[df_results$task == task,]
# Fit the linear mixed-effects model
task_model <- lmer(formula, data = df_task, REML = TRUE)
# Extract summary information
model_summary <- summary(task_model)
# Create results dataframe
results_df <- data.frame(
task = task,
effect = rownames(model_summary$coefficients),
estimate = model_summary$coefficients[, "Estimate"],
std_error = model_summary$coefficients[, "Std. Error"],
t_value = model_summary$coefficients[, "t value"],
p_value = formatC(model_summary$coefficients[, "Pr(>|t|)"], format = "e", digits = 3)
)
# Optionally filter out intercept
if (!include_intercept) {
results_df <- results_df[results_df$effect != "(Intercept)", ]
}
return(results_df)
}
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized.csv')
df_results <- read.csv(results_fn)
# Set the variable types
factor_columns <- c('modality', 'ground_truth', 'word_index', 'top_pred', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)
df_results
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
entropy_formula <- 'entropy ~ modality + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- lmer(entropy_formula, data = df_results)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE,  show.df = TRUE)
report (model)
entropy_formula <- 'entropy ~ modality + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- lmer(entropy_formula, data = df_results)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE,  show.df = TRUE)
report (model)
# Function to extract p-values and test statistics
extract_lme_results <- function(task, df_results, formula, include_intercept = FALSE) {
# Subset data for the specific task
df_task <- df_results[df_results$task == task,]
# Fit the linear mixed-effects model
task_model <- lmer(formula, data = df_task, REML = TRUE)
# Extract summary information
model_summary <- summary(task_model)
# Create results dataframe
results_df <- data.frame(
task = task,
effect = rownames(model_summary$coefficients),
estimate = model_summary$coefficients[, "Estimate"],
std_error = model_summary$coefficients[, "Std. Error"],
t_value = model_summary$coefficients[, "t value"],
p_value = formatC(model_summary$coefficients[, "Pr(>|t|)"], format = "e", digits = 3)
)
# Optionally filter out intercept
if (!include_intercept) {
results_df <- results_df[results_df$effect != "(Intercept)", ]
}
return(results_df)
}
# Formula used in the original models
human_task_entropy_formula <- 'entropy ~ modality + (1|word_index)'
# List of tasks
tasks <- c('black', 'wheretheressmoke', 'howtodraw')
# Extract results for all tasks (excluding intercept by default)
results_list <- lapply(tasks, function(task) {
extract_lme_results(task, df_results, human_task_entropy_formula)
})
# Combine results
combined_results <- do.call(rbind, results_list)
# Apply multiple comparison corrections
# Convert back to numeric for p.adjust
original_p_values <- as.numeric(as.character(combined_results$p_value))
combined_results$bonferroni_p = formatC(p.adjust(original_p_values, method = "bonferroni"), format = "e", digits = 3)
combined_results$fdr_p = formatC(p.adjust(original_p_values, method = "fdr"), format = "e", digits = 3)
combined_results$holm_p = formatC(p.adjust(original_p_values, method = "holm"), format = "e", digits = 3)
# Print corrected results
print(combined_results)
# Optional: Identify significant effects after correction
significant_bonferroni <- combined_results[as.numeric(as.character(combined_results$bonferroni_p)) < 0.05, ]
significant_fdr <- combined_results[as.numeric(as.character(combined_results$fdr_p)) < 0.05, ]
significant_holm <- combined_results[as.numeric(as.character(combined_results$holm_p)) < 0.05, ]
print("Significant effects (Bonferroni):")
print(significant_bonferroni)
print("Significant effects (FDR):")
print(significant_fdr)
print("Significant effects (Holm):")
print(significant_holm)
# Print corrected results
print(combined_results)
# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)
entropy_formula <- 'entropy ~ modality * prominence_mean + (1|word_index:task)'
# Fit the logistic mixed-effects model
model <- lmer(entropy_formula, data = df_results)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE, show.df=TRUE)
report (model)
preds <- data.frame(get_model_data(model, type='pred', terms = c("prominence_mean", "modality")))
head(preds)
colors <- c('#82C564', '#F7CD84')
ggplot(preds, aes(x=x, y=predicted, color=group)) +
geom_point(
data = df_results,
aes(x=prominence_mean, y=entropy, color=modality),
stroke=1,
alpha=0.15) +
geom_line(size=1) +
geom_ribbon(aes(ymin=conf.low, ymax=conf.high, fill=group), alpha=0.3, color=NA) +
theme(
aspect.ratio = 6/7,
axis.line = element_line(colour = "black"),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
) +
scale_color_manual(values=colors) +
scale_fill_manual(values=colors) +
labs(
x = "Prosodic Prominence",
y = "Entropy",
title = "Prosody reduces entropy of prediction distributions")
ggsave('human_entropy-prosody-interaction.pdf', device = "pdf")
# Print corrected results
print(combined_results)
