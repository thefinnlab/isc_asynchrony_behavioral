---
title: "continuous_accuracy"
output: html_document
date: "2024-10-09"
---

```{r}
# remove all variables
rm(list=ls()) 

# install.packages(c("lmerTest", "emmeans", "glue", "ggeffects", "ggplot2", 
#                    "ggsignif", "ggpattern", "viridis", "forcats", "dplyr",
#                    "stringr", "tools", "lmer4", "combinat", "ggpubr", "report",
#                    "rcompanion", "sjPlot", "glmmTMB", "DHARMa", "tidyverse"))

library(lmerTest)
library(emmeans)
library(glue)
library(ggeffects)
library(ggplot2)
library(ggsignif)
library(ggpattern)
library(viridis)
library(forcats)
library(dplyr)
library(stringr)
library(tools)
library(combinat)
library(ggpubr)
library(report)
library(rcompanion)
library(sjPlot)

library(DHARMa)
library(glmmTMB)
library(tidyverse)
library(performance)
library(ordinal)
```
## Functions

```{r}
# Function to convert specified columns to factors
convert_columns_to_factors <- function(df, columns_to_convert) {
  # Check if all specified columns exist in the dataframe
  if (!all(columns_to_convert %in% names(df))) {
    missing_cols <- columns_to_convert[!columns_to_convert %in% names(df)]
    stop(paste("The following columns do not exist in the dataframe:", 
               paste(missing_cols, collapse = ", ")))
  }
  
  # Convert each specified column to factor
  for (col in columns_to_convert) {
    df[[col]] <- factor(df[[col]])
  }
  
  return(df)
}

```


## Set up directories

Set up directories - since we're doing this locally (and mounting the server), 
we're going to use a different base_dir

```{r}

base_dir<-'/Volumes/FinnLab/tommy/isc_asynchrony_behavior/derivatives/'
results_dir<-glue('{base_dir}results/behavioral/')
plots_dir <- glue('{base_dir}plots/final/')

```

## Function for task-wise analysis

```{r}

# Define a function to get pairwise modality contrasts for a single task
get_modality_contrast <- function(task_data, formula) {
  # Refit model for this task data
  task_model <- glmer(formula, data = task_data, family = binomial(link="logit"))
  return (task_model)
  # # Obtain emmeans and pairwise contrasts for modality
  # emm <- emmeans(task_model, ~ modality)
  # print (contrast(emm, method = "pairwise"))
}

```


## Analysis of individual subjects

```{r}

# Load the results file
results_fn<-glue('{results_dir}all-task_subject-behavior_lemmatized.csv')
df_results <- read.csv(results_fn)

# Set the variable types
factor_columns <- c('modality', 'prolific_id', 'subject', 'word_index', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)

df_results
```

### Calculate LME for subject accuracy

Include random effects of task and subject 

```{r}

# subject model for accuracy --> test if audio > text
audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

subject_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task) + (1|subject:modality:task)'

# Fit the logistic mixed-effects model
model <- glmer(subject_accuracy_formula, 
               data = df_results, 
               family = binomial(link = "logit"))

report (model)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)
```

### Calculate LME for each task

```{r}

subject_task_formula <- 'accuracy ~ modality + (1|word_index) + (1|subject:modality)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- glmer(subject_task_formula, data=df_task, family=binomial(link="logit"))
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}
```

## Load data compiled across all tasks

We first compiled data across all tasks

```{r}

# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized.csv')
df_results <- read.csv(results_fn)

# Set the variable types
factor_columns <- c('modality', 'ground_truth', 'word_index', 'top_pred', 'task')
df_results <- convert_columns_to_factors(df_results, factor_columns)

df_results
```

### Binary Accuracy -- Calculate for all tasks

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_accuracy_formula <- 'accuracy ~ modality + (1|word_index:task)'

# Fit the logistic mixed-effects model
model <- glmer(human_accuracy_formula, 
               data = df_results, 
               family = binomial(link = "logit"))

summary (model)

tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)
summary (model)

# check_model(model)
```

### Binary accuracy -- task-wise predictions

```{r}

human_task_accuracy_formula <- 'accuracy ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- glmer(human_task_accuracy_formula, data=df_task, family=binomial(link="logit"))
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}

```

### Continuous Accuracy

Predict continuous accuracy from modality controlling for word index

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_continuous_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index:task)'

# Fit the linear mixed-effects model
model <- lmer(human_continuous_accuracy_formula, data = df_results, REML=TRUE)

summary (model)
tab_model(model, show.se=TRUE, show.stat=TRUE, show.r2=TRUE)

human_task_accuracy_formula <- 'fasttext_top_word_accuracy ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- lmer(human_task_accuracy_formula, data=df_task, REML=TRUE)
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report(task_model))
}

```

### Prediction density??

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_prediction_density_formula <- 'fasttext_prediction_distances ~ modality + (1|word_index:task)'

# Fit the linear mixed-effects model
model <- lmer(human_prediction_density_formula, data = df_results, REML=TRUE)

report (model)

human_task_prediction_density_formula <- 'fasttext_prediction_distances ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- lmer(human_task_prediction_density_formula, data=df_task, REML=TRUE)
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}

```

### Distribution entropy

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_entropy_formula <- 'entropy ~ modality + (1|word_index:task)'

# Fit the linear mixed-effects model
model <- lmer(human_entropy_formula, data = df_results, REML=TRUE)

report (model)
summary (model)

human_task_entropy_formula <- 'entropy ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- lmer(human_task_entropy_formula, data=df_task, REML=TRUE)
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}
```

### Predictability 

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_predictability_formula <- 'predictability ~ modality + (1|word_index:task)'

# Fit the linear mixed-effects model
model <- lmer(human_predictability_formula, data = df_results, REML=TRUE)

report (model)
summary (model)

human_task_predictablity_formula <- 'predictability ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results$task)){
  df_task <- df_results[df_results$task == task,]
  
  task_model <- lmer(human_task_predictablity_formula, data=df_task, REML=TRUE)
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}

```

```{r}

# Function to remove outliers based on IQR of a specific column and print the number of removed values
remove_outliers <- function(df, column) {
  if (!column %in% names(df)) {
    stop("Column not found in dataframe.")
  }
  if (!is.numeric(df[[column]])) {
    stop("Selected column is not numeric.")
  }

  initial_rows <- nrow(df)  # Store the initial number of rows

  # Calculate IQR bounds for the specified column
  Q1 <- quantile(df[[column]], 0.25)
  Q3 <- quantile(df[[column]], 0.75)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 2 * IQR
  upper_bound <- Q3 + 2 * IQR

  # Filter rows based on the specified column's bounds
  df_clean <- df[df[[column]] >= lower_bound & df[[column]] <= upper_bound, ]
  
  # Calculate and print number of rows removed
  removed_rows <- initial_rows - nrow(df_clean)
  cat("Number of rows removed due to outliers in column", column, ":", removed_rows, "\n")
  
  return(df_clean)
}

```
### Reaction times 

```{r}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

human_rt_formula <- 'average_rt ~ modality + (1|word_index:task)'

df_results_no_outliers <- remove_outliers(df_results, 'average_rt')

# Fit the linear mixed-effects model
model <- glmer(human_rt_formula, data = df_results_no_outliers, family = Gamma(link = "log"))


report (model)
summary (model)

human_task_rt_formula <- 'average_rt ~ modality + (1|word_index)'

# Apply the function across tasks and combine results
for (task in unique(df_results_no_outliers$task)){
  df_task <- df_results_no_outliers[df_results_no_outliers$task == task,]
  
  task_model <- glmer(human_task_rt_formula, data=df_task, family=Gamma(link="log"))
  
  # model <- get_modality_contrast(df_task, subject_task_formula)
  print (task)
  print (report (task_model))
}

```


## Human LLM comparisons

```{r}
library(tidyverse)
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-model-lemmatized.csv')
df_results <- read.csv(results_fn)

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(modality_type = case_when(
    # modality %in% c("audio", "text") ~ "human",
    modality %in% c("audio") ~ "audio",
    modality %in% c("text") ~ "text",
    modality %in% c('roberta', 'electra', 'xlm-prophetnet') ~ "mlm",
    TRUE ~ "clm"  # Default case
        
  ))

df_results <- df_results %>%
  separate(entropy_accuracy_group, into = c("entropy_group", "accuracy_group"), sep = "-", remove = FALSE)


# Set the variable types
factor_columns <- c('modality', 'word_index', 'top_pred', 'task', 'entropy_accuracy_group', "entropy_group", "accuracy_group", 'modality_type', 'task')

df_results <- convert_columns_to_factors(df_results, factor_columns)
df_results$accuracy <- df_results$accuracy/100

# set order of variables
df_results$modality_type <- factor(df_results$modality_type, levels = c("audio", "text", "clm", "mlm"))

df_results$entropy_accuracy_group <- factor(df_results$entropy_accuracy_group, levels = c("low-high", "high-high", "low-low", "high-low"))

df_results$entropy_group <- factor(df_results$entropy_group, levels = c("high", "low"))
df_results$accuracy_group <- factor(df_results$accuracy_group, levels = c("high", "low"))

# To remove rows with NA values in that column:
df_results <- df_results[!is.na(df_results$entropy_accuracy_group), ]

unique(df_results$modality_type)
unique(df_results$entropy_accuracy_group)

colnames(df_results)

df_results

# Get unique combinations of modality, model_name, and task
unique_combinations <- df_results %>%
  distinct(modality, task)

# Initialize an empty dataframe to store the merged data
merged_df <- data.frame()

# Iterate through the unique combinations and merge the data
for (i in 1:nrow(unique_combinations)) {
  modality <- unique_combinations$modality[i]
  task <- unique_combinations$task[i]
  
  # Filter df_results to get the current row
  current_row <- df_results %>%
    filter(modality == !!modality, task == !!task)
  
  word_indices <- current_row$word_index
  
  # Filter df_prosody_results to get the matching prosody_mean
  prosody_mean <- df_prosody_results %>%
    filter(modality == 'audio', task == !!task, word_index %in% word_indices) %>%
    pull(prosody_mean)
  
  # Add the prosody_mean to the current_row and append to the merged_df
  current_row$prosody_mean <- prosody_mean
  merged_df <- rbind(merged_df, current_row)
}

df_results <- merged_df
```
### Binary accuracy -- human v. models

```{r}
library(sjPlot)

# contrasts(df_results$modality_type) <- contr.poly(length(unique(df_results$modality_type)))

human_v_model_formula <- 'accuracy ~ modality_type + (1|modality) + (1|word_index:task)'

model <- glmer(human_v_model_formula, data=df_results, family=binomial(link="logit"))
tab_model(model)
# check_model(model)

# plot_model(model, type="pred", terms = c("prosody_mean", "modality_type"))
```

### Test effect of continuous accuracy

```{r}

contrasts(df_results$modality_type) <- contr.poly(length(unique(df_results$modality_type)))

human_v_model_formula <- 'fasttext_avg_accuracy ~ modality_type + (1|modality) + (1|word_index:task)'

model <- lmer(human_v_model_formula, data=df_results, REML=TRUE)

tab_model(model)

# plot_model(model, type="pred", terms = c("prosody_mean", "modality_type"))


```

### Test effect of word quadrant

```{r}

library(sjPlot)
library(sjmisc)
library(ggplot2)

# set order of variables
human_greater_model <- contr.poly(length(unique(df_results$modality_type)))
# entropy_accuracy_order <- contr.poly(length(unique(df_results$entropy_accuracy_group)))
# entropy_high_v_low <- c(1, -1)
# contrasts(df_results$entropy) <- cbind(entropy_high_v_low)
  

# contrasts(df_results$modality_type) <- cbind(human_greater_model)
# contrasts(df_results$entropy_accuracy_group) <- cbind(entropy_accuracy_order)

human_v_model_formula <- 'fasttext_avg_accuracy ~ modality_type * entropy_group + (1|modality) + (1|word_index:task)'

model <- lmer(human_v_model_formula, data=df_results, REML=TRUE)

report(model)

# emmeans(model, pairwise ~ modality_type * entropy)

```

```{r}

tab_model(model)
plot_model(model, type = "int") +
  labs(x="Modality", y="Cosine similarity") +
  ggtitle('Effect of model entropy on accuracy') 
  # scale_color_discrete(
  #   labels = c("Low entropy, high accuracy", 
  #              "High entropy, high accuracy", 
  #              "Low entropy, low accuracy", 
  #              "High entropy, low accuracy")  # Custom legend labels)
  # ) +
  # labs(fill = "Grouped entropy and accuracy")  # Custom legend title

```

### Test creating contrasts of accuracy --> do this 

```{r}
# Function to create contrasts between baseline modalities and models, including entropy
create_accuracy_contrasts <- function(df) {
  # Get unique models (excluding audio and text)
  models <- setdiff(unique(df$modality), c("audio", "text"))
  
  # Initialize lists to store results
  contrasts <- list()
  
  # Create contrasts for both audio and text
  for (baseline in c("audio", "text")) {
    baseline_acc <- df$fasttext_avg_accuracy[df$modality == baseline]
    
    for (model in models) {
      
      model_acc <- df$fasttext_avg_accuracy[df$modality == model]
      model_entropy <- df$entropy[df$modality == model]
      word_index <- df$word_index[df$modality == model]
      task <- df$task[df$modality == model]
      
      
      contrasts[[length(contrasts) + 1]] <- data.frame(
        baseline_modality = baseline,
        model = model,
        accuracy_difference = baseline_acc[1:length(model_acc)] - model_acc,
        model_entropy = model_entropy,
        word_index = word_index,
        task = task
      )
    }
  }
  
  # Combine all contrasts into a single dataframe
  result <- do.call(rbind, contrasts)
  
  # Reset row names
  rownames(result) <- NULL
  
  # Order by baseline_modality and absolute accuracy difference
  result <- result[order(result$baseline_modality,
                        result$task,
                        result$word_index,
                        -abs(result$accuracy_difference)), ]
  return(result)
}

df_modality_model_contrasts <- create_accuracy_contrasts(df_results)

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_modality_model_contrasts <- df_modality_model_contrasts %>%
  mutate(model_type = case_when(
    # modality %in% c("audio", "text") ~ "human",
    model %in% c('roberta', 'electra', 'xlm-prophetnet') ~ "mlm",
    TRUE ~ "clm"  # Default case

  ))

df_modality_model_contrasts <- convert_columns_to_factors(df_modality_model_contrasts, c('baseline_modality', "model", "word_index", "task", "model_type"))

audio_greater_text <- c(1, -1)
contrasts(df_modality_model_contrasts$baseline_modality) <- cbind(audio_greater_text)

human_v_model_formula <- 'accuracy_difference ~ model_type * baseline_modality * model_entropy + (1|model) + (1|word_index:task)'

model <- lmer(human_v_model_formula, data=df_modality_model_contrasts, REML=TRUE)
# 
report(model)
tab_model(model)

#
# Create plot using plot_model
plot_model(model,
          type = "pred",
          terms = c("model_entropy", "baseline_modality"),
          show.data = FALSE,
          jitter = 0.1,
          alpha = 0.3,  # transparency of points
          title = "Predicted Accuracy Difference by Entropy and Modality",
          axis.title = c("Model Entropy", "Accuracy Difference"),
          legend.title = "Baseline Modality")
```

## Human LLM distribution comparisons


```{r}

# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-model-distributions-lemmatized.csv')
df_results <- read.csv(results_fn)

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(model_type = case_when(
    # modality %in% c("audio", "text") ~ "human",
    model_name %in% c('roberta', 'electra', 'xlm-prophetnet') ~ "mlm",
    TRUE ~ "clm"  # Default case

  ))

# Set the variable types
factor_columns <- c('modality', 'word_index', 'accuracy_group', 'entropy_group', 'task', 'model_type', 'model_name')

df_results <- convert_columns_to_factors(df_results, factor_columns)

prosody_results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized_prosody.csv')
df_prosody_results <- read.csv(prosody_results_fn)
df_prosody_results <- df_prosody_results[df_prosody_results$word_index %in% df_results$word_index, ]

# Get unique combinations of modality, model_name, and task
unique_combinations <- df_results %>%
  distinct(modality, model_name, task)

# Initialize an empty dataframe to store the merged data
merged_df <- data.frame()

# Iterate through the unique combinations and merge the data
for (i in 1:nrow(unique_combinations)) {
  modality <- unique_combinations$modality[i]
  model_name <- unique_combinations$model_name[i]
  task <- unique_combinations$task[i]
  
  # Filter df_results to get the current row
  current_row <- df_results %>%
    filter(modality == !!modality, model_name == !!model_name, task == !!task)
  
  # Filter df_prosody_results to get the matching prosody_mean
  prosody_mean <- df_prosody_results %>%
    filter(modality == !!modality, task == !!task) %>%
    pull(prosody_mean)
  
  # Add the prosody_mean to the current_row and append to the merged_df
  current_row$prosody_mean <- prosody_mean
  merged_df <- rbind(merged_df, current_row)
}

df_results <- merged_df
```

### Predictability comparison
```{r, fig.width=5, fig.height=4}


replace_infinite <- function(data) {
  # Check if input is a data frame
  if (!is.data.frame(data)) {
    stop("Input must be a data frame")
  }
  
  # Process each numeric column
  for (col in names(data)) {
    if (is.numeric(data[[col]])) {
      # Get finite values only for calculating true maximum
      finite_values <- data[[col]][is.finite(data[[col]])]
      
      if (length(finite_values) > 0) {
        # Calculate maximum of finite values
        col_max <- max(finite_values)
        
        # Replace Inf and -Inf with column maximum
        data[[col]][is.infinite(data[[col]])] <- col_max
      }
    }
  }
  
  return(data)
}


audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

df_clean <- replace_infinite(df_results)
df_clean <- df_clean[df_clean$log_odds_predictability_human != 0,]
# df_clean <- df_clean[df_clean$predictability_human != 0, ]

human_model_predictability_formula <- 'log_odds_predictability_model ~ modality * prosody_mean * log_odds_predictability_human + (1|word_index:task) + (1|model_name)'

model <- lmer(human_model_predictability_formula, data = df_clean) #, family=Gamma(link='log'))
tab_model(model)

check_model(model)

plot_model(model, type='pred', terms=c( "log_odds_predictability_human", "modality"), show.data = FALSE)


```

### KL Divergence test

```{r, fig.width=5, fig.height=4}

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

kldiv_formula <- 'kl_divergence ~ modality + (1|word_index:task) + (1|model_name)'

# # Fit the linear mixed-effects model
model <- lmer(kldiv_formula, data = df_results, REML=TRUE,
              control = lmerControl(optimizer ="Nelder_Mead"))

# report (model)
tab_model(model)
summary (model)

plot_model(model, type="pred", terms=c("prosody_mean", "modality"))

check_model(model)
# 
# model_kldiv_formula <- 'kl_divergence ~ modality + (1|word_index:task)'
# 
# # Apply the function across tasks and combine results
# for (model_name in unique(df_results$model_name)){
#   df_model <- df_results[df_results$model_name == model_name,]
# 
#   model <- lmer(model_kldiv_formula, data=df_model, REML=TRUE)
#   print (model_name)
#   print (report (model))
# }

```


## Prosody analyses


```{r}

# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized_prosody.csv')
df_results <- read.csv(results_fn)

# Set the variable types
factor_columns <- c('modality', 'word_index', 'task')

df_results <- convert_columns_to_factors(df_results, factor_columns)
df_results <- na.omit(df_results)

df_results[df_results$prosody_mean < 0,]$prosody_mean = 0
```

### Evaluate effect of prosody on accuracy

```{r}
#, fig.width=5, fig.height=4}
# check_model(model)

library(DHARMa)

audio_greater_text <- c(1,-1)
contrasts(df_results$modality) <- cbind(audio_greater_text)

entropy_formula <- 'entropy ~ modality * prosody_mean + (1|word_index:task)'

# Apply the function across tasks and combine results
model <- lmer(entropy_formula, data=df_results)

# model <- glmer(subject_task_formula, data=df_results)

summary(model)
tab_model(model)

check_model(model)

simulationOutput <- simulateResiduals(fittedModel = model, plot = F)
plot(simulationOutput)

g <- plot_model(model, type="pred", terms=c("task", "modality"), show.data = TRUE)
print (g)
```


### Test reduction in KL w/ prosody models

```{r}

# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-prosody-model-distributions-lemmatized.csv')

results_fn
df_results <- read.csv(results_fn)

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(model_type = case_when(
    grepl("joint-loss_prosody-embed", model_name) ~ "joint-prosody",
    grepl("clm-loss_prosody-embed", model_name) ~ "clm-prosody",
    TRUE ~ "clm"  # Default case

  ))

# df_results <- df_results[grepl("gigaspeech", df_results$model_name),]

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(dataset = case_when(
    grepl("helsinki", model_name) ~ "helsinki",
    TRUE ~ "gigaspeech"  # Default case

  ))

# set order of variables
df_results$model_type <- factor(df_results$model_type, levels = c("clm", "joint-prosody", "clm-prosody"))

# Set the variable types
factor_columns <- c('modality', 'word_index', 'accuracy_group', 'entropy_group', 'task', 'model_type', 'model_name')

df_results <- convert_columns_to_factors(df_results, factor_columns)

prosody_results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized_prosody.csv')
df_prosody_results <- read.csv(prosody_results_fn)
df_prosody_results <- df_prosody_results[df_prosody_results$word_index %in% df_results$word_index, ]

# Get unique combinations of modality, model_name, and task
unique_combinations <- df_results %>%
  distinct(modality, model_name, task)

# Initialize an empty dataframe to store the merged data
merged_df <- data.frame()

# Iterate through the unique combinations and merge the data
for (i in 1:nrow(unique_combinations)) {
  modality <- unique_combinations$modality[i]
  model_name <- unique_combinations$model_name[i]
  task <- unique_combinations$task[i]
  
  # Filter df_results to get the current row
  current_row <- df_results %>%
    filter(modality == !!modality, model_name == !!model_name, task == !!task)
  
  # Filter df_prosody_results to get the matching prosody_mean
  prosody_mean <- df_prosody_results %>%
    filter(modality == !!modality, task == !!task) %>%
    pull(prosody_mean)
  
  # Add the prosody_mean to the current_row and append to the merged_df
  current_row$prosody_mean <- prosody_mean
  merged_df <- rbind(merged_df, current_row)
}

```

### Prosody KL Divergence

```{r}

audio_greater_text <- c(1,-1)
contrasts(merged_df$modality) <- cbind(audio_greater_text)

clm_joint_prosody <- contr.poly(length(unique(merged_df$model_type)))
contrasts(merged_df$model_type) <- cbind(clm_joint_prosody)


kldiv_formula <- 'kl_divergence ~ modality * model_type * prosody_mean + (1|dataset) + (1|word_index:task)'

# # Fit the linear mixed-effects model
model <- glmer(kldiv_formula, data = merged_df) #, REML=FALSE)

# report (model)
tab_model(model)
# 
# check_collinearity(model)

plot_model(model, type='pred', terms=c("prosody_mean", "modality", "model_type"))

```

### Test accuracy as a function of prosody in prosody models

```{r}
# Load the results file
results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_window-size-25_human-prosody-model-lemmatized.csv')

df_results <- read.csv(results_fn)

df_results <- df_results[!(df_results$modality %in% c('audio', 'text')),]

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(model_type = case_when(
    grepl("joint-loss_prosody-embed", modality) ~ "joint-prosody",
    grepl("clm-loss_prosody-embed", modality) ~ "clm-prosody",
    TRUE ~ "clm"  # Default case

  ))

# df_results <- df_results[grepl("helsinki", df_results$modality),]

# ADD A COLUMN THAT GROUPS HUMANS/MODELS
df_results <- df_results %>%
  mutate(dataset = case_when(
    grepl("helsinki", modality) ~ "helsinki",
    grepl("gigaspeech", modality) ~ "gigaspeech",
  ))

# set order of variables
df_results$model_type <- factor(df_results$model_type, levels = c("joint-prosody", "clm-prosody", "clm"))

# Set the variable types
factor_columns <- c('modality', 'word_index', 'task', 'model_type', 'dataset')

df_results <- convert_columns_to_factors(df_results, factor_columns)

prosody_results_fn<-glue('{results_dir}all-task_group-analyzed-behavior_human-lemmatized_prosody.csv')
df_prosody_results <- read.csv(prosody_results_fn)
df_prosody_results <- df_prosody_results[df_prosody_results$word_index %in% df_results$word_index, ]

# Get unique combinations of modality, model_name, and task
unique_combinations <- df_results %>%
  distinct(modality, task)

# Initialize an empty dataframe to store the merged data
merged_df <- data.frame()

# Iterate through the unique combinations and merge the data
for (i in 1:nrow(unique_combinations)) {
  modality <- unique_combinations$modality[i]
  task <- unique_combinations$task[i]
  
  # Filter df_results to get the current row
  current_row <- df_results %>%
    filter(modality == !!modality, task == !!task)
  
  # Filter df_prosody_results to get the matching prosody_mean
  prosody_mean <- df_prosody_results %>%
    filter(modality == 'audio', task == !!task) %>%
    pull(prosody_mean)
  
  # Add the prosody_mean to the current_row and append to the merged_df
  current_row$prosody_mean <- prosody_mean
  merged_df <- rbind(merged_df, current_row)
}

df_results <- merged_df
df_results$accuracy <- df_results$accuracy / 100
```

### Test effect of model training on accuracy

```{r}

clm_joint_prosody <- contr.poly(length(unique(merged_df$model_type)))
contrasts(merged_df$model_type) <- cbind(clm_joint_prosody)


accuracy_formula <- 'entropy ~ model_type * prosody_mean + (1|word_index:task)'

# # Fit the linear mixed-effects model
model <- glmer(accuracy_formula, data = df_results) #, REML=FALSE)

# report (model)
tab_model(model)
# 
# check_collinearity(model)

plot_model(model, type='pred', terms=c("prosody_mean", "model_type"))

```
