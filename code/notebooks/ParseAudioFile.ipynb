{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d079eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7967b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'pilot-version-04'\n",
    "TASK = 'black'\n",
    "\n",
    "percent_sampled = 0.25 # number of items to sample for each subject\n",
    "n_counts_per_item = 25 # number of times items are seen across subjects\n",
    "\n",
    "# set directories\n",
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/'\n",
    "preproc_dir = os.path.join(base_dir, 'stimuli', 'preprocessed')\n",
    "task_out_dir = os.path.join(base_dir, 'stimuli', 'presentation_orders', EXPERIMENT_NAME, TASK)\n",
    "\n",
    "if not os.path.exists(task_out_dir):\n",
    "    os.makedirs(task_out_dir)\n",
    "\n",
    "# load preprocessed transcript\n",
    "df_task_preproc_fn = os.path.join(preproc_dir, TASK, f'{TASK}_transcript_preprocessed')\n",
    "df_preproc = pd.read_csv(f'{df_task_preproc_fn}.csv')\n",
    "\n",
    "# find indices for presentation and set number of items each subject sees\n",
    "nwp_indices = np.where(df_preproc['NWP_Candidate'])[0]\n",
    "n_items_per_subject = round(len(nwp_indices) * percent_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eaf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_times(df, start_idx, end_idx):\n",
    "    \n",
    "    onset = df.iloc[start_idx]['Onset']\n",
    "    offset = df.iloc[end_idx]['Onset']\n",
    "    \n",
    "    duration = offset - onset\n",
    "    \n",
    "    return onset, offset, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318c1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim = AudioSegment.from_file(stim_fn)\n",
    "stim_length = stim.duration_seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451052d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'black'\n",
    "\n",
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/'\n",
    "stim_dir = os.path.join(base_dir, 'stimuli')\n",
    "out_dir = os.path.join(stim_dir, 'cut_audio', task)\n",
    "\n",
    "stim_fn = os.path.join(stim_dir, 'audio', f'{task}_audio.wav')\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90cdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, curr_idx in enumerate(nwp_indices):\n",
    "    # if we're on the first index we use the start of the file\n",
    "    if i == 0:\n",
    "        _, offset, _ = get_cut_times(df_preproc, 0, curr_idx)\n",
    "        onset = 0\n",
    "        duration = offset\n",
    "    elif i == len(nwp_indices):\n",
    "        onset, _, _ = get_cut_times(df_preproc, curr_idx, curr_idx)\n",
    "        duration = stim_length - onset\n",
    "    else:\n",
    "        prev_idx = nwp_indices[i-1]\n",
    "        onset, _, duration = get_cut_times(df_preproc, prev_idx, curr_idx)\n",
    "    \n",
    "    out_fn = os.path.join(out_dir, f'{task}_segment-{str(i+1).zfill(5)}.wav')\n",
    "    cmd = f'ffmpeg -y -ss {onset} -t {duration} -i {stim_fn} {out_fn}'\n",
    "    subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc26007",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f'ffmpeg -ss 792.350000 -t 2 -i {stim_fn} end.wav'\n",
    "\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27e7eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abefde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc.iloc[nwp_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30262897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2bf32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f'ffmpeg -ss 10 -t 10 -i {stim_fn} test.mp4'\n",
    "\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e592559",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioSegment.from_wav(stim_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7e186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydub does things in milliseconds\n",
    "ten_seconds = 10 * 1000\n",
    "twenty_seconds = 20 * 1000\n",
    "\n",
    "cut = audio[ten_seconds:twenty_seconds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab736c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut.export(\"test.mp4\", format=\"mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49d75b",
   "metadata": {},
   "source": [
    "# Test consecutive constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86551c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import argparse\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from config import *\n",
    "from randomization_utils import create_balanced_orders, get_consecutive_list_idxs, sort_consecutive_constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74010fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-n', '--experiment_name', type=str)\n",
    "parser.add_argument('-t', '--task', type=str)\n",
    "parser.add_argument('-p', '--percent_sampled', type=float, default=0.25) # percentage of items to sample for each subject\n",
    "parser.add_argument('-c', '--n_participants_per_item', type=int, default=25) # number of times items are seen across subjects\n",
    "parser.add_argument('-i', '--consecutive_spacing', type=int, default=2) # number of times items are seen across subjects\n",
    "\n",
    "p = parser.parse_args([f'-ntest', f'-tblack'])\n",
    "                   \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b650d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directories\n",
    "preproc_dir = os.path.join(BASE_DIR, 'stimuli', 'preprocessed')\n",
    "task_out_dir = os.path.join(BASE_DIR, 'stimuli', 'presentation_orders', p.experiment_name, p.task, 'preproc')\n",
    "\n",
    "if not os.path.exists(task_out_dir):\n",
    "    os.makedirs(task_out_dir)\n",
    "\n",
    "# load preprocessed transcript\n",
    "df_task_preproc_fn = os.path.join(preproc_dir, p.task, f'{p.task}_transcript-preprocessed')\n",
    "df_preproc = pd.read_csv(f'{df_task_preproc_fn}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1b9f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating orders for 100 subjects\n"
     ]
    }
   ],
   "source": [
    "# find indices for presentation and set number of items each subject sees\n",
    "nwp_indices = np.where(df_preproc['NWP_Candidate'])[0]\n",
    "n_items_per_subject = round(len(nwp_indices) * p.percent_sampled)\n",
    "\n",
    "# create experiment structure for subjects --> sort the indices\n",
    "subject_experiment_orders = create_balanced_orders(items=nwp_indices, n_elements_per_subject=n_items_per_subject, use_each_times=p.n_participants_per_item)\n",
    "subject_experiment_orders = list(map(sorted, subject_experiment_orders))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594c60d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from preproc_utils import sort_consecutive_constraint, check_consecutive_spacing\n",
    "\n",
    "orders = sort_consecutive_constraint(subject_experiment_orders, consecutive_spacing=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b30b5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /dartfs-\n",
      "[nltk_data]     hpc/rc/home/w/f003rjw/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from praatio import textgrid as tgio\n",
    "\n",
    "# sys.path.append('../utils/')\n",
    "\n",
    "from config import *\n",
    "from preproc_utils import update_dataframe_from_praat, dataframe_to_textgrid, get_cut_times, cut_audio_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "688ace29",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'black'\n",
    "\n",
    "# set directories\n",
    "stim_dir = os.path.join(BASE_DIR, 'stimuli')\n",
    "preproc_dir = os.path.join(stim_dir, 'preprocessed')\n",
    "\n",
    "# load preprocessed transcript and find indices that are to be predicted\n",
    "df_preproc_fn = os.path.join(preproc_dir, task, f'{task}_transcript-preprocessed.csv')\n",
    "df_preproc = pd.read_csv(df_preproc_fn)\n",
    "\n",
    "## Segments are defined as follows\n",
    "##  - Start = where a previous segment left of --> will contain the prior segment's predicted word\n",
    "##  - Stop = ending right before a word prediction\n",
    "## Therefore adjusting the end of one will cause a shift in the subsequent segment time\n",
    "\n",
    "# create dataframe that accompanies written audio segments\n",
    "# get segment file if it exists\n",
    "df_segments_fn = os.path.join(preproc_dir, task, f'{task}_transcript-segments.csv')\n",
    "praat_fn = os.path.join(preproc_dir, task, f'{task}_transcript-praat.TextGrid')\n",
    "\n",
    "# if a textgrid file exists, we open it and use it in to adjust the times\n",
    "if os.path.exists(praat_fn):\n",
    "    tg = tgio.openTextgrid(praat_fn, False)\n",
    "#     df_preproc = update_dataframe_from_praat(df_preproc, tg)\n",
    "# else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3713a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_preproc.copy()\n",
    "\n",
    "for idx in range(len(df)):\n",
    "\n",
    "    word = tg.getTier('word').entries[idx]\n",
    "\n",
    "    df.loc[idx, 'Onset'] = word.start\n",
    "    df.loc[idx, 'Offset'] = word.end\n",
    "    df.loc[idx, 'Duration'] = word.end - word.start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af7274dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Written</th>\n",
       "      <th>Case</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop_Word</th>\n",
       "      <th>Word_Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>NWP_Candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>So</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.260000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>1.960000</td>\n",
       "      <td>2.273722</td>\n",
       "      <td>0.313722</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>2.273722</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>0.176278</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>junior</td>\n",
       "      <td>success</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective or numeral, ordinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>junior</td>\n",
       "      <td>2.460000</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>success</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>in</td>\n",
       "      <td>3.140000</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>college</td>\n",
       "      <td>3.410000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when</td>\n",
       "      <td>success</td>\n",
       "      <td>WRB</td>\n",
       "      <td>Wh-adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>when</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>got</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>got</td>\n",
       "      <td>5.090000</td>\n",
       "      <td>5.330000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>my</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>my</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>5.553515</td>\n",
       "      <td>0.203515</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>first</td>\n",
       "      <td>success</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective or numeral, ordinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>first</td>\n",
       "      <td>5.553585</td>\n",
       "      <td>6.340000</td>\n",
       "      <td>0.786415</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paying</td>\n",
       "      <td>success</td>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, present participle or gerund</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>paying</td>\n",
       "      <td>7.110000</td>\n",
       "      <td>7.660000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>job</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>job</td>\n",
       "      <td>7.660000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>in</td>\n",
       "      <td>success</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>in</td>\n",
       "      <td>8.650000</td>\n",
       "      <td>8.930000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>my</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>my</td>\n",
       "      <td>8.930000</td>\n",
       "      <td>9.170917</td>\n",
       "      <td>0.240917</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>field</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>field</td>\n",
       "      <td>9.176691</td>\n",
       "      <td>9.990000</td>\n",
       "      <td>0.813309</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>on</td>\n",
       "      <td>success</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>on</td>\n",
       "      <td>10.290000</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>10.500000</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>radio</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>radio</td>\n",
       "      <td>10.620000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>This</td>\n",
       "      <td>11.820000</td>\n",
       "      <td>11.970000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>is</td>\n",
       "      <td>success</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>verb, present tense, 3rd person singular</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>is</td>\n",
       "      <td>11.970000</td>\n",
       "      <td>12.090000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>not</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>not</td>\n",
       "      <td>12.090000</td>\n",
       "      <td>12.277681</td>\n",
       "      <td>0.187681</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>an</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>an</td>\n",
       "      <td>12.277681</td>\n",
       "      <td>12.386365</td>\n",
       "      <td>0.108684</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>internship</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>internship</td>\n",
       "      <td>12.386365</td>\n",
       "      <td>13.390000</td>\n",
       "      <td>1.003635</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I'm</td>\n",
       "      <td>success</td>\n",
       "      <td>NNP</td>\n",
       "      <td>noun, proper, singular</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I'm</td>\n",
       "      <td>13.830000</td>\n",
       "      <td>14.040000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word_Written     Case   POS                             POS_Definition  \\\n",
       "0            So  success    RB                                     adverb   \n",
       "1             I  success   PRP                          pronoun, personal   \n",
       "2           was  success   VBD                           verb, past tense   \n",
       "3             a  success    DT                                 determiner   \n",
       "4        junior  success    JJ              adjective or numeral, ordinal   \n",
       "5            in  success    IN  preposition or conjunction, subordinating   \n",
       "6       college  success    NN             noun, common, singular or mass   \n",
       "7          when  success   WRB                                  Wh-adverb   \n",
       "8             I  success   PRP                          pronoun, personal   \n",
       "9           got  success   VBD                           verb, past tense   \n",
       "10           my  success  PRP$                        pronoun, possessive   \n",
       "11        first  success    JJ              adjective or numeral, ordinal   \n",
       "12       paying  success   VBG         verb, present participle or gerund   \n",
       "13          job  success    NN             noun, common, singular or mass   \n",
       "14           in  success    IN  preposition or conjunction, subordinating   \n",
       "15           my  success  PRP$                        pronoun, possessive   \n",
       "16        field  success    NN             noun, common, singular or mass   \n",
       "17           on  success    IN  preposition or conjunction, subordinating   \n",
       "18          the  success    DT                                 determiner   \n",
       "19        radio  success    NN             noun, common, singular or mass   \n",
       "20         This  success    DT                                 determiner   \n",
       "21           is  success   VBZ   verb, present tense, 3rd person singular   \n",
       "22          not  success    RB                                     adverb   \n",
       "23           an  success    DT                                 determiner   \n",
       "24   internship  success    NN             noun, common, singular or mass   \n",
       "25          I'm  success   NNP                     noun, proper, singular   \n",
       "\n",
       "   Punctuation  Stop_Word  Word_Vocab      Onset     Offset  Duration  \\\n",
       "0                    True          So   0.240000   0.630000  0.390000   \n",
       "1                    True           I   0.680000   1.260000  0.580000   \n",
       "2                    True         was   1.960000   2.273722  0.313722   \n",
       "3                    True           a   2.273722   2.450000  0.176278   \n",
       "4                   False      junior   2.460000   3.140000  0.680000   \n",
       "5                    True          in   3.140000   3.410000  0.270000   \n",
       "6                   False     college   3.410000   4.200000  0.790000   \n",
       "7                    True        when   4.790000   5.020000  0.230000   \n",
       "8                    True           I   5.020000   5.090000  0.070000   \n",
       "9                   False         got   5.090000   5.330000  0.240000   \n",
       "10                   True          my   5.350000   5.553515  0.203515   \n",
       "11                  False       first   5.553585   6.340000  0.786415   \n",
       "12                  False      paying   7.110000   7.660000  0.550000   \n",
       "13                  False         job   7.660000   8.300000  0.640000   \n",
       "14                   True          in   8.650000   8.930000  0.280000   \n",
       "15                   True          my   8.930000   9.170917  0.240917   \n",
       "16                  False       field   9.176691   9.990000  0.813309   \n",
       "17                   True          on  10.290000  10.500000  0.210000   \n",
       "18                   True         the  10.500000  10.620000  0.120000   \n",
       "19          .       False       radio  10.620000  11.360000  0.740000   \n",
       "20                   True        This  11.820000  11.970000  0.150000   \n",
       "21                   True          is  11.970000  12.090000  0.120000   \n",
       "22                   True         not  12.090000  12.277681  0.187681   \n",
       "23                   True          an  12.277681  12.386365  0.108684   \n",
       "24          ,       False  internship  12.386365  13.390000  1.003635   \n",
       "25                   True         I'm  13.830000  14.040000  0.210000   \n",
       "\n",
       "    Named_Entity  NWP_Candidate  \n",
       "0          False          False  \n",
       "1          False          False  \n",
       "2          False          False  \n",
       "3          False          False  \n",
       "4          False           True  \n",
       "5          False          False  \n",
       "6          False           True  \n",
       "7          False          False  \n",
       "8          False          False  \n",
       "9          False           True  \n",
       "10         False          False  \n",
       "11         False           True  \n",
       "12         False           True  \n",
       "13         False           True  \n",
       "14         False          False  \n",
       "15         False          False  \n",
       "16         False           True  \n",
       "17         False          False  \n",
       "18         False          False  \n",
       "19         False           True  \n",
       "20         False          False  \n",
       "21         False          False  \n",
       "22         False          False  \n",
       "23         False          False  \n",
       "24         False           True  \n",
       "25         False          False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b8e5679a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_segments = pd.read_csv(df_segments_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17dcf3cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_idxs = np.where(df_preproc['NWP_Candidate'])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7ca8a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "69cb8be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_idx = df_preproc.index[-1]\n",
    "if last_idx not in prediction_idxs:\n",
    "    print ('Here')\n",
    "    prediction_idxs = np.append(prediction_idxs, last_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fd6e5f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,    6,    9,   11,   12,   13,   16,   19,   24,   26,   28,\n",
       "         32,   34,   35,   36,   39,   40,   46,   49,   51,   52,   53,\n",
       "         56,   58,   60,   61,   64,   70,   75,   77,   81,   85,   87,\n",
       "         90,   91,   92,   95,   99,  100,  101,  102,  104,  106,  111,\n",
       "        113,  116,  117,  118,  119,  123,  125,  129,  131,  133,  135,\n",
       "        136,  143,  144,  148,  150,  154,  156,  157,  159,  163,  164,\n",
       "        166,  167,  171,  172,  174,  175,  176,  179,  180,  181,  184,\n",
       "        185,  187,  190,  192,  194,  196,  198,  201,  203,  208,  212,\n",
       "        214,  216,  217,  220,  221,  222,  224,  225,  226,  227,  235,\n",
       "        236,  239,  244,  248,  253,  255,  257,  259,  263,  265,  266,\n",
       "        268,  269,  272,  276,  279,  281,  285,  295,  296,  297,  299,\n",
       "        302,  305,  308,  312,  315,  317,  319,  323,  324,  325,  327,\n",
       "        328,  329,  331,  332,  334,  337,  340,  341,  343,  344,  346,\n",
       "        347,  349,  354,  355,  356,  358,  361,  364,  368,  369,  373,\n",
       "        380,  381,  383,  384,  390,  391,  393,  395,  396,  398,  401,\n",
       "        403,  404,  409,  413,  417,  421,  424,  425,  426,  428,  431,\n",
       "        433,  434,  436,  440,  441,  442,  443,  446,  448,  449,  450,\n",
       "        451,  462,  464,  465,  467,  469,  470,  471,  476,  478,  479,\n",
       "        480,  483,  485,  487,  489,  496,  497,  500,  502,  504,  507,\n",
       "        508,  512,  517,  518,  520,  524,  525,  528,  530,  532,  534,\n",
       "        536,  537,  540,  547,  549,  551,  553,  558,  562,  563,  565,\n",
       "        567,  572,  573,  576,  578,  580,  582,  583,  584,  585,  588,\n",
       "        590,  592,  593,  597,  598,  600,  602,  603,  604,  607,  609,\n",
       "        611,  613,  618,  619,  620,  621,  623,  626,  627,  629,  630,\n",
       "        631,  638,  639,  641,  645,  648,  649,  650,  651,  657,  659,\n",
       "        663,  665,  666,  670,  672,  673,  677,  678,  681,  684,  687,\n",
       "        693,  696,  701,  704,  707,  710,  713,  716,  722,  724,  727,\n",
       "        731,  734,  736,  740,  741,  744,  746,  747,  751,  754,  756,\n",
       "        758,  759,  762,  766,  768,  770,  774,  775,  779,  780,  781,\n",
       "        782,  784,  791,  793,  796,  797,  799,  800,  802,  804,  805,\n",
       "        807,  813,  815,  817,  819,  821,  823,  824,  828,  829,  833,\n",
       "        834,  836,  837,  840,  842,  845,  848,  850,  856,  858,  861,\n",
       "        864,  867,  868,  870,  871,  874,  875,  878,  880,  882,  884,\n",
       "        885,  886,  888,  889,  892,  894,  896,  901,  903,  904,  907,\n",
       "        909,  910,  912,  914,  919,  920,  923,  925,  927,  929,  935,\n",
       "        936,  937,  941,  943,  948,  949,  951,  952,  953,  954,  955,\n",
       "        956,  957,  958,  959,  962,  963,  965,  970,  972,  974,  977,\n",
       "        981,  984,  986,  990,  994,  995,  998, 1004, 1009, 1011, 1014,\n",
       "       1017, 1020, 1022, 1026, 1027, 1028, 1029, 1030, 1032, 1035, 1036,\n",
       "       1037, 1043, 1048, 1050, 1055, 1056, 1058, 1062, 1064, 1066, 1068,\n",
       "       1071, 1075, 1077, 1078, 1082, 1085, 1088, 1094, 1096, 1097, 1099,\n",
       "       1100, 1102, 1104, 1106, 1109, 1112, 1114, 1116, 1118, 1123, 1125,\n",
       "       1127, 1128, 1130, 1133, 1136, 1139, 1141, 1143, 1147, 1149, 1159,\n",
       "       1161, 1162, 1169, 1170, 1174, 1175, 1178, 1182, 1183, 1187, 1190,\n",
       "       1191, 1193, 1195, 1197, 1199, 1202, 1206, 1207, 1211, 1212, 1214,\n",
       "       1215, 1217, 1220, 1221, 1225, 1227, 1230, 1233, 1236, 1239, 1240,\n",
       "       1241, 1244, 1246, 1247, 1250, 1251, 1253, 1255, 1256, 1258, 1264,\n",
       "       1265, 1269, 1273, 1277, 1278, 1281, 1284, 1287, 1291, 1294, 1298,\n",
       "       1300, 1303, 1305, 1306, 1308, 1311, 1313, 1318, 1319, 1321, 1322,\n",
       "       1325, 1328, 1329, 1331, 1333, 1336, 1338, 1343, 1345, 1347, 1350,\n",
       "       1358, 1360, 1365, 1369, 1370, 1371, 1376, 1380, 1381, 1382, 1383,\n",
       "       1385, 1388, 1394, 1408, 1412, 1416, 1421, 1424, 1427, 1430, 1433,\n",
       "       1436, 1440, 1443, 1445, 1446, 1448, 1450, 1454, 1456, 1462, 1465,\n",
       "       1466, 1468, 1473, 1476, 1479, 1482, 1485, 1488, 1490, 1493, 1496,\n",
       "       1499, 1501, 1507, 1509, 1514, 1516, 1519, 1522, 1525, 1526, 1530,\n",
       "       1534, 1539, 1540])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ad851f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs = np.split(df_preproc, prediction_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5f2f978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_columns = ['Word_Written', 'Punctuation', 'Onset', 'Offset', 'Duration']\n",
    "\n",
    "sampled = split_dfs[0][sample_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2e23d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in split_dfs:\n",
    "    if df.empty:\n",
    "        print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "134a6f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792.35"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Onset'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "461eff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/narratives/lib/python3.7/site-packages/pandas/core/frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "sampled[['Norm_Onset', 'Norm_Offset']] = sampled[['Onset', 'Offset']] - sampled['Onset'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "755a8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['filename', 'critical_word', 'transcript_index', 'clip_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a829985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)] = {\n",
    "    'filename': 'test',\n",
    "    'critical_word': 'test',\n",
    "    'transcript_index': 1,\n",
    "    'clip_words': sample_json\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "280bc67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Word_Written\":\"So\",\"Punctuation\":\" \",\"Onset\":0.24,\"Offset\":0.63,\"Duration\":0.39,\"Norm_Onset\":0.0,\"Norm_Offset\":0.39},{\"Word_Written\":\"I\",\"Punctuation\":\" \",\"Onset\":0.68,\"Offset\":1.26,\"Duration\":0.58,\"Norm_Onset\":0.44,\"Norm_Offset\":1.02},{\"Word_Written\":\"was\",\"Punctuation\":\" \",\"Onset\":1.96,\"Offset\":2.3,\"Duration\":0.34,\"Norm_Onset\":1.72,\"Norm_Offset\":2.06},{\"Word_Written\":\"a\",\"Punctuation\":\" \",\"Onset\":2.3,\"Offset\":2.45,\"Duration\":0.15,\"Norm_Onset\":2.06,\"Norm_Offset\":2.21}]'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clip_words'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c2167ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dfs[0].loc[0, 'test'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e644a6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Written</th>\n",
       "      <th>Case</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop_Word</th>\n",
       "      <th>Word_Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>NWP_Candidate</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>So</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.39</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.45</td>\n",
       "      <td>0.15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Word_Written     Case  POS     POS_Definition Punctuation  Stop_Word  \\\n",
       "0           So  success   RB             adverb                   True   \n",
       "1            I  success  PRP  pronoun, personal                   True   \n",
       "2          was  success  VBD   verb, past tense                   True   \n",
       "3            a  success   DT         determiner                   True   \n",
       "\n",
       "  Word_Vocab  Onset  Offset  Duration  Named_Entity  NWP_Candidate  test  \n",
       "0         So   0.24    0.63      0.39         False          False   1.0  \n",
       "1          I   0.68    1.26      0.58         False          False   NaN  \n",
       "2        was   1.96    2.30      0.34         False          False   NaN  \n",
       "3          a   2.30    2.45      0.15         False          False   NaN  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "641fae05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_json = sampled.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "74da6a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"Word_Written\":\"So\",\"Punctuation\":\" \",\"Onset\":0.24,\"Offset\":0.63,\"Duration\":0.39,\"Norm_Onset\":0.0,\"Norm_Offset\":0.39},{\"Word_Written\":\"I\",\"Punctuation\":\" \",\"Onset\":0.68,\"Offset\":1.26,\"Duration\":0.58,\"Norm_Onset\":0.44,\"Norm_Offset\":1.02},{\"Word_Written\":\"was\",\"Punctuation\":\" \",\"Onset\":1.96,\"Offset\":2.3,\"Duration\":0.34,\"Norm_Onset\":1.72,\"Norm_Offset\":2.06},{\"Word_Written\":\"a\",\"Punctuation\":\" \",\"Onset\":2.3,\"Offset\":2.45,\"Duration\":0.15,\"Norm_Onset\":2.06,\"Norm_Offset\":2.21}]'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
