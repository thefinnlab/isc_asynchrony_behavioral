{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4491ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /dartfs/rc/lab/F/FinnLab/tommy/models/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 14:26:31.002346: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-20 14:26:31.002456: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-20 14:26:31.010289: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-20 14:26:31.766770: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-20 14:26:42.905218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys, glob\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from praatio import textgrid as tgio\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from config import *\n",
    "from preproc_utils import gentle_fill_missing_words, create_word_prediction_df, clean_hyphenated_words, clean_named_entities, dataframe_to_textgrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29d72a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'black'\n",
    "overwrite = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769edc24",
   "metadata": {},
   "source": [
    "### Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77c8f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directories\n",
    "stim_dir = os.path.join(BASE_DIR, 'stimuli')\n",
    "gentle_dir = os.path.join(stim_dir, 'gentle')\n",
    "preproc_dir = os.path.join(stim_dir,'preprocessed')\n",
    "task_out_dir = os.path.join(preproc_dir, task)\n",
    "backup_dir = os.path.join(task_out_dir, 'src')\n",
    "\n",
    "audio_fn = glob.glob(os.path.join(stim_dir, 'audio', f'*{task}*.wav'))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a8b08",
   "metadata": {},
   "source": [
    "### Load adjusted file\n",
    "\n",
    "Currently we only mapped the word tier from gentle to praat -- need to map the phoneme tier as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1273dd20-b7e3-457b-90f2-7f3b2cf1d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gentle_to_textgrid(alignment_fn):\n",
    "\t\"\"\"\n",
    "\tTake a filename and its associated transcription and fill in all the gaps\n",
    "\t\"\"\"\n",
    "    \n",
    "\trearranged_words = []\n",
    "\tfile_ons = 0\n",
    "\t\n",
    "\t# load the alignment file\n",
    "\twith open(alignment_fn, encoding='utf-8') as f:\n",
    "\t\tcontent = json.load(f)\n",
    "\tall_ons = content['words'][0]['start']\n",
    "\t\n",
    "\tfor ix, word in enumerate(content['words']):\n",
    "\t\t# if the word was successfully aligned\n",
    "\t\tif word['case'] == 'success' or word['case'] == 'assumed':\n",
    "\t\t\tword_ons = np.round(word['start'], 3)\n",
    "\t\t\tword_off = np.round(word['end'], 3)\n",
    "\t\t\ttarget = word['alignedWord']\n",
    "\t\t\trearranged_words.append((word_ons, word_off, target))\n",
    "\t\telse:\n",
    "\t\t\t# search forwards and backwards to find the previous and next word\n",
    "\t\t\t# use the end and start times to get word times \n",
    "\t\t\ttarget = content['words'][ix]['word']\n",
    "\t\t\tprev_end, next_start = align_missing_word(content, ix)\n",
    "\t\t\t\n",
    "\t\t\trearranged_words.append((prev_end, next_start, target))\n",
    "\t\n",
    "\t# adjust for overlap in times\n",
    "\tfor ix, word_times in enumerate(rearranged_words):\n",
    "\t\tif ix != 0:\n",
    "\t\t\tprev_start, prev_end, prev_word = rearranged_words[ix-1]\n",
    "\t\t\tcurr_start, curr_end, curr_word = word_times\n",
    "\n",
    "\t\t\t# if the current start time is before the previous end --> adjust\n",
    "\t\t\t# to be the previous end time\n",
    "\t\t\tif curr_start < prev_end:\n",
    "\t\t\t\trearranged_words[ix] = (prev_end, curr_end, curr_word)\n",
    "\t\t\t\tcurr_start, curr_end, curr_word = rearranged_words[ix]\n",
    "\n",
    "\t\t\t# if the current end time is after the current start time\n",
    "\t\t\t# set to be the next start time\n",
    "\t\t\tif curr_end < curr_start and (ix+1 != len(rearranged_words)):\n",
    "\t\t\t\tnext_start, next_end, next_word = rearranged_words[ix+1]\n",
    "\t\t\t\trearranged_words[ix] = (curr_start, next_start, curr_word)\n",
    "\t\t\t\tcurr_start, curr_end, curr_word = rearranged_words[ix]\n",
    "\n",
    "\t\t\t# final catch is adding a tiny bit of padding to the end word to adjust\n",
    "\t\t\tif curr_end == curr_start:\n",
    "\t\t\t\trearranged_words[ix] = (curr_start, curr_end+0.0001, curr_word)\n",
    "\t\n",
    "\ttg = tgio.Textgrid()\n",
    "\ttg.addTier(tgio.IntervalTier('word', rearranged_words))\n",
    "\treturn content, tg\n",
    "\n",
    "def gentle_fill_missing_words(alignment_fn):\n",
    "\t'''\n",
    "\tA simple way to fill missing aligned words\n",
    "\t'''\n",
    "\t\n",
    "\t# load the alignment file\n",
    "\twith open(alignment_fn, encoding='utf-8') as f:\n",
    "\t\tcontent = json.load(f)\n",
    "\t\t\n",
    "\tfor ix, word in enumerate(content['words']):\n",
    "\t\tif word['case'] != 'success':\n",
    "\t\t\tprev_end, next_start = align_missing_word(content, ix)\n",
    "\t\t\tcontent['words'][ix].update({'start': prev_end, 'end': next_start, 'case': 'assumed'})\n",
    "\t\t\t\n",
    "\treturn content\n",
    "\n",
    "def align_missing_word(content, ix):\n",
    "\t'''\n",
    "\tSearches from a word in both directions and then distributes time evenly\n",
    "\t'''\n",
    "\t# keep track of how many are missing\n",
    "\tforward_ix = ix\n",
    "\tforward_missing = 0\n",
    "\t\n",
    "\t# search forward\n",
    "\twhile True:\n",
    "\t\t# move one forward\n",
    "\t\tforward_ix += 1\n",
    "\t\tif content['words'][forward_ix]['case'] == 'success':\n",
    "\t\t\tnext_start = np.round(content['words'][forward_ix]['start'], 3)\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tforward_missing += 1\n",
    "\t\n",
    "\t# keep track of how many are missing\n",
    "\tback_ix = ix\n",
    "\tback_missing = 0\n",
    "\t\n",
    "\twhile True:\n",
    "\t\t# move one backwards\n",
    "\t\tback_ix -= 1\n",
    "\t\t\n",
    "\t\tif content['words'][back_ix]['case'] == 'success':\n",
    "\t\t\tprev_end = np.round(content['words'][back_ix]['end'], 3)\n",
    "\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tback_missing += 1\n",
    "\t\n",
    "\t# space evenly between the number of missing items\n",
    "\ttotal_missing = back_missing + forward_missing + 1 # add one to include current item\n",
    "\tx_vals = np.linspace(prev_end, next_start, total_missing + 2)[1:-1] # add 2 to pad the points on either side\n",
    "\t\n",
    "\t# if there is anything missing\n",
    "\t# normalize indices to 0\n",
    "\tmissing_ixs = np.arange(ix-back_missing,ix+forward_missing+1)\n",
    "\t\n",
    "\t# index of the value in the interpolated array\n",
    "\tarr_ix = np.argwhere(ix == missing_ixs)\n",
    "\t\n",
    "\t# then extract value from that array and round\n",
    "\tnext_start = x_vals[arr_ix].squeeze()\n",
    "\tnext_start = np.round(next_start, 3)\n",
    "\t\n",
    "\t# have to adjust prev end to be the interpolated value\n",
    "\tif len(missing_ixs) > 1 and arr_ix:\n",
    "\t\tprev_end = x_vals[np.argwhere(ix == missing_ixs)-1].squeeze()\n",
    "\t\tprev_end = np.round(prev_end, 3)\n",
    "\t\n",
    "\treturn prev_end, next_start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3ab9c6ba-4b62-4059-87b2-66d20afaca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_phonemes_to_word(word_start, word_end, phonemes):\n",
    "    \"\"\"\n",
    "    Scale phoneme timings to match the word boundaries.\n",
    "    \"\"\"\n",
    "    word_duration = word_end - word_start\n",
    "    phoneme_duration = sum(p['duration'] for p in phonemes)\n",
    "    scale_factor = word_duration / phoneme_duration\n",
    "\n",
    "    scaled_phonemes = []\n",
    "    current_time = word_start\n",
    "    for phone in phonemes:\n",
    "        scaled_duration = phone['duration'] * scale_factor\n",
    "        phone_end = current_time + scaled_duration\n",
    "        scaled_phonemes.append({\n",
    "            'start': current_time,\n",
    "            'end': phone_end,\n",
    "            'phone': phone['phone']\n",
    "        })\n",
    "        current_time = phone_end\n",
    "\n",
    "    # Adjust the last phoneme to exactly match the word end time\n",
    "    if scaled_phonemes:\n",
    "        scaled_phonemes[-1]['end'] = word_end\n",
    "\n",
    "    return scaled_phonemes\n",
    "\n",
    "def gentle_to_textgrid_phoneme(alignment_fn, word_textgrid):\n",
    "    \"\"\"\n",
    "    Take a filename of a Gentle alignment JSON and a word TextGrid, and return a Praat TextGrid for the phoneme tier,\n",
    "    including CMU phoneme categories, with phonemes scaled to match word boundaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # load the alignment file\n",
    "    with open(align_fn, encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "    \n",
    "    word_tier = word_textgrid.getTier('word')\n",
    "    rearranged_phones = []\n",
    "    \n",
    "    word_index = 0\n",
    "    gentle_index = 0\n",
    "    while gentle_index < len(content['words']) and word_index < len(word_tier):\n",
    "        gentle_word = content['words'][gentle_index]\n",
    "        word_interval = word_tier.entries[word_index]\n",
    "                \n",
    "        # Check if we need to combine hyphenated words\n",
    "        if '-' in word_interval.label.lower() and gentle_word['word'] != word_interval.label.lower():\n",
    "\n",
    "            combined_word = gentle_word['word']\n",
    "            combined_phones = gentle_word['phones'] if 'phones' in gentle_word else []\n",
    "            next_gentle_index = gentle_index + 1\n",
    "            \n",
    "            while next_gentle_index < len(content['words']):\n",
    "                next_word = content['words'][next_gentle_index]\n",
    "                combined_word += next_word['word']\n",
    "                if 'phones' in next_word:\n",
    "                    combined_phones.extend(next_word['phones'])\n",
    "                \n",
    "                if combined_word.lower() == word_interval.label.lower().replace('-', ''):\n",
    "                    # We've found a match for the hyphenated word\n",
    "                    gentle_word = {\n",
    "                        'word': word_interval.label.lower(),\n",
    "                        'phones': combined_phones,\n",
    "                        'case': 'success' if all(w['case'] == 'success' for w in content['words'][gentle_index:next_gentle_index+1]) else 'partial'\n",
    "                    }\n",
    "                    gentle_index = next_gentle_index\n",
    "                    break\n",
    "                next_gentle_index += 1\n",
    "        \n",
    "        if gentle_word['case'] == 'success' and 'phones' in gentle_word and gentle_word['word'].lower() == word_interval.label.lower():\n",
    "            word_start, word_end = word_interval.start, word_interval.end\n",
    "            \n",
    "            # Scale phonemes to match the word boundaries\n",
    "            scaled_phonemes = scale_phonemes_to_word(word_start, word_end, gentle_word['phones'])\n",
    "            \n",
    "            for phone in scaled_phonemes:\n",
    "                phone_start = np.round(phone['start'], 3)\n",
    "                phone_end = np.round(phone['end'], 3)\n",
    "                phone_label = phone['phone']\n",
    "                \n",
    "                # only get the first phoneme --> this maps to CMU phoneme dictionary\n",
    "                phone_label = phone_label.split('_')[0].upper()\n",
    "                \n",
    "                rearranged_phones.append((phone_start, phone_end, phone_label))\n",
    "            \n",
    "            word_index += 1\n",
    "        else:\n",
    "            # If we can't find a match, move to the next word in both Gentle and TextGrid\n",
    "            word_index += 1\n",
    "        \n",
    "        gentle_index += 1\n",
    "    \n",
    "    # Sort phones by start time (in case they're not already in order)\n",
    "    rearranged_phones.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Fill gaps with silence\n",
    "    final_phones = []\n",
    "    for ix in range(len(rearranged_phones)):\n",
    "        curr_start, curr_end, curr_phone = rearranged_phones[ix]\n",
    "        if ix > 0:\n",
    "            prev_start, prev_end, prev_phone = final_phones[-1]\n",
    "            if curr_start > prev_end:\n",
    "                # Insert silence\n",
    "                final_phones.append((prev_end, curr_start, \"\"))\n",
    "        final_phones.append((curr_start, curr_end, curr_phone))\n",
    "    \n",
    "    # tg = tgio.Textgrid()\n",
    "    word_textgrid.addTier(tgio.IntervalTier('phone', final_phones))\n",
    "    return word_textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "64538882-b841-4e9c-b042-cf43cab9f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'demon'\n",
    "\n",
    "praat_fn = os.path.join(preproc_dir, task, f'{task}_transcript-praat.TextGrid')\n",
    "align_fn = os.path.join(gentle_dir, task, 'align.json')\n",
    "\n",
    "word_textgrid = tgio.openTextgrid(praat_fn, False)\n",
    "phone_textgrid = gentle_to_textgrid_phoneme(align_fn, word_textgrid)\n",
    "\n",
    "# tg_phone.getTier('phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "30070e23-2562-4b25-84b9-a8940d9507e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = tgio.Textgrid()\n",
    "\n",
    "for tier_name in ['phone', 'word']:\n",
    "    tier = phone_textgrid.getTier(tier_name)\n",
    "    tg.addTier(tier)\n",
    "\n",
    "praat_phone_fn = os.path.join(preproc_dir, task, f'{task}_transcript-praat_phone.TextGrid')\n",
    "tg.save(praat_phone_fn, 'long_textgrid', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0b0eda-1cec-49c3-8c9f-afefb5ad75fc",
   "metadata": {},
   "source": [
    "## Test this outside the function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
