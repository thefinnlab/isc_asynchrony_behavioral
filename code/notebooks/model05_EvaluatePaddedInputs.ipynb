{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4967440b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/pandas/__init__.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     ArrowDtype,\n\u001b[1;32m     52\u001b[0m     Int8Dtype,\n\u001b[1;32m     53\u001b[0m     Int16Dtype,\n\u001b[1;32m     54\u001b[0m     Int32Dtype,\n\u001b[1;32m     55\u001b[0m     Int64Dtype,\n\u001b[1;32m     56\u001b[0m     UInt8Dtype,\n\u001b[1;32m     57\u001b[0m     UInt16Dtype,\n\u001b[1;32m     58\u001b[0m     UInt32Dtype,\n\u001b[1;32m     59\u001b[0m     UInt64Dtype,\n\u001b[1;32m     60\u001b[0m     Float32Dtype,\n\u001b[1;32m     61\u001b[0m     Float64Dtype,\n\u001b[1;32m     62\u001b[0m     CategoricalDtype,\n\u001b[1;32m     63\u001b[0m     PeriodDtype,\n\u001b[1;32m     64\u001b[0m     IntervalDtype,\n\u001b[1;32m     65\u001b[0m     DatetimeTZDtype,\n\u001b[1;32m     66\u001b[0m     StringDtype,\n\u001b[1;32m     67\u001b[0m     BooleanDtype,\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     NA,\n\u001b[1;32m     70\u001b[0m     isna,\n\u001b[1;32m     71\u001b[0m     isnull,\n\u001b[1;32m     72\u001b[0m     notna,\n\u001b[1;32m     73\u001b[0m     notnull,\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     Index,\n\u001b[1;32m     76\u001b[0m     CategoricalIndex,\n\u001b[1;32m     77\u001b[0m     RangeIndex,\n\u001b[1;32m     78\u001b[0m     MultiIndex,\n\u001b[1;32m     79\u001b[0m     IntervalIndex,\n\u001b[1;32m     80\u001b[0m     TimedeltaIndex,\n\u001b[1;32m     81\u001b[0m     DatetimeIndex,\n\u001b[1;32m     82\u001b[0m     PeriodIndex,\n\u001b[1;32m     83\u001b[0m     IndexSlice,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     NaT,\n\u001b[1;32m     86\u001b[0m     Period,\n\u001b[1;32m     87\u001b[0m     period_range,\n\u001b[1;32m     88\u001b[0m     Timedelta,\n\u001b[1;32m     89\u001b[0m     timedelta_range,\n\u001b[1;32m     90\u001b[0m     Timestamp,\n\u001b[1;32m     91\u001b[0m     date_range,\n\u001b[1;32m     92\u001b[0m     bdate_range,\n\u001b[1;32m     93\u001b[0m     Interval,\n\u001b[1;32m     94\u001b[0m     interval_range,\n\u001b[1;32m     95\u001b[0m     DateOffset,\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     to_numeric,\n\u001b[1;32m     98\u001b[0m     to_datetime,\n\u001b[1;32m     99\u001b[0m     to_timedelta,\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     Flags,\n\u001b[1;32m    102\u001b[0m     Grouper,\n\u001b[1;32m    103\u001b[0m     factorize,\n\u001b[1;32m    104\u001b[0m     unique,\n\u001b[1;32m    105\u001b[0m     value_counts,\n\u001b[1;32m    106\u001b[0m     NamedAgg,\n\u001b[1;32m    107\u001b[0m     array,\n\u001b[1;32m    108\u001b[0m     Categorical,\n\u001b[1;32m    109\u001b[0m     set_eng_float_format,\n\u001b[1;32m    110\u001b[0m     Series,\n\u001b[1;32m    111\u001b[0m     DataFrame,\n\u001b[1;32m    112\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/pandas/core/api.py:23\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     ArrowDtype,\n\u001b[1;32m     11\u001b[0m     CategoricalDtype,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     PeriodDtype,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     isna,\n\u001b[1;32m     18\u001b[0m     isnull,\n\u001b[1;32m     19\u001b[0m     notna,\n\u001b[1;32m     20\u001b[0m     notnull,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     factorize,\n\u001b[1;32m     25\u001b[0m     unique,\n\u001b[1;32m     26\u001b[0m     value_counts,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboolean\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BooleanDtype\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/pandas/core/algorithms.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m doc\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     construct_1d_object_array_from_listlike,\n\u001b[1;32m     38\u001b[0m     np_find_common_type,\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     41\u001b[0m     ensure_float64,\n\u001b[1;32m     42\u001b[0m     ensure_object,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     needs_i8_conversion,\n\u001b[1;32m     56\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat_compat\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/pandas/core/dtypes/cast.py:86\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_list_like\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     80\u001b[0m     is_valid_na_for_dtype,\n\u001b[1;32m     81\u001b[0m     isna,\n\u001b[1;32m     82\u001b[0m     na_value_for_dtype,\n\u001b[1;32m     83\u001b[0m     notna,\n\u001b[1;32m     84\u001b[0m )\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _arrow_dtype_mapping\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     90\u001b[0m         Sequence,\n\u001b[1;32m     91\u001b[0m         Sized,\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1532\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1506\u001b[0m, in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1624\u001b[0m, in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/joint-clm-prosody/')\n",
    "\n",
    "from config import *\n",
    "from src.data.components.audio_text_dataset import AudioTextDataset, load_audio, parse_textgrid, process_wavelet_file, extract_word_segment, pool_embeddings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from src.data.components.collators import audio_text_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f05b20",
   "metadata": {},
   "source": [
    "### Set up dataset and create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata exists. Processing only new files...\n",
      "No new files to process\n"
     ]
    }
   ],
   "source": [
    "# DATASET = 'pfka-moth-stories'\n",
    "# split = 'train'\n",
    "DATASET = 'gigaspeech/m'\n",
    "split = 'test'\n",
    "text_model_name = 'gpt2'\n",
    "audio_model_name = 'wav2vec2'\n",
    "\n",
    "dataset_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', DATASET)\n",
    "cache_dir = os.path.join(SCRATCH_DIR, 'nlp-datasets', DATASET)\n",
    "\n",
    "# create datasets\n",
    "dataset = AudioTextDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    cache_dir=cache_dir,\n",
    "    audio_model_name=audio_model_name, \n",
    "    text_model_name=text_model_name, \n",
    "    split=split,\n",
    ")\n",
    "\n",
    "dataset.preprocess_data()\n",
    "\n",
    "dataset._initialize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227153d",
   "metadata": {},
   "source": [
    "### Create segments for the current item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load and validate all file data\n",
    "fn = dataset.file_names[0]\n",
    "file_data = dataset._load_file_data(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d796e",
   "metadata": {},
   "source": [
    "#### Get text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ff6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text through tokenizer and get token information\n",
    "text = \" \".join([word['text'] for word in file_data['words']])\n",
    "text_tokens = dataset.text_tokenizer(text)\n",
    "\n",
    "# Get token counts and associated word ids\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4a1d4",
   "metadata": {},
   "source": [
    "#### Segment audio word level segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7a5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file_data['words']\n",
    "waveform = file_data['waveform']\n",
    "sample_rate = 16000\n",
    "\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)\n",
    "\n",
    "segments = []\n",
    "\n",
    "for word, idx, n_tokens in zip(words, word_ids, token_counts):\n",
    "    if n_tokens > 1:\n",
    "        ratios = torch.tensor([len(x) for x in self.text_tokenizer.batch_decode(\n",
    "            text_tokens['input_ids'][idx:idx+n_tokens])])\n",
    "        ratios = ratios / ratios.sum()\n",
    "        word_segments = extract_word_segment(waveform, sample_rate, \n",
    "                                            word[\"start\"], word[\"end\"], ratios=ratios)\n",
    "    else:\n",
    "        word_segments = extract_word_segment(waveform, sample_rate, \n",
    "                                            word[\"start\"], word[\"end\"])\n",
    "    segments.extend(word_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031cf0f",
   "metadata": {},
   "source": [
    "### Check padded inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbeb53",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be36c8",
   "metadata": {},
   "source": [
    "#### wav2vec2 comparison \n",
    "\n",
    "The problem can be [found here](https://github.com/huggingface/transformers/issues/21534) in a github issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b69b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "## This version exhibits differences based on padding\n",
    "# audio_model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# This version does not exhibit differences\n",
    "audio_model_name = \"facebook/wav2vec2-large-960h-lv60\"\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(audio_model_name)\n",
    "audio_model = AutoModel.from_pretrained(audio_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd757",
   "metadata": {},
   "source": [
    "#### data2vec comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69d10ebf9d64aa98d4c241f6017bf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e502ac1a9ed84952a8f0fef2abf24506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cebf21672914cfa80d70dcf4d98781a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809dfced32354ab388ff83f1c0be8167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22323c3a6f5347e38d9732c76930a42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17988ec174924b049e68d1070117ccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/373M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffa53e99a7742d38dcf09a4d341e990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/373M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "## This version exhibits differences based on padding\n",
    "# audio_model_name = \"facebook/data2vec-audio-base-960h\"\n",
    "\n",
    "## Both models exhibit differences\n",
    "audio_model_name = \"patrickvonplaten/data2vec-base-960h\" #\"facebook/data2vec-audio-large-100h\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(audio_model_name)\n",
    "audio_model = AutoModel.from_pretrained(audio_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f91654",
   "metadata": {},
   "source": [
    "#### Run with padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8590ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the batch\n",
    "padded_features = processor(segments, sampling_rate=sample_rate, padding=True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    padded_outs = audio_model(**padded_features).last_hidden_state\n",
    "\n",
    "attention_mask = audio_model._get_feature_vector_attention_mask(\n",
    "    padded_outs.shape[1], \n",
    "    padded_features['attention_mask']\n",
    ")\n",
    "\n",
    "padded_embeds = pool_embeddings(padded_outs, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86ecb4",
   "metadata": {},
   "source": [
    "#### Run without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2121f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the batch\n",
    "no_pad_features = processor(segments[:1], sampling_rate=sample_rate,return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    no_pad_outs = audio_model(**no_pad_features).last_hidden_state\n",
    "\n",
    "attention_mask = audio_model._get_feature_vector_attention_mask(\n",
    "    no_pad_outs.shape[1], \n",
    "    no_pad_features['attention_mask']\n",
    ")\n",
    "\n",
    "no_pad_embed = pool_embeddings(no_pad_outs, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf16612",
   "metadata": {},
   "source": [
    "#### Compare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd348802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9360])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(padded_embeds[:1], no_pad_embed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b991e",
   "metadata": {},
   "source": [
    "## Test whisper transcription pipeleine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb25a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/')\n",
    "\n",
    "from config import *\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3062c",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baeea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-tiny\"#\"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,  # batch size for inference - set based on your device\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106aaff",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "audio_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'libritts-r', 'audio', 'test-clean')\n",
    "\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=audio_dir)\n",
    "# dataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f6c537b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def chunk(it, size):\n",
    "\tit = iter(it)\n",
    "\treturn iter(lambda: tuple(itertools.islice(it, size)), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ec11dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': ' He hoped there would be stew for dinner, turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick, peppered flower fatten sauce. Stuffed into you, his belly, counseled him.'},\n",
       " {'text': ' It would be a gloomy secret night.'},\n",
       " {'text': ' After early nightfall, the yellow lamps would light up here and there the squalid quarter of the brothels.'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(list(x), batch_size=3, generate_kwargs={\"language\": \"english\", \"return_timestamps\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c686cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91011/91011 [17:37<00:00, 86.07it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "transcripts_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'voxceleb2', 'transcripts', 'val')\n",
    "\n",
    "transcripts_fns = sorted(glob.glob(os.path.join(transcripts_dir, '*')))\n",
    "\n",
    "for transcript_fn in tqdm(transcripts_fns):\n",
    "    # Open the file for reading\n",
    "    with open(transcript_fn, 'r') as file:\n",
    "        # Read the contents of the file\n",
    "        content = file.read()\n",
    "        \n",
    "        # Convert the content to lowercase\n",
    "        content = content.lower()\n",
    "    \n",
    "    # Save the content back to the same file (overwrite)\n",
    "    with open(transcript_fn, 'w') as file:\n",
    "        file.write(content)\n",
    "\n",
    "    # print(f\"Processed and saved: {transcript_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c3f8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'voxceleb2', 'audio', 'test')\n",
    "audio_fns = sorted(glob.glob(os.path.join(audio_dir, '*')))\n",
    "\n",
    "audio_batches = list(chunk(audio_fns, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "880e1702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1af0db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = audio_batches[414]\n",
    "\n",
    "# for fn in batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e39f6068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50258, 50363, 14, 67, 446, 16883, 14, 81, 66, 14, 44990, 14, 37, 14, 37, 7729, 37880, 14, 20367, 296, 1385, 14, 77, 75, 79, 12, 20367, 296, 1385, 14, 3080, 87, 384, 28512, 17, 14, 46069, 14, 31636, 14, 327, 12791, 2009, 24, 62, 41, 86, 42, 81, 35, 22, 32, 64, 62, 23, 78, 62, 1360, 28052, 13, 86, 706, 50257], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:06<00:00, 10.57it/s]\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "for fn in tqdm(batch):\n",
    "    # Read and write to fix format issues\n",
    "    data, samplerate = sf.read(fn)\n",
    "    # sf.write(fn, data, samplerate, subtype='PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a51647",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m video_dir \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASETS_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnlp-datasets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxceleb2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get all video files\u001b[39;00m\n\u001b[1;32m      6\u001b[0m video_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m), recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "video_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'voxceleb2', 'video', 'train')\n",
    "\n",
    "# Get all video files\n",
    "video_files = sorted(glob.glob(os.path.join(video_dir, \"*.mp4\"), recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0386b8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(av_dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0d080285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "\n",
    "av_dataset_info = os.path.join(DATASETS_DIR, 'nlp-datasets', 'avspeech', 'metadata.jsonl')\n",
    "jsonObj = pd.read_json(path_or_buf=av_dataset_info, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "88129a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_count': 12354,\n",
       " 'description': '山梨簿記学院\\u3000http://yboki.com/\\n体験講義動画\\u3000「為替手形の基本」',\n",
       " 'format': {'tags': {'major_brand': 'mp42',\n",
       "   'compatible_brands': 'isommp42',\n",
       "   'minor_version': '0'},\n",
       "  'start_time': 0.0,\n",
       "  'nb_streams': 2,\n",
       "  'format_name': ['3gp', '3g2', 'mov', 'mp4', 'mj2', 'm4a'],\n",
       "  'bit_rate': 1116993,\n",
       "  'nb_programs': 0,\n",
       "  'duration': 1044.3639,\n",
       "  'probe_score': 100,\n",
       "  'size': 145818419},\n",
       " 'video_id': 'w0Q5gH4hb7I',\n",
       " 'creation_time': '2016-08-27T15:10:23.000000Z',\n",
       " 'height': 720,\n",
       " 'dislike_count': 2,\n",
       " 'channel_id': 'UCZ-aFwJHTPY3gLQYN2S-Y3Q',\n",
       " 'like_count': 30,\n",
       " 'subtitles': {},\n",
       " 'duration': 1044,\n",
       " 'title': '為替手形の基本\\u3000簿記会計',\n",
       " 'tags': ['為替手形', '名宛人', '振出人', '引受人'],\n",
       " 'width': 1280,\n",
       " 'categories': ['Education']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['metadata'].iloc[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
