{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4967440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import os, sys\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/joint-clm-prosody/')\n",
    "\n",
    "from config import *\n",
    "# from src.data.components.audio_text_dataset import AudioTextDataset, load_audio, parse_textgrid, process_wavelet_file, extract_word_segment, pool_embeddings\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from src.data.components.collators import audio_text_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f05b20",
   "metadata": {},
   "source": [
    "### Set up dataset and create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata exists. Processing only new files...\n",
      "No new files to process\n"
     ]
    }
   ],
   "source": [
    "# DATASET = 'pfka-moth-stories'\n",
    "# split = 'train'\n",
    "DATASET = 'gigaspeech/m'\n",
    "split = 'test'\n",
    "text_model_name = 'gpt2'\n",
    "audio_model_name = 'wav2vec2'\n",
    "\n",
    "dataset_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', DATASET)\n",
    "cache_dir = os.path.join(SCRATCH_DIR, 'nlp-datasets', DATASET)\n",
    "\n",
    "# create datasets\n",
    "dataset = AudioTextDataset(\n",
    "    dataset_dir=dataset_dir,\n",
    "    cache_dir=cache_dir,\n",
    "    audio_model_name=audio_model_name, \n",
    "    text_model_name=text_model_name, \n",
    "    split=split,\n",
    ")\n",
    "\n",
    "dataset.preprocess_data()\n",
    "\n",
    "dataset._initialize_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227153d",
   "metadata": {},
   "source": [
    "### Create segments for the current item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae87c0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load and validate all file data\n",
    "fn = dataset.file_names[0]\n",
    "file_data = dataset._load_file_data(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d796e",
   "metadata": {},
   "source": [
    "#### Get text tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51ff6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text through tokenizer and get token information\n",
    "text = \" \".join([word['text'] for word in file_data['words']])\n",
    "text_tokens = dataset.text_tokenizer(text)\n",
    "\n",
    "# Get token counts and associated word ids\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4a1d4",
   "metadata": {},
   "source": [
    "#### Segment audio word level segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f7a5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file_data['words']\n",
    "waveform = file_data['waveform']\n",
    "sample_rate = 16000\n",
    "\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)\n",
    "\n",
    "segments = []\n",
    "\n",
    "for word, idx, n_tokens in zip(words, word_ids, token_counts):\n",
    "    if n_tokens > 1:\n",
    "        ratios = torch.tensor([len(x) for x in self.text_tokenizer.batch_decode(\n",
    "            text_tokens['input_ids'][idx:idx+n_tokens])])\n",
    "        ratios = ratios / ratios.sum()\n",
    "        word_segments = extract_word_segment(waveform, sample_rate, \n",
    "                                            word[\"start\"], word[\"end\"], ratios=ratios)\n",
    "    else:\n",
    "        word_segments = extract_word_segment(waveform, sample_rate, \n",
    "                                            word[\"start\"], word[\"end\"])\n",
    "    segments.extend(word_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0031cf0f",
   "metadata": {},
   "source": [
    "### Check padded inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bbeb53",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5be36c8",
   "metadata": {},
   "source": [
    "#### wav2vec2 comparison \n",
    "\n",
    "The problem can be [found here](https://github.com/huggingface/transformers/issues/21534) in a github issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6b69b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "## This version exhibits differences based on padding\n",
    "# audio_model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# This version does not exhibit differences\n",
    "audio_model_name = \"facebook/wav2vec2-large-960h-lv60\"\n",
    "\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(audio_model_name)\n",
    "audio_model = AutoModel.from_pretrained(audio_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bd757",
   "metadata": {},
   "source": [
    "#### data2vec comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dce1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69d10ebf9d64aa98d4c241f6017bf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e502ac1a9ed84952a8f0fef2abf24506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cebf21672914cfa80d70dcf4d98781a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809dfced32354ab388ff83f1c0be8167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22323c3a6f5347e38d9732c76930a42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17988ec174924b049e68d1070117ccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/373M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffa53e99a7742d38dcf09a4d341e990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/373M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "\n",
    "## This version exhibits differences based on padding\n",
    "# audio_model_name = \"facebook/data2vec-audio-base-960h\"\n",
    "\n",
    "## Both models exhibit differences\n",
    "audio_model_name = \"patrickvonplaten/data2vec-base-960h\" #\"facebook/data2vec-audio-large-100h\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(audio_model_name)\n",
    "audio_model = AutoModel.from_pretrained(audio_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f91654",
   "metadata": {},
   "source": [
    "#### Run with padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8590ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the batch\n",
    "padded_features = processor(segments, sampling_rate=sample_rate, padding=True, return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    padded_outs = audio_model(**padded_features).last_hidden_state\n",
    "\n",
    "attention_mask = audio_model._get_feature_vector_attention_mask(\n",
    "    padded_outs.shape[1], \n",
    "    padded_features['attention_mask']\n",
    ")\n",
    "\n",
    "padded_embeds = pool_embeddings(padded_outs, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86ecb4",
   "metadata": {},
   "source": [
    "#### Run without padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2121f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the batch\n",
    "no_pad_features = processor(segments[:1], sampling_rate=sample_rate,return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    no_pad_outs = audio_model(**no_pad_features).last_hidden_state\n",
    "\n",
    "attention_mask = audio_model._get_feature_vector_attention_mask(\n",
    "    no_pad_outs.shape[1], \n",
    "    no_pad_features['attention_mask']\n",
    ")\n",
    "\n",
    "no_pad_embed = pool_embeddings(no_pad_outs, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf16612",
   "metadata": {},
   "source": [
    "#### Compare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cd348802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9360])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(padded_embeds[:1], no_pad_embed[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b991e",
   "metadata": {},
   "source": [
    "## Test video chopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/')\n",
    "\n",
    "from config import *\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106aaff",
   "metadata": {},
   "source": [
    "### Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c83f348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import glob\n",
    "import argparse\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.io import read_video\n",
    "import shutil\n",
    "# from transformers import AutoImageProcessor, AutoProcessor, AutoModel, AutoTokenizer\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/feature-extraction/')\n",
    "\n",
    "import utils\n",
    "\n",
    "from extract_dataset_features import (\n",
    "    initialize_models,\n",
    "    load_file_data,\n",
    "    process_video,\n",
    "    process_single_file\n",
    ")\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "# sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/joint-clm-prosody/')\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96e7971e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "def get_args(dataset):\n",
    "    args = SimpleNamespace(\n",
    "        text_model = 'gpt2',\n",
    "        audio_model = 'wav2vec2',\n",
    "        video_model = 'data2vec',\n",
    "        split='train',\n",
    "        num_shards=5,\n",
    "        current_shard=0,\n",
    "        min_words=4,\n",
    "        max_words=128,\n",
    "        overwrite=0,\n",
    "        output_dir=os.path.join(DATASETS_DIR, 'nlp-datasets', dataset)\n",
    "    )\n",
    "    return args\n",
    "\n",
    "# Using the args\n",
    "dataset = 'lrs3'\n",
    "args = get_args(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "afbd9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60 and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'lrs3')\n",
    "\n",
    "# Initialize models used in the preprocessing \n",
    "models = initialize_models(args)\n",
    "\n",
    "# Prepare directory structure --> only this script is needed for video\n",
    "dirs, splits = utils.prepare_directory_structure(dataset_dir, [args.split], video=True)\n",
    "\n",
    "# Setup cache directories\n",
    "model_combo = f\"{args.text_model}\"\n",
    "\n",
    "if args.audio_model:\n",
    "    model_combo += f\"-{args.audio_model}\"\n",
    "\n",
    "if args.video_model:\n",
    "    model_combo += f\"-{args.video_model}\"\n",
    "\n",
    "# Create cache for our features and a temp directory for writing progress\n",
    "dirs[\"cache_dir\"] = os.path.join(args.output_dir, 'features', model_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c6bf39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features for train split...\n",
      "\n",
      "Current shard: 1/5\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nExtracting features for {args.split} split...\", flush=True)\n",
    "print(f\"\\nCurrent shard: {args.current_shard+1}/{args.num_shards}\", flush=True)\n",
    "split_dirs = {k: os.path.join(v, args.split) for k, v in dirs.items()}\n",
    "\n",
    "# Make the json directories\n",
    "split_dirs[\"temp_dir\"] = os.path.join(split_dirs[\"cache_dir\"], 'temp')\n",
    "split_dirs[\"errors_dir\"] = os.path.join(split_dirs[\"cache_dir\"], 'errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1b19768c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 1/5 with 23670 files\n"
     ]
    }
   ],
   "source": [
    "# Get file list \n",
    "all_fns = sorted(os.listdir(split_dirs[\"textgrids\"]))\n",
    "all_fns = [os.path.splitext(fn)[0] for fn in all_fns]\n",
    "\n",
    "# Apply sharding logic --> divide dataset into number of shards \n",
    "if args.num_shards > 1:\n",
    "    # Calculate shard size and starting/ending indices\n",
    "    shard_size = math.ceil(len(all_fns) / args.num_shards)\n",
    "    start_idx = args.current_shard * shard_size\n",
    "    end_idx = min(start_idx + shard_size, len(all_fns))\n",
    "    \n",
    "    # Get only the files for the current shard\n",
    "    all_fns = all_fns[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing shard {args.current_shard+1}/{args.num_shards} with {len(all_fns)} files\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc244746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 existing json files and 23670 files to process\n"
     ]
    }
   ],
   "source": [
    "# Count existing json files\n",
    "existing_count = 0\n",
    "to_process_count = 0\n",
    "\n",
    "process_fns = []\n",
    "\n",
    "for fn in all_fns:\n",
    "    temp_json_fn = utils.get_temp_json_path(split_dirs['temp_dir'], fn)\n",
    "    # error_json_fn = utils.get_temp_json_path(split_dirs['errors_dir'], fn)\n",
    "\n",
    "    # If the file was successfully or unsuccessfully processed, a json exists\n",
    "    if (os.path.exists(temp_json_fn)): #and not args.overwrite:\n",
    "        existing_count += 1\n",
    "    else:\n",
    "        process_fns.append(fn)\n",
    "        to_process_count += 1\n",
    "\n",
    "print(f\"Found {existing_count} existing json files and {to_process_count} files to process\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d847b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21638 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/21638 [00:01<1:01:41,  5.84it/s]\n"
     ]
    }
   ],
   "source": [
    "args.min_words = 3\n",
    "counter = 0\n",
    "max_files = 10\n",
    "short_files = []\n",
    "\n",
    "for file_name in tqdm(process_fns):\n",
    "    # Load file data\n",
    "    file_data = load_file_data(args, split_dirs, models, file_name)\n",
    "\n",
    "    if file_data is None:\n",
    "        continue\n",
    "\n",
    "    if len(file_data['words']) < 4:\n",
    "        short_files.append(file_name)\n",
    "        counter += 1\n",
    "        # continue\n",
    "    if counter == max_files:\n",
    "        break\n",
    "    # print (len(file_data['words']))\n",
    "# else:\n",
    "#     message = process_single_file(args, split_dirs, models, file_name)\n",
    "#     if not message:\n",
    "#         continue\n",
    "#     break\n",
    "#         # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb14bab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At time 2.24/2.20 -- over tolerance by 0.04s, or 1 idx\n"
     ]
    }
   ],
   "source": [
    "file_data = load_file_data(args, split_dirs, models, process_fns[2])\n",
    "\n",
    "# Process text\n",
    "text = \" \".join([word['text'] for word in file_data['words']])\n",
    "text_tokens = models['tokenizer'](text)\n",
    "\n",
    "# Get unique ids and number of tokens\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)\n",
    "\n",
    "print (f'At time {offset}/{media_length/rate:.2f} -- over tolerance by {offset-media_length/rate:.2f}s, or {(offset * rate - media_length):.0f} idx', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a92b56b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m word_segments \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_media_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/utils.py:455\u001b[0m, in \u001b[0;36mextract_media_segment\u001b[0;34m(media_data, rate, onset, offset, ratios, end_tolerance, time_axis)\u001b[0m\n\u001b[1;32m    453\u001b[0m     torch_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(indices, device\u001b[38;5;241m=\u001b[39mmedia_data\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# Use index_select for PyTorch tensors\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     segment \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmedia_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Use take for NumPy arrays\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     segment \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake(media_data, indices, axis\u001b[38;5;241m=\u001b[39mtime_axis)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "word_segments = utils.extract_media_segment(video_data, fps, word[\"start\"], word[\"end\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b34dc5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overby' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     ratios \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Extract word segments with weighted ratios\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m word_segments \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_media_segment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43monset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mratios\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mratios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_tolerance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend_tolerance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m segments\u001b[38;5;241m.\u001b[39mextend(word_segments)\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/preproc-datasets/utils.py:432\u001b[0m, in \u001b[0;36mextract_media_segment\u001b[0;34m(media_data, rate, onset, offset, ratios, end_tolerance, time_axis)\u001b[0m\n\u001b[1;32m    429\u001b[0m over_by \u001b[38;5;241m=\u001b[39m offset \u001b[38;5;241m-\u001b[39m (media_length\u001b[38;5;241m/\u001b[39mrate)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# We have set an end tolerance (in s) and we're within that tolerance\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m end_tolerance \u001b[38;5;129;01mand\u001b[39;00m \u001b[43moverby\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_tolerance:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPassed tolerance check: time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmedia_length\u001b[38;5;241m/\u001b[39mrate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms // over \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mover_by\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    434\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m media_length\n",
      "\u001b[0;31mNameError\u001b[0m: name 'overby' is not defined"
     ]
    }
   ],
   "source": [
    "end_tolerance = 1\n",
    "file_data = load_file_data(args, split_dirs, models, process_fns[2])\n",
    "\n",
    "# Process text\n",
    "text = \" \".join([word['text'] for word in file_data['words']])\n",
    "text_tokens = models['tokenizer'](text)\n",
    "\n",
    "# Get unique ids and number of tokens\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)\n",
    "\n",
    "words, video_data, fps = [file_data.get(item) for item in ['words', 'video_data', 'video_fps']]\n",
    "processor, model = [models['video'].get(item) for item in ['processor', 'model']]\n",
    "\n",
    "# Get word token ids\n",
    "word_ids, token_counts = np.unique(text_tokens.word_ids(), return_counts=True)\n",
    "\n",
    "segments = []\n",
    "\n",
    "# Process each word\n",
    "for i, (word, idx, n_tokens) in enumerate(zip(words, word_ids, token_counts)):\n",
    "    if n_tokens > 1:\n",
    "        # If current word has multiple tokens, create ratios based on length of tokens\n",
    "        ratios = [len(x) for x in models['tokenizer'].batch_decode(text_tokens[\"input_ids\"][idx:idx+n_tokens])]\n",
    "        ratios = torch.tensor(ratios)\n",
    "        ratios = ratios / ratios.sum()\n",
    "    else:\n",
    "        ratios = None\n",
    "\n",
    "    # Extract word segments with weighted ratios\n",
    "    word_segments = utils.extract_media_segment(\n",
    "        video_data, \n",
    "        rate = fps, \n",
    "        onset = word[\"start\"], \n",
    "        offset = word[\"end\"], \n",
    "        ratios = ratios, \n",
    "        end_tolerance = end_tolerance if (i+1) == len(words) else None\n",
    "    )\n",
    "\n",
    "    segments.extend(word_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4df10b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0214507a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0210,  0.0053, -0.0112,  ..., -0.0545, -0.0241,  0.0113],\n",
       "        [-0.0199,  0.0224, -0.0165,  ..., -0.0474, -0.0183,  0.0007],\n",
       "        [-0.0199,  0.0300, -0.0150,  ..., -0.0447, -0.0090,  0.0012]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_video(file_data, models, text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "159fe88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatched number of tokens/words: 01GWGmg5jn8_50017\n"
     ]
    }
   ],
   "source": [
    "process_single_file(args, split_dirs, models, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video.from_file(\"OUT.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "98120ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video': {'fps': [25.0], 'duration': [5.52]},\n",
       " 'audio': {'framerate': [16000.0], 'duration': [5.52]}}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f39a2b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'i', 'start': 0.08, 'end': 0.46},\n",
       " {'text': 'waited', 'start': 1.14, 'end': 1.43},\n",
       " {'text': 'until', 'start': 1.43, 'end': 1.63},\n",
       " {'text': 'i', 'start': 1.63, 'end': 1.64},\n",
       " {'text': 'was', 'start': 1.64, 'end': 1.71},\n",
       " {'text': 'in', 'start': 1.71, 'end': 1.77},\n",
       " {'text': 'my', 'start': 1.77, 'end': 1.84},\n",
       " {'text': 'late', 'start': 1.84, 'end': 2.04},\n",
       " {'text': '20s', 'start': 2.04, 'end': 2.47}]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_data['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1891b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import VideoReader\n",
    "reader = VideoReader(video_path)\n",
    "metadata = reader.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "89f9ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f962ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m dirs, splits \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprepare_directory_structure(dataset_dir, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], video\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m video_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m video_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m      9\u001b[0m fn \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(video_files[\u001b[38;5;241m0\u001b[39m]))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/glob.py:28\u001b[0m, in \u001b[0;36mglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, include_hidden)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mglob\u001b[39m(pathname, \u001b[38;5;241m*\u001b[39m, root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dir_fd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m         include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/glob.py:97\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, root_dir, dir_fd, recursive, dironly, include_hidden)\u001b[0m\n\u001b[1;32m     95\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minclude_hidden\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_hidden\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name)\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/glob.py:106\u001b[0m, in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dir_fd, dironly, include_hidden)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_glob1\u001b[39m(dirname, pattern, dir_fd, dironly, include_hidden\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 106\u001b[0m     names \u001b[38;5;241m=\u001b[39m \u001b[43m_listdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[1;32m    108\u001b[0m         names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m include_hidden \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/glob.py:178\u001b[0m, in \u001b[0;36m_listdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_listdir\u001b[39m(dirname, dir_fd, dironly):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(_iterdir(dirname, dir_fd, dironly)) \u001b[38;5;28;01mas\u001b[39;00m it:\n\u001b[0;32m--> 178\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/glob.py:167\u001b[0m, in \u001b[0;36m_iterdir\u001b[0;34m(dirname, dir_fd, dironly)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m fsencode(entry\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'lrs3')\n",
    "\n",
    "# Prepare directory structure --> only this script is needed for video\n",
    "dirs, splits = utils.prepare_directory_structure(dataset_dir, ['train'], video=True)\n",
    "\n",
    "video_dir = os.path.join(dataset_dir, 'video', 'train')\n",
    "video_files = sorted(glob.glob(os.path.join(video_dir, \"*.mp4\"), recursive=True))\n",
    "\n",
    "fn = os.path.splitext(os.path.basename(video_files[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "1478b55e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'extract_dataset_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[297], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/dartfs/rc/lab/F/FinnLab/code/modeling/preproc-datasets/feature-extraction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mextract_dataset_features\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     initialize_models,\n\u001b[1;32m     19\u001b[0m     load_file_data,\n\u001b[1;32m     20\u001b[0m     process_video\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'extract_dataset_features'"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import argparse\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision.io import read_video\n",
    "import shutil\n",
    "# from transformers import AutoImageProcessor, AutoProcessor, AutoModel, AutoTokenizer\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/code/modeling/preproc-datasets/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/code/modeling/preproc-datasets/feature-extraction')\n",
    "\n",
    "import utils\n",
    "from extract_dataset_features import (\n",
    "    initialize_models,\n",
    "    load_file_data,\n",
    "    process_video\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bfddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b9cff4",
   "metadata": {},
   "source": [
    "### Classify the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fa2db90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0cf83c776643ee968a931438ec54d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef49134510842f3bc4d97068271376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/6.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473d6a7e1c06458a8d91687ff7a45249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "model_id = \"facebook/mms-lid-256\"\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7ab8a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = os.path.join(DATASETS_DIR, 'nlp-datasets', 'avspeech')\n",
    "\n",
    "all_fns = [os.path.join(root, f) for root, dirs, files in os.walk(dataset_dir) for f in files if f.endswith(\".mp4\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9092715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "11382a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(all_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "a471edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_audio = []\n",
    "for x in all_fns[:20]:\n",
    "    waveform, audio_sr = utils.load_audio(x)\n",
    "    waveform, audio_sr = utils.resample_audio(waveform, orig_sr=audio_sr, target_sr=16000)\n",
    "    all_audio.append(waveform.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "34dbac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English\n",
    "inputs = processor(all_audio, sampling_rate=audio_sr, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "\n",
    "lang_id = torch.argmax(outputs, dim=-1)[0].item()\n",
    "detected_lang = model.config.id2label[lang_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9b746e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_ids = torch.argmax(outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "93428650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xab.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xac.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xad.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xae.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xaf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xag.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xah.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xai.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xaj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xak.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xal.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xam.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xan.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xao.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xap.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xaq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xar.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xas.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xat.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xau.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xav.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xaw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xax.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xay.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xaz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xba.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbe.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xby.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xbz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xca.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xce.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xch.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xci.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xck.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xco.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xct.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xcz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xda.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xde.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xds.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xdz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xea.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xec.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xed.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xee.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xef.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xei.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xej.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xek.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xel.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xem.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xen.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xep.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xer.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xes.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xet.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xeu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xev.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xew.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xex.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xey.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xez.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfa.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfe.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xff.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xft.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xfz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xga.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xge.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xgz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xha.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhe.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xho.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xht.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xhz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xia.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xib.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xic.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xid.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xie.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xif.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xig.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xih.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xii.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xij.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xik.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xil.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xim.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xin.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xio.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xip.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xir.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xis.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xit.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xix.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xiz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xja.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xje.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xji.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjs.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xju.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xjz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xka.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xke.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xki.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xko.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xks.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xku.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xky.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xkz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xla.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xld.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xle.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xli.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xll.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xln.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xls.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xly.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xlz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xma.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xme.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xml.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xms.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xmz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xna.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xne.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xng.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xni.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnk.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnl.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnm.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnn.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xno.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnp.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnr.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xns.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnt.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnu.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnv.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnw.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnx.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xny.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xnz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoa.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xob.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xod.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoe.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xof.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xog.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoh.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoj.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xok.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xol.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xom.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xon.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoo.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xop.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoq.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xor.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xos.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xot.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xou.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xov.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xow.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xox.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoy.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xoz.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpa.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpb.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpc.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpd.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpe.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpf.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpg.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xph.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpi.tar',\n",
       " '/dartfs/rc/lab/F/FinnLab/datasets/nlp-datasets/avspeech/clips/xpj.tar']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob(os.path.join(dataset_dir, 'clips', '*.tar')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "284baca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "por\n",
      "eng\n",
      "por\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "epo\n",
      "bos\n",
      "eng\n",
      "eng\n",
      "eng\n",
      "deu\n"
     ]
    }
   ],
   "source": [
    "for id in lang_ids:\n",
    "    print (model.config.id2label[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "88129a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_count': 12354,\n",
       " 'description': '\\u3000http://yboki.com/\\n\\u3000',\n",
       " 'format': {'tags': {'major_brand': 'mp42',\n",
       "   'compatible_brands': 'isommp42',\n",
       "   'minor_version': '0'},\n",
       "  'start_time': 0.0,\n",
       "  'nb_streams': 2,\n",
       "  'format_name': ['3gp', '3g2', 'mov', 'mp4', 'mj2', 'm4a'],\n",
       "  'bit_rate': 1116993,\n",
       "  'nb_programs': 0,\n",
       "  'duration': 1044.3639,\n",
       "  'probe_score': 100,\n",
       "  'size': 145818419},\n",
       " 'video_id': 'w0Q5gH4hb7I',\n",
       " 'creation_time': '2016-08-27T15:10:23.000000Z',\n",
       " 'height': 720,\n",
       " 'dislike_count': 2,\n",
       " 'channel_id': 'UCZ-aFwJHTPY3gLQYN2S-Y3Q',\n",
       " 'like_count': 30,\n",
       " 'subtitles': {},\n",
       " 'duration': 1044,\n",
       " 'title': '\\u3000',\n",
       " 'tags': ['', '', '', ''],\n",
       " 'width': 1280,\n",
       " 'categories': ['Education']}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj['metadata'].iloc[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
