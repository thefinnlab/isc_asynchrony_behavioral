{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71db0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /dartfs-hpc/rc/home/w/f003rjw/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import os, sys, glob\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/utils/gentle')\n",
    "\n",
    "import gentle\n",
    "from config import *\n",
    "from preproc_utils import create_balanced_orders, get_consecutive_list_idxs, sort_consecutive_constraint, check_consecutive_spacing\n",
    "\n",
    "# from text_utils import get_pos_tags, get_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0e456",
   "metadata": {},
   "source": [
    "# Set directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d50ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/'\n",
    "stim_dir = os.path.join(base_dir, 'stimuli')\n",
    "cache_dir = os.path.join('/dartfs/rc/lab/F/FinnLab/tommy/', 'models')\n",
    "\n",
    "gentle_dir = os.path.join(stim_dir, 'gentle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "4abc0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(model_dir, model_name, task, window_size, top_n):\n",
    "    '''\n",
    "    Loads model data from directory\n",
    "    '''\n",
    "\n",
    "    model_dir = os.path.join(model_dir, task, model_name, f'window-size-{window_size}')\n",
    "    results_fn = natsorted(glob.glob(os.path.join(model_dir, f'*top-{top_n}*')))[0]\n",
    "\n",
    "    # load the data, remove nans\n",
    "    model_results = pd.read_csv(results_fn)\n",
    "    model_results['glove_continuous_accuracy'] = model_results['glove_continuous_accuracy'].apply(np.nan_to_num)\n",
    "    model_results['word2vec_continuous_accuracy'] = model_results['word2vec_continuous_accuracy'].apply(np.nan_to_num)\n",
    "\n",
    "    return model_results\n",
    "\n",
    "def get_stim_candidate_idxs(task):\n",
    "    '''\n",
    "    Find the NWP candidate indices of a preprocessed transcript\n",
    "    '''\n",
    "\n",
    "    preproc_fn = os.path.join(STIM_DIR, 'preprocessed', task, f'{task}_transcript-preprocessed.csv')\n",
    "    df_preproc = pd.read_csv(preproc_fn)\n",
    "    nwp_idxs = np.where(df_preproc['NWP_Candidate'])[0]\n",
    "\n",
    "    return df_preproc, nwp_idxs\n",
    "\n",
    "def divide_nwp_dataframe(df, accuracy_type, percentile):\n",
    "\n",
    "    df_divide = df.copy()\n",
    "\n",
    "    # first find the lowest and highest percentile for entropy\n",
    "    low_entropy_idxs = df['entropy'] < np.nanpercentile(df['entropy'], percentile)\n",
    "    high_entropy_idxs = df['entropy'] >= np.nanpercentile(df['entropy'], 100-percentile)\n",
    "\n",
    "    ## set names for entropy group\n",
    "    df_divide.loc[low_entropy_idxs, 'entropy_group'] = 'low'\n",
    "    df_divide.loc[high_entropy_idxs, 'entropy_group'] = 'high'\n",
    "\n",
    "    # repeat for continuous accuracy\n",
    "    low_accuracy_idxs = df[accuracy_type] < np.nanpercentile(df[accuracy_type], percentile)\n",
    "    high_accuracy_idxs = df[accuracy_type] >= np.nanpercentile(df[accuracy_type], 100-percentile)\n",
    "\n",
    "    ## set names for accuracy group\n",
    "    df_divide.loc[low_accuracy_idxs, 'accuracy_group'] = 'low'\n",
    "    df_divide.loc[high_accuracy_idxs, 'accuracy_group'] = 'high'\n",
    "\n",
    "    return df_divide.dropna()\n",
    "\n",
    "def get_quadrant_distributions(df_divide, indices):\n",
    "    \n",
    "    df_idx = df_divide.loc[indices]\n",
    "    \n",
    "    # get the items as a dictionary for passing out to aggregate\n",
    "    quadrant_dist = {f'{labels[0]}-entropy_{labels[1]}-accuracy': round(len(df)/len(df_idx), 2) \n",
    "                 for labels, df in df_idx.groupby(['entropy_group', 'accuracy_group'])}\n",
    "\n",
    "    df_quadrants = pd.DataFrame.from_dict(quadrant_dist, orient='index').T\n",
    "    \n",
    "    return df_quadrants\n",
    "\n",
    "def select_prediction_words(df_divide, remove_perc, select_perc, min_spacing_thresh=3):\n",
    "    '''\n",
    "    \n",
    "    df_divide: candidate words divided into quartiles based on entropy and accuracy\n",
    "    \n",
    "    remove_perc: percentage of words to remove based on proximity to other words\n",
    "        helps ensure decent spacing between presented words\n",
    "        \n",
    "    select_perc: percentage of words to select for presentation    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_divide['spacing'] = np.hstack([np.nan, np.diff(df_divide.index)])\n",
    "    \n",
    "    quadrant_distributions = get_quadrant_distributions(df_divide, df_divide.index).to_numpy()\n",
    "    \n",
    "    updated = []\n",
    "\n",
    "    for i, df in df_divide.groupby(['entropy_group', 'accuracy_group']):\n",
    "        # find how many words to remove in the quadrant based on the percent\n",
    "        n_words = round(remove_perc * len(df))\n",
    "        df = df.sort_values(by='spacing').iloc[n_words:]\n",
    "        updated.append(df.sort_index())\n",
    "\n",
    "    updated = pd.concat(updated).sort_index()\n",
    "    updated_distributions = get_quadrant_distributions(updated, updated.index).to_numpy()\n",
    "    assert (np.isclose(quadrant_distributions, updated_distributions, atol=0.01).all())\n",
    "    \n",
    "    # make sure it is scaled to the original dataframe\n",
    "    select_perc = select_perc/(1-remove_perc)\n",
    "    min_spacing = 0\n",
    "    RANDOM_STATE = 0\n",
    "    \n",
    "    print (f'Selecting {select_perc*100:.2f}% of remaining items')\n",
    "    \n",
    "    while (min_spacing < min_spacing_thresh):\n",
    "        # now sample the words from each quadrant\n",
    "        sampled = []\n",
    "\n",
    "        for i, df in updated.groupby(['entropy_group', 'accuracy_group']):\n",
    "\n",
    "            df_sampled = df.sample(frac=select_perc, random_state=RANDOM_STATE).sort_index()\n",
    "            sampled.append((len(df_sampled), df_sampled))\n",
    "\n",
    "        n_sampled, sampled = zip(*sampled)\n",
    "        sampled = pd.concat(sampled).sort_index()\n",
    "\n",
    "        min_spacing = np.diff(sampled.index).min()\n",
    "        \n",
    "        RANDOM_STATE += 1\n",
    "    \n",
    "    print (f'Min spacing of {min_spacing}')\n",
    "    print (f'{len(sampled)} total words')\n",
    "\n",
    "    return sampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "id": "f8e12786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 80.00% of remaining items\n",
      "Min spacing of 3\n",
      "216 total words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12795/1665222949.py:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_entropy_idxs, 'entropy_group'] = 'low'\n",
      "/tmp/ipykernel_12795/1665222949.py:44: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_accuracy_idxs, 'accuracy_group'] = 'low'\n"
     ]
    }
   ],
   "source": [
    "all_tasks_quadrants = []\n",
    "\n",
    "models_dir = os.path.join(DERIVATIVES_DIR, 'model-predictions')\n",
    "model_name = 'gpt2-xl'\n",
    "task = 'wheretheressmoke'\n",
    "\n",
    "df_preproc, candidate_idxs = get_stim_candidate_idxs(task)\n",
    "\n",
    "model_results = load_model_data(models_dir, model_name=model_name, task=task, top_n=5, window_size=100)\n",
    "model_results.loc[:, 'binary_accuracy'] = model_results['binary_accuracy'].astype(bool)\n",
    "model_results = model_results.iloc[candidate_idxs]\n",
    "\n",
    "df_divide = divide_nwp_dataframe(model_results, accuracy_type='word2vec_continuous_accuracy', percentile=45)\n",
    "\n",
    "\n",
    "df_selected = select_prediction_words(df_divide, remove_perc=0.5, select_perc=0.4, min_spacing_thresh=2)\n",
    "\n",
    "# fig, axes, df_quadrants = plot_quadrant_distributions(model_results.dropna(), 'word2vec_continuous_accuracy', 45)\n",
    "\n",
    "# plt.suptitle(f'{model_name} - task {task}')\n",
    "# out_fn = os.path.join(out_dir, f'{model_name}-{task}_quadrant-distributions.jpg')\n",
    "\n",
    "# plt.savefig(out_fn, dpi=300)\n",
    "# plt.close('all')\n",
    "\n",
    "# df_quadrants['model_name'] = model_name\n",
    "# all_tasks_quadrants.append(df_quadrants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "ff920d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   4,   11,   23,   30,   34,   38,   47,   51,   61,   64,\n",
       "       ...\n",
       "       1710, 1722, 1732, 1738, 1745, 1758, 1762, 1772, 1783, 1817],\n",
       "      dtype='int64', length=216)"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "id": "0870c22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Written</th>\n",
       "      <th>Case</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop_Word</th>\n",
       "      <th>Word_Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>NWP_Candidate</th>\n",
       "      <th>entropy_group</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.127781</td>\n",
       "      <td>0.115309</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reached</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>reached</td>\n",
       "      <td>0.127781</td>\n",
       "      <td>0.493847</td>\n",
       "      <td>0.366067</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>over</td>\n",
       "      <td>0.493847</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.466470</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>success</td>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>and</td>\n",
       "      <td>1.539002</td>\n",
       "      <td>1.661162</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secretly</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>secretly</td>\n",
       "      <td>1.664915</td>\n",
       "      <td>2.377098</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>590.470522</td>\n",
       "      <td>590.611418</td>\n",
       "      <td>0.140897</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>still</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>still</td>\n",
       "      <td>590.611418</td>\n",
       "      <td>590.999320</td>\n",
       "      <td>0.387902</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>miss</td>\n",
       "      <td>success</td>\n",
       "      <td>VBP</td>\n",
       "      <td>verb, present tense, not 3rd person singular</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>miss</td>\n",
       "      <td>590.999320</td>\n",
       "      <td>591.188889</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>the</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>591.188889</td>\n",
       "      <td>591.265763</td>\n",
       "      <td>0.076874</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>smoking</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>smoking</td>\n",
       "      <td>591.265763</td>\n",
       "      <td>591.747619</td>\n",
       "      <td>0.481856</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word_Written     Case  POS                                POS_Definition  \\\n",
       "0               I  success  PRP                             pronoun, personal   \n",
       "1         reached  success  VBD                              verb, past tense   \n",
       "2            over  success   RB                                        adverb   \n",
       "3             and  success   CC                     conjunction, coordinating   \n",
       "4        secretly  success   RB                                        adverb   \n",
       "...           ...      ...  ...                                           ...   \n",
       "1822            I  success  PRP                             pronoun, personal   \n",
       "1823        still  success   RB                                        adverb   \n",
       "1824         miss  success  VBP  verb, present tense, not 3rd person singular   \n",
       "1825          the  success   DT                                    determiner   \n",
       "1826      smoking  success   NN                noun, common, singular or mass   \n",
       "\n",
       "     Punctuation  Stop_Word Word_Vocab       Onset      Offset  Duration  \\\n",
       "0                      True          I    0.012472    0.127781  0.115309   \n",
       "1                     False    reached    0.127781    0.493847  0.366067   \n",
       "2                      True       over    0.493847    0.960317  0.466470   \n",
       "3                      True        and    1.539002    1.661162  0.122160   \n",
       "4                     False   secretly    1.664915    2.377098  0.712183   \n",
       "...          ...        ...        ...         ...         ...       ...   \n",
       "1822                   True          I  590.470522  590.611418  0.140897   \n",
       "1823                  False      still  590.611418  590.999320  0.387902   \n",
       "1824                  False       miss  590.999320  591.188889  0.189569   \n",
       "1825                   True        the  591.188889  591.265763  0.076874   \n",
       "1826           .      False    smoking  591.265763  591.747619  0.481856   \n",
       "\n",
       "      Named_Entity  NWP_Candidate entropy_group accuracy_group  \n",
       "0            False          False           NaN            NaN  \n",
       "1            False           True           NaN            NaN  \n",
       "2            False          False           NaN            NaN  \n",
       "3            False          False           NaN            NaN  \n",
       "4            False           True          high            low  \n",
       "...            ...            ...           ...            ...  \n",
       "1822         False          False           NaN            NaN  \n",
       "1823         False           True           NaN            NaN  \n",
       "1824         False           True           NaN            NaN  \n",
       "1825         False          False           NaN            NaN  \n",
       "1826         False           True           NaN            NaN  \n",
       "\n",
       "[1827 rows x 14 columns]"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "028e9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_idxs = df_selected.index\n",
    "\n",
    "df_preproc.loc[selected_idxs, ['entropy_group', 'accuracy_group']] = df_selected[['entropy_group', 'accuracy_group']]\n",
    "df_preproc.loc[selected_idxs, 'Selected_CaNndidate'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "af66ec4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "107a4e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "d301c3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Written</th>\n",
       "      <th>Case</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop_Word</th>\n",
       "      <th>Word_Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>NWP_Candidate</th>\n",
       "      <th>entropy_group</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secretly</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>secretly</td>\n",
       "      <td>1.664915</td>\n",
       "      <td>2.377098</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>foot</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>foot</td>\n",
       "      <td>5.061272</td>\n",
       "      <td>5.370295</td>\n",
       "      <td>0.309022</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>door</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>door</td>\n",
       "      <td>8.094104</td>\n",
       "      <td>8.453288</td>\n",
       "      <td>0.359184</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>shoes</td>\n",
       "      <td>success</td>\n",
       "      <td>NNS</td>\n",
       "      <td>noun, common, plural</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>shoes</td>\n",
       "      <td>11.675964</td>\n",
       "      <td>12.006014</td>\n",
       "      <td>0.330050</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>crying</td>\n",
       "      <td>success</td>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, present participle or gerund</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>crying</td>\n",
       "      <td>12.693651</td>\n",
       "      <td>13.252381</td>\n",
       "      <td>0.558730</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>seven</td>\n",
       "      <td>success</td>\n",
       "      <td>CD</td>\n",
       "      <td>numeral, cardinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>seven</td>\n",
       "      <td>561.777047</td>\n",
       "      <td>562.144223</td>\n",
       "      <td>0.367176</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>way</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>way</td>\n",
       "      <td>563.132653</td>\n",
       "      <td>563.302268</td>\n",
       "      <td>0.169615</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>think</td>\n",
       "      <td>success</td>\n",
       "      <td>VBP</td>\n",
       "      <td>verb, present tense, not 3rd person singular</td>\n",
       "      <td>, \"</td>\n",
       "      <td>False</td>\n",
       "      <td>think</td>\n",
       "      <td>567.482766</td>\n",
       "      <td>567.745516</td>\n",
       "      <td>0.262750</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>held</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>held</td>\n",
       "      <td>573.289569</td>\n",
       "      <td>573.588889</td>\n",
       "      <td>0.299320</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>quit</td>\n",
       "      <td>success</td>\n",
       "      <td>VB</td>\n",
       "      <td>verb, base form</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>quit</td>\n",
       "      <td>587.417460</td>\n",
       "      <td>587.653056</td>\n",
       "      <td>0.235595</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word_Written     Case  POS                                POS_Definition  \\\n",
       "4        secretly  success   RB                                        adverb   \n",
       "11           foot  success   NN                noun, common, singular or mass   \n",
       "23           door  success   NN                noun, common, singular or mass   \n",
       "30          shoes  success  NNS                          noun, common, plural   \n",
       "34         crying  success  VBG            verb, present participle or gerund   \n",
       "...           ...      ...  ...                                           ...   \n",
       "1758        seven  success   CD                             numeral, cardinal   \n",
       "1762          way  success   NN                noun, common, singular or mass   \n",
       "1772        think  success  VBP  verb, present tense, not 3rd person singular   \n",
       "1783         held  success  VBD                              verb, past tense   \n",
       "1817         quit  success   VB                               verb, base form   \n",
       "\n",
       "     Punctuation  Stop_Word Word_Vocab       Onset      Offset  Duration  \\\n",
       "4                     False   secretly    1.664915    2.377098  0.712183   \n",
       "11                    False       foot    5.061272    5.370295  0.309022   \n",
       "23                    False       door    8.094104    8.453288  0.359184   \n",
       "30                    False      shoes   11.675964   12.006014  0.330050   \n",
       "34            ,       False     crying   12.693651   13.252381  0.558730   \n",
       "...          ...        ...        ...         ...         ...       ...   \n",
       "1758                  False      seven  561.777047  562.144223  0.367176   \n",
       "1762                  False        way  563.132653  563.302268  0.169615   \n",
       "1772         , \"      False      think  567.482766  567.745516  0.262750   \n",
       "1783                  False       held  573.289569  573.588889  0.299320   \n",
       "1817                  False       quit  587.417460  587.653056  0.235595   \n",
       "\n",
       "      Named_Entity  NWP_Candidate entropy_group accuracy_group  \n",
       "4            False           True          high            low  \n",
       "11           False           True          high            low  \n",
       "23           False           True           low           high  \n",
       "30           False           True           low            low  \n",
       "34           False           True          high            low  \n",
       "...            ...            ...           ...            ...  \n",
       "1758         False           True           low            low  \n",
       "1762         False           True           low           high  \n",
       "1772         False           True          high           high  \n",
       "1783         False           True          high            low  \n",
       "1817         False           True           low           high  \n",
       "\n",
       "[216 rows x 14 columns]"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preproc.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8896a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_chunks(lst, n, shuffle=False):\n",
    "    \"\"\"Created randomized n-sized chunks from lst.\"\"\"\n",
    "    \n",
    "    tmp_lst = lst.copy()\n",
    "    n_total = len(lst)\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(tmp_lst)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for i in range(0, len(tmp_lst), n):\n",
    "        all_chunks.append(tmp_lst[i:i + n])\n",
    "    \n",
    "    # distribute remaining items across orders\n",
    "    if len(all_chunks) != n_total//n:\n",
    "        remainder = all_chunks.pop()\n",
    "        \n",
    "        for i, item in enumerate(remainder):      \n",
    "            all_chunks[i%n].append(item)\n",
    "    \n",
    "    # lastly sort for ordered indices\n",
    "    all_chunks = [sorted(chunk) for chunk in all_chunks]\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "57343178",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029556650246305417\n",
      "0.029556650246305417\n",
      "0.029556650246305417\n",
      "0.029556650246305417\n"
     ]
    }
   ],
   "source": [
    "for order in test_orders:\n",
    "    print (len(order)/len(df_preproc))\n",
    "\n",
    "# len(test_orders[0])/len(df_preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "1541e781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17, 0.31, 0.34, 0.18]])"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadrant_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "c2a14ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17, 0.31, 0.34, 0.18]])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadrant_distribution = get_quadrant_distributions(df_selected, df_selected.index).to_numpy()\n",
    "quadrant_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "e37feb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #10\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #11\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #12\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "starting\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n"
     ]
    }
   ],
   "source": [
    "# percent_sampled = 0.\n",
    "n_orders = 4\n",
    "n_participants_per_item = 50\n",
    "consecutive_spacing = 10\n",
    "\n",
    "# find distribution of selected words from the divided quadrants\n",
    "quadrant_distribution = get_quadrant_distributions(df_divide, df_selected.index).to_numpy()\n",
    "deviation_threshold = 0.05\n",
    "order_distributions = np.zeros((n_orders, 4))\n",
    "\n",
    "# find indices for presentation and set number of items each subject sees\n",
    "nwp_indices = sorted(df_selected.index)\n",
    "\n",
    "# # Find lists with consecutive items violating our constraint\n",
    "\n",
    "while not (np.allclose(quadrant_distribution, order_distributions, atol=deviation_threshold)):\n",
    "    \n",
    "    subject_experiment_orders = random_chunks(nwp_indices, len(nwp_indices)//n_orders, shuffle=True)\n",
    "    \n",
    "    print ('starting')\n",
    "#     test_orders = subject_experiment_orders.copy()\n",
    "    idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_spacing=consecutive_spacing)\n",
    "    subject_experiment_orders = sort_consecutive_constraint(subject_experiment_orders, consecutive_spacing=consecutive_spacing)\n",
    "    \n",
    "    \n",
    "    order_distributions = [get_quadrant_distributions(df_divide, order).to_numpy() for order in subject_experiment_orders]\n",
    "    \n",
    "    # sometimes the randomized order makes a quadrant be dropped --> reset and try again\n",
    "    if not all([order.shape[-1] == 4 for order in order_distributions]):\n",
    "        order_distributions = np.zeros((n_orders, 4))\n",
    "# # Test again once we have completed resorting\n",
    "# idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_spacing=p.consecutive_spacing)\n",
    "# print (f'Lists violating consecutive index constraint: {100*(len(idxs))/len(subject_experiment_orders)}%')\n",
    "\n",
    "# uniq, counts = np.unique(subject_experiment_orders, return_counts=True)\n",
    "# print (f'All counts per word: {np.sum(counts >= p.n_participants_per_item) / len(counts)*100}%')\n",
    "\n",
    "# counts = Counter(tuple(o) for o in subject_experiment_orders)\n",
    "# unique_orders = np.sum([v for k, v in counts.items()]) / len(counts)\n",
    "\n",
    "# print (f'Unique orders: {unique_orders*100}%')\n",
    "\n",
    "# orders_meeting_consecutive = np.sum([check_consecutive_spacing(order, consecutive_spacing=p.consecutive_spacing) for order in subject_experiment_orders]) / len(subject_experiment_orders)\n",
    "# print (f'Consecutive constraint: {orders_meeting_consecutive*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a428b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the items as a dictionary for passing out to aggregate\n",
    "quadrant_dist = {f'{labels[0]}-entropy_{labels[1]}-accuracy': round(len(df)/len(df_divide), 2) \n",
    "             for labels, df in df_divide.groupby(['entropy_group', 'accuracy_group'])}\n",
    "\n",
    "df_quadrants = pd.DataFrame.from_dict(quadrant_dist, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "87267151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.15, 0.3 , 0.4 , 0.14]]),\n",
       " array([[0.13, 0.4 , 0.33, 0.14]]),\n",
       " array([[0.15, 0.34, 0.38, 0.12]]),\n",
       " array([[0.13, 0.33, 0.36, 0.18]])]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_random_orders(n_orders, n_participants_per_item, consecutive_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "170031b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_quadrant_distributions(df_divide, order).to_numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f296b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12,\n",
       "  89,\n",
       "  95,\n",
       "  158,\n",
       "  169,\n",
       "  202,\n",
       "  282,\n",
       "  355,\n",
       "  390,\n",
       "  408,\n",
       "  425,\n",
       "  463,\n",
       "  496,\n",
       "  504,\n",
       "  535,\n",
       "  582,\n",
       "  597,\n",
       "  608,\n",
       "  691,\n",
       "  696,\n",
       "  742,\n",
       "  822,\n",
       "  904,\n",
       "  997,\n",
       "  1024,\n",
       "  1032,\n",
       "  1065,\n",
       "  1081,\n",
       "  1105,\n",
       "  1110,\n",
       "  1222,\n",
       "  1227,\n",
       "  1235,\n",
       "  1245,\n",
       "  1282,\n",
       "  1302,\n",
       "  1307,\n",
       "  1345,\n",
       "  1447,\n",
       "  1486,\n",
       "  1565,\n",
       "  1617,\n",
       "  1637,\n",
       "  1722,\n",
       "  1798],\n",
       " [17,\n",
       "  73,\n",
       "  99,\n",
       "  114,\n",
       "  204,\n",
       "  253,\n",
       "  261,\n",
       "  323,\n",
       "  329,\n",
       "  351,\n",
       "  531,\n",
       "  605,\n",
       "  610,\n",
       "  646,\n",
       "  661,\n",
       "  701,\n",
       "  738,\n",
       "  762,\n",
       "  795,\n",
       "  802,\n",
       "  813,\n",
       "  839,\n",
       "  896,\n",
       "  902,\n",
       "  955,\n",
       "  1103,\n",
       "  1136,\n",
       "  1162,\n",
       "  1259,\n",
       "  1290,\n",
       "  1371,\n",
       "  1411,\n",
       "  1443,\n",
       "  1448,\n",
       "  1459,\n",
       "  1484,\n",
       "  1513,\n",
       "  1551,\n",
       "  1607,\n",
       "  1701,\n",
       "  1710,\n",
       "  1735,\n",
       "  1747,\n",
       "  1754],\n",
       " [144,\n",
       "  151,\n",
       "  187,\n",
       "  198,\n",
       "  217,\n",
       "  251,\n",
       "  292,\n",
       "  324,\n",
       "  376,\n",
       "  475,\n",
       "  536,\n",
       "  548,\n",
       "  578,\n",
       "  606,\n",
       "  681,\n",
       "  713,\n",
       "  749,\n",
       "  786,\n",
       "  806,\n",
       "  873,\n",
       "  933,\n",
       "  940,\n",
       "  980,\n",
       "  987,\n",
       "  1044,\n",
       "  1087,\n",
       "  1102,\n",
       "  1154,\n",
       "  1172,\n",
       "  1231,\n",
       "  1255,\n",
       "  1278,\n",
       "  1327,\n",
       "  1354,\n",
       "  1367,\n",
       "  1409,\n",
       "  1438,\n",
       "  1451,\n",
       "  1571,\n",
       "  1579,\n",
       "  1610,\n",
       "  1629,\n",
       "  1655,\n",
       "  1765],\n",
       " [11,\n",
       "  23,\n",
       "  103,\n",
       "  145,\n",
       "  153,\n",
       "  161,\n",
       "  188,\n",
       "  227,\n",
       "  248,\n",
       "  332,\n",
       "  372,\n",
       "  385,\n",
       "  406,\n",
       "  453,\n",
       "  552,\n",
       "  571,\n",
       "  601,\n",
       "  650,\n",
       "  685,\n",
       "  699,\n",
       "  718,\n",
       "  766,\n",
       "  809,\n",
       "  817,\n",
       "  865,\n",
       "  901,\n",
       "  975,\n",
       "  1030,\n",
       "  1056,\n",
       "  1068,\n",
       "  1161,\n",
       "  1193,\n",
       "  1276,\n",
       "  1285,\n",
       "  1331,\n",
       "  1355,\n",
       "  1375,\n",
       "  1476,\n",
       "  1533,\n",
       "  1542,\n",
       "  1590,\n",
       "  1662,\n",
       "  1733,\n",
       "  1774]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "43c33632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.13, 0.28, 0.38, 0.2 ]]), array([[0.2 , 0.35, 0.3 , 0.15]])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ba1a568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.17, 0.35, 0.29, 0.19]]), array([[0.17, 0.28, 0.39, 0.16]])]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "351ef407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(test_orders[0]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
