{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c71db0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import os, sys, glob\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/utils/gentle')\n",
    "\n",
    "import gentle\n",
    "from config import *\n",
    "import preproc_utils as preproc\n",
    "# from preproc_utils import create_balanced_orders, get_consecutive_list_idxs, sort_consecutive_constraint, check_consecutive_spacing\n",
    "\n",
    "# from text_utils import get_pos_tags, get_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0e456",
   "metadata": {},
   "source": [
    "# Set directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d50ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/'\n",
    "stim_dir = os.path.join(base_dir, 'stimuli')\n",
    "cache_dir = os.path.join('/dartfs/rc/lab/F/FinnLab/tommy/', 'models')\n",
    "\n",
    "gentle_dir = os.path.join(stim_dir, 'gentle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4abc0b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_data(model_dir, model_name, task, window_size, top_n):\n",
    "    '''\n",
    "    Loads model data from directory\n",
    "    '''\n",
    "\n",
    "    model_dir = os.path.join(model_dir, task, model_name, f'window-size-{window_size}')\n",
    "    results_fn = natsorted(glob.glob(os.path.join(model_dir, f'*top-{top_n}*')))[0]\n",
    "\n",
    "    # load the data, remove nans\n",
    "    model_results = pd.read_csv(results_fn)\n",
    "#     model_results['glove_continuous_accuracy'] = model_results['glove_continuous_accuracy'].apply(np.nan_to_num)\n",
    "#     model_results['word2vec_continuous_accuracy'] = model_results['word2vec_continuous_accuracy'].apply(np.nan_to_num)\n",
    "\n",
    "    model_results['glove_avg_accuracy'] = model_results['glove_avg_accuracy'] #.apply(np.nan_to_num)\n",
    "    model_results['word2vec_avg_accuracy'] = model_results['word2vec_avg_accuracy'] #.apply(np.nan_to_num)\n",
    "    model_results['fasttext_avg_accuracy'] = model_results['fasttext_avg_accuracy'] #.apply(np.nan_to_num)\n",
    "\n",
    "    model_results['glove_max_accuracy'] = model_results['glove_max_accuracy'] #.apply(np.nan_to_num)\n",
    "    model_results['word2vec_max_accuracy'] = model_results['word2vec_max_accuracy'] #.apply(np.nan_to_num)\n",
    "    model_results['fasttext_max_accuracy'] = model_results['fasttext_max_accuracy'] #.apply(np.nan_to_num)\n",
    "\n",
    "    return model_results\n",
    "\n",
    "def get_stim_candidate_idxs(task):\n",
    "    '''\n",
    "    Find the NWP candidate indices of a preprocessed transcript\n",
    "    '''\n",
    "\n",
    "    preproc_fn = os.path.join(STIM_DIR, 'preprocessed', task, f'{task}_transcript-preprocessed.csv')\n",
    "    df_preproc = pd.read_csv(preproc_fn)\n",
    "    nwp_idxs = np.where(df_preproc['NWP_Candidate'])[0]\n",
    "\n",
    "    return df_preproc, nwp_idxs\n",
    "\n",
    "def divide_nwp_dataframe(df, accuracy_type, percentile):\n",
    "\n",
    "    df_divide = df.copy()\n",
    "\n",
    "    # first find the lowest and highest percentile for entropy\n",
    "    low_entropy_idxs = df['entropy'] < np.nanpercentile(df['entropy'], percentile)\n",
    "    high_entropy_idxs = df['entropy'] >= np.nanpercentile(df['entropy'], 100-percentile)\n",
    "\n",
    "    ## set names for entropy group\n",
    "    df_divide.loc[low_entropy_idxs, 'entropy_group'] = 'low'\n",
    "    df_divide.loc[high_entropy_idxs, 'entropy_group'] = 'high'\n",
    "\n",
    "    # repeat for continuous accuracy\n",
    "    low_accuracy_idxs = df[accuracy_type] < np.nanpercentile(df[accuracy_type], percentile)\n",
    "    high_accuracy_idxs = df[accuracy_type] >= np.nanpercentile(df[accuracy_type], 100-percentile)\n",
    "\n",
    "    ## set names for accuracy group\n",
    "    df_divide.loc[low_accuracy_idxs, 'accuracy_group'] = 'low'\n",
    "    df_divide.loc[high_accuracy_idxs, 'accuracy_group'] = 'high'\n",
    "\n",
    "    return df_divide #.dropna()\n",
    "\n",
    "# def get_quadrant_distributions(df_divide, indices):\n",
    "    \n",
    "#     df_idx = df_divide.loc[indices]\n",
    "    \n",
    "#     # get the items as a dictionary for passing out to aggregate\n",
    "#     quadrant_dist = {f'{labels[0]}-entropy_{labels[1]}-accuracy': round(len(df)/len(df_idx), 2) \n",
    "#                  for labels, df in df_idx.groupby(['entropy_group', 'accuracy_group'])}\n",
    "\n",
    "#     df_quadrants = pd.DataFrame.from_dict(quadrant_dist, orient='index').T\n",
    "    \n",
    "#     return df_quadrants\n",
    "\n",
    "def select_prediction_words(df_divide, remove_perc, select_perc, min_spacing_thresh=3):\n",
    "    '''\n",
    "    \n",
    "    df_divide: candidate words divided into quartiles based on entropy and accuracy\n",
    "    \n",
    "    remove_perc: percentage of words to remove based on proximity to other words\n",
    "        helps ensure decent spacing between presented words\n",
    "        \n",
    "    select_perc: percentage of words to select for presentation    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_divide['spacing'] = np.hstack([np.nan, np.diff(df_divide.index)])\n",
    "    \n",
    "    quadrant_distributions = get_quadrant_distributions(df_divide, df_divide.index).to_numpy()\n",
    "    \n",
    "    updated = []\n",
    "\n",
    "    for i, df in df_divide.groupby(['entropy_group', 'accuracy_group']):\n",
    "        # find how many words to remove in the quadrant based on the percent\n",
    "        n_words = round(remove_perc * len(df))\n",
    "        df = df.sort_values(by='spacing').iloc[n_words:]\n",
    "        updated.append(df.sort_index())\n",
    "\n",
    "    updated = pd.concat(updated).sort_index()\n",
    "    updated_distributions = get_quadrant_distributions(updated, updated.index).to_numpy()\n",
    "    \n",
    "    print (quadrant_distributions)\n",
    "    print (updated_distributions)\n",
    "    \n",
    "    assert (np.isclose(quadrant_distributions, updated_distributions, atol=0.01).all())\n",
    "    \n",
    "    # make sure it is scaled to the original dataframe\n",
    "    select_perc = select_perc/(1-remove_perc)\n",
    "    min_spacing = 0\n",
    "    RANDOM_STATE = 0\n",
    "    \n",
    "    print (f'Selecting {select_perc*100:.2f}% of remaining items')\n",
    "    \n",
    "    while (min_spacing < min_spacing_thresh):\n",
    "        # now sample the words from each quadrant\n",
    "        sampled = []\n",
    "\n",
    "        for i, df in updated.groupby(['entropy_group', 'accuracy_group']):\n",
    "\n",
    "            df_sampled = df.sample(frac=select_perc, random_state=RANDOM_STATE).sort_index()\n",
    "            sampled.append((len(df_sampled), df_sampled))\n",
    "\n",
    "        n_sampled, sampled = zip(*sampled)\n",
    "        sampled = pd.concat(sampled).sort_index()\n",
    "\n",
    "        min_spacing = np.diff(sampled.index).min()\n",
    "        \n",
    "        RANDOM_STATE += 1\n",
    "    \n",
    "    print (f'Min spacing of {min_spacing}')\n",
    "    print (f'{len(sampled)} total words')\n",
    "\n",
    "    return sampled\n",
    "\n",
    "import random\n",
    "\n",
    "def random_chunks(lst, n, shuffle=False):\n",
    "    \"\"\"Created randomized n-sized chunks from lst.\"\"\"\n",
    "    \n",
    "    tmp_lst = lst.copy()\n",
    "    n_total = len(lst)\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(tmp_lst)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    for i in range(0, len(tmp_lst), n):\n",
    "        all_chunks.append(tmp_lst[i:i + n])\n",
    "    \n",
    "    # distribute remaining items across orders\n",
    "    if len(all_chunks) != n_total//n:\n",
    "        remainder = all_chunks.pop()\n",
    "        \n",
    "        for i, item in enumerate(remainder):      \n",
    "            all_chunks[i%n].append(item)\n",
    "    \n",
    "    # lastly sort for ordered indices\n",
    "    all_chunks = [sorted(chunk) for chunk in all_chunks]\n",
    "    \n",
    "    return all_chunks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adventuresinsayingyes\n",
    "black\n",
    "bronx\n",
    "example_trial\n",
    "eyespy\n",
    "milkywayoriginal\n",
    "milkywaysynonyms\n",
    "milkywayvodka\n",
    "nwp_practice_trial\n",
    "piemanpni\n",
    "prettymouth\n",
    "shame\n",
    "tunnel\n",
    "wheretheressmoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95a2f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12 0.29 0.29 0.11]]\n",
      "[[0.14 0.35 0.36 0.14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/preproc_utils.py:523: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_entropy_idxs, 'entropy_group'] = 'low'\n",
      "/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/preproc_utils.py:531: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_accuracy_idxs, 'accuracy_group'] = 'low'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m model_results \u001b[38;5;241m=\u001b[39m model_results\u001b[38;5;241m.\u001b[39miloc[candidate_idxs]\n\u001b[1;32m     13\u001b[0m df_divide \u001b[38;5;241m=\u001b[39m preproc\u001b[38;5;241m.\u001b[39mdivide_nwp_dataframe(model_results, accuracy_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword2vec_avg_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, percentile\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m df_selected \u001b[38;5;241m=\u001b[39m \u001b[43mpreproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_prediction_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_divide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_perc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect_perc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_spacing_thresh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/preproc_utils.py:583\u001b[0m, in \u001b[0;36mselect_prediction_words\u001b[0;34m(df_divide, remove_perc, select_perc, min_spacing_thresh)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mprint\u001b[39m (quadrant_distributions)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mprint\u001b[39m (updated_distributions)\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mallclose(quadrant_distributions, updated_distributions, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m))\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# make sure it is scaled to the original dataframe\u001b[39;00m\n\u001b[1;32m    586\u001b[0m select_perc \u001b[38;5;241m=\u001b[39m select_perc\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mremove_perc)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_tasks_quadrants = []\n",
    "\n",
    "models_dir = os.path.join(DERIVATIVES_DIR, 'model-predictions')\n",
    "model_name = 'gpt2-xl'\n",
    "task = 'life'\n",
    "\n",
    "df_preproc, candidate_idxs = get_stim_candidate_idxs(task)\n",
    "\n",
    "model_results = load_model_data(models_dir, model_name=model_name, task=task, top_n=5, window_size=100)\n",
    "model_results.loc[:, 'binary_accuracy'] = model_results['binary_accuracy'].astype(bool)\n",
    "model_results = model_results.iloc[candidate_idxs]\n",
    "\n",
    "df_divide = divide_nwp_dataframe(model_results, accuracy_type='word2vec_avg_accuracy', percentile=45)\n",
    "\n",
    "\n",
    "df_selected = preproc.select_prediction_words(df_divide, remove_perc=0.5, select_perc=0.4, min_spacing_thresh=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e37feb41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    4    5    8   11   15   19   22   23   24   27   31   35   37\n",
      "   38   39   41   43   44   45   48   51   53   57   58   62   67   68\n",
      "   72   75   77   78   82   86   91   94   98   99  103  106  108  110\n",
      "  117  119  121  122  125  128  129  133  138  141  142  145  148  151\n",
      "  154  160  161  162  169  173  178  179  181  183  184  185  186  189\n",
      "  190  191  193  195  196  197  199  201  203  205  210  214  216  218\n",
      "  220  221  223  224  230  233  239  240  242  246  247  248  250  251\n",
      "  254  255  257  262  264  265  268  269  270  273  274  278  281  283\n",
      "  285  286  290  294  298  299  304  306  308  309  311  312  315  320\n",
      "  321  325  327  334  339  340  342  345  347  351  355  359  362  365\n",
      "  368  370  371  374  378  380  382  383  385  386  387  390  391  394\n",
      "  398  401  402  405  409  410  414  417  420  421  425  426  431  432\n",
      "  434  435  442  447  451  457  460  462  464  465  467  470  473  476\n",
      "  478  480  482  486  488  491  496  497  498  499  504  506  508  511\n",
      "  512  513  514  518  520  527  533  535  541  545  548  550  554  556\n",
      "  559  564  568  571  573  578  580  583  588  590  591  593  597  601\n",
      "  604  606  608  610  611  615  620  623  624  625  626  632  633  634\n",
      "  638  643  645  647  648  649  651  653  658  662  664  665  667  670\n",
      "  673  678  681  684  685  687  690  694  695  697  700  703  710  713\n",
      "  717  718  721  729  731  732  735  737  738  742  748  752  753  759\n",
      "  760  765  767  768  769  774  779  780  782  785  788  792  794  796\n",
      "  797  802  805  806  808  811  812  813  817  820  822  824  827  829\n",
      "  833  834  837  841  842  843  846  848  852  853  859  860  865  870\n",
      "  871  877  879  880  881  888  894  900  904  907  910  911  914  915\n",
      "  918  922  923  929  935  936  939  943  945  949  954  957  959  960\n",
      "  961  965  966  967  970  977  978  979  983  986  987  989  994  996\n",
      "  997 1002 1003 1004 1006 1007 1009 1013 1015 1023 1027 1030 1033 1036\n",
      " 1040 1043 1044 1047 1048 1049 1051 1056 1058 1059 1063 1068 1072 1073\n",
      " 1076 1077 1081 1083 1085 1087 1089 1092 1095 1096 1097 1100 1102 1103\n",
      " 1106 1109 1112 1113 1115 1118 1121 1122 1125 1127 1132 1140 1141 1145\n",
      " 1147 1151 1154 1156 1160 1163 1166 1169 1170 1175 1181 1183 1185 1187\n",
      " 1188 1191 1197 1204 1206 1209 1210 1211 1214 1215 1218 1229 1234 1237\n",
      " 1239 1243 1244 1247 1249 1251 1252 1254 1260 1262 1266 1267 1269 1273\n",
      " 1274 1276 1279 1284 1289 1290 1292 1293 1295 1299 1300 1304 1307 1309\n",
      " 1313 1319 1320 1324 1328 1329 1332 1335 1336 1339 1341 1345 1346 1351\n",
      " 1352 1355 1358 1361 1362 1363 1367 1368 1369 1373 1376 1377 1381 1385\n",
      " 1386 1390 1393 1396 1399 1401 1403 1404 1406 1407 1410 1417 1419 1422\n",
      " 1424 1428 1430 1433 1434 1436 1438 1442 1444 1447 1448 1454 1457 1464\n",
      " 1467 1469 1472 1479 1482 1485 1488 1494 1496 1498 1501 1504 1507 1509\n",
      " 1511 1513 1518 1520 1522 1526 1531 1532 1537 1538 1540 1542 1543 1546\n",
      " 1550 1551 1552 1553 1554 1555 1558 1559 1561 1565 1568 1569 1570 1572\n",
      " 1573 1575 1577 1578 1579 1580 1583 1584 1585 1591 1592 1595 1599 1600\n",
      " 1601 1604 1608 1612 1613 1614 1618 1619 1621 1622 1626 1627 1631 1636\n",
      " 1641 1642 1646 1649 1652 1655 1657 1664 1665 1668 1669 1673 1677 1678\n",
      " 1679 1681 1682 1686 1689 1696 1698 1702 1703 1706 1707 1712 1718 1721\n",
      " 1725 1731 1732 1735 1737 1738 1740 1744 1746 1751 1752 1756 1758 1762\n",
      " 1763 1765 1767 1772 1773 1776 1784 1789 1790 1792 1794 1796 1803 1805\n",
      " 1808 1810 1812 1814 1817 1820 1829 1830 1832 1837 1838 1843 1846 1850\n",
      " 1854 1858 1861 1862 1866 1867 1869 1873 1875 1878 1879 1880 1881 1882\n",
      " 1885 1887 1889 1893 1894 1899 1903 1904 1908 1909 1912 1914 1920 1923\n",
      " 1924 1929 1930 1932 1933 1935 1936 1937 1939 1940 1943 1945 1946 1948\n",
      " 1949 1950 1954 1955 1956 1958 1966 1969 1972 1973 1978 1984 1992 1997\n",
      " 1999 2003 2005 2008 2009 2012 2017 2021 2025 2031 2034 2037 2038 2048\n",
      " 2049 2054 2059 2063 2069 2070 2073 2076 2077 2079 2081 2085 2089 2090\n",
      " 2091 2094 2095 2098 2100 2101 2103 2105 2107 2108 2111 2112 2117 2120\n",
      " 2121 2123 2126 2130 2132 2135 2140 2143 2145 2148 2150 2151 2153 2155\n",
      " 2159 2162 2166 2168 2169 2171 2172 2173 2176 2179 2183 2185 2186 2187\n",
      " 2189 2191 2194 2195 2198]\n",
      "[[0.14 0.35 0.36 0.14]]\n",
      "[[0.15 0.35 0.36 0.14]]\n",
      "Selecting 80.00% of remaining items\n",
      "Min spacing of 2\n",
      "260 total words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ipykernel_19889/2894707255.py:44: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_entropy_idxs, 'entropy_group'] = 'low'\n",
      "/scratch/ipykernel_19889/2894707255.py:52: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'low' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_divide.loc[low_accuracy_idxs, 'accuracy_group'] = 'low'\n"
     ]
    }
   ],
   "source": [
    "all_tasks_quadrants = []\n",
    "\n",
    "models_dir = os.path.join(DERIVATIVES_DIR, 'model-predictions')\n",
    "model_name = 'gpt2-xl'\n",
    "task = 'life'\n",
    "\n",
    "df_preproc, candidate_idxs = get_stim_candidate_idxs(task)\n",
    "\n",
    "model_results = load_model_data(models_dir, model_name=model_name, task=task, top_n=5, window_size=100)\n",
    "model_results.loc[:, 'binary_accuracy'] = model_results['binary_accuracy'].astype(bool)\n",
    "model_results = model_results.iloc[candidate_idxs]\n",
    "\n",
    "df_divide = divide_nwp_dataframe(model_results, accuracy_type='word2vec_avg_accuracy', percentile=45)\n",
    "\n",
    "\n",
    "df_selected = select_prediction_words(df_divide, remove_perc=0.5, select_perc=0.4, min_spacing_thresh=2)\n",
    "\n",
    "# # percent_sampled = 0.\n",
    "# n_orders = 3\n",
    "# n_participants_per_item = 50\n",
    "# consecutive_spacing = 8\n",
    "\n",
    "# # find distribution of selected words from the divided quadrants\n",
    "# quadrant_distribution = get_quadrant_distributions(df_divide, df_selected.index).to_numpy()\n",
    "# deviation_threshold = 0.05\n",
    "# order_distributions = np.zeros((n_orders, 4))\n",
    "\n",
    "# # find indices for presentation and set number of items each subject sees\n",
    "# nwp_indices = sorted(df_selected.index)\n",
    "\n",
    "# # # Find lists with consecutive items violating our constraint\n",
    "\n",
    "# while not (np.allclose(quadrant_distribution, order_distributions, atol=deviation_threshold)):\n",
    "    \n",
    "#     subject_experiment_orders = random_chunks(nwp_indices, len(nwp_indices)//n_orders, shuffle=True)\n",
    "    \n",
    "#     print ('starting')\n",
    "# #     test_orders = subject_experiment_orders.copy()\n",
    "#     idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_spacing=consecutive_spacing)\n",
    "#     subject_experiment_orders = sort_consecutive_constraint(subject_experiment_orders, consecutive_spacing=consecutive_spacing)\n",
    "    \n",
    "#     order_distributions = [get_quadrant_distributions(df_divide, order).to_numpy() for order in subject_experiment_orders]\n",
    "    \n",
    "#     # sometimes the randomized order makes a quadrant be dropped --> reset and try again\n",
    "#     if not all([order.shape[-1] == 4 for order in order_distributions]):\n",
    "#         order_distributions = np.zeros((n_orders, 4))\n",
    "# # # Test again once we have completed resorting\n",
    "# # idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_spacing=p.consecutive_spacing)\n",
    "# # print (f'Lists violating consecutive index constraint: {100*(len(idxs))/len(subject_experiment_orders)}%')\n",
    "\n",
    "# # uniq, counts = np.unique(subject_experiment_orders, return_counts=True)\n",
    "# # print (f'All counts per word: {np.sum(counts >= p.n_participants_per_item) / len(counts)*100}%')\n",
    "\n",
    "# # counts = Counter(tuple(o) for o in subject_experiment_orders)\n",
    "# # unique_orders = np.sum([v for k, v in counts.items()]) / len(counts)\n",
    "\n",
    "# # print (f'Unique orders: {unique_orders*100}%')\n",
    "\n",
    "# # orders_meeting_consecutive = np.sum([check_consecutive_spacing(order, consecutive_spacing=p.consecutive_spacing) for order in subject_experiment_orders]) / len(subject_experiment_orders)\n",
    "# # print (f'Consecutive constraint: {orders_meeting_consecutive*100}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "id": "52732374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #10\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #11\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #12\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #10\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #11\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #12\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #13\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lists w/ violation: 2\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #9\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #10\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #11\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #6\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #7\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #8\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #5\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 1\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n",
      "Starting pass #1\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #2\n",
      "Number of lists w/ violation: 3\n",
      "Starting pass #3\n",
      "Number of lists w/ violation: 2\n",
      "Starting pass #4\n",
      "Number of lists w/ violation: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word_Written</th>\n",
       "      <th>Case</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS_Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop_Word</th>\n",
       "      <th>Word_Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Named_Entity</th>\n",
       "      <th>NWP_Candidate</th>\n",
       "      <th>entropy_group</th>\n",
       "      <th>accuracy_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.127781</td>\n",
       "      <td>0.115309</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reached</td>\n",
       "      <td>success</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>reached</td>\n",
       "      <td>0.127781</td>\n",
       "      <td>0.493847</td>\n",
       "      <td>0.366067</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>over</td>\n",
       "      <td>0.493847</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.466470</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>success</td>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>and</td>\n",
       "      <td>1.539002</td>\n",
       "      <td>1.661162</td>\n",
       "      <td>0.122160</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>secretly</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>secretly</td>\n",
       "      <td>1.664915</td>\n",
       "      <td>2.377098</td>\n",
       "      <td>0.712183</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>high</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>I</td>\n",
       "      <td>success</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>I</td>\n",
       "      <td>590.470522</td>\n",
       "      <td>590.611418</td>\n",
       "      <td>0.140897</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>still</td>\n",
       "      <td>success</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>still</td>\n",
       "      <td>590.611418</td>\n",
       "      <td>590.999320</td>\n",
       "      <td>0.387902</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>miss</td>\n",
       "      <td>success</td>\n",
       "      <td>VBP</td>\n",
       "      <td>verb, present tense, not 3rd person singular</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>miss</td>\n",
       "      <td>590.999320</td>\n",
       "      <td>591.188889</td>\n",
       "      <td>0.189569</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>the</td>\n",
       "      <td>success</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>591.188889</td>\n",
       "      <td>591.265763</td>\n",
       "      <td>0.076874</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>smoking</td>\n",
       "      <td>success</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>smoking</td>\n",
       "      <td>591.265763</td>\n",
       "      <td>591.747619</td>\n",
       "      <td>0.481856</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1827 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word_Written     Case  POS                                POS_Definition  \\\n",
       "0               I  success  PRP                             pronoun, personal   \n",
       "1         reached  success  VBD                              verb, past tense   \n",
       "2            over  success   RB                                        adverb   \n",
       "3             and  success   CC                     conjunction, coordinating   \n",
       "4        secretly  success   RB                                        adverb   \n",
       "...           ...      ...  ...                                           ...   \n",
       "1822            I  success  PRP                             pronoun, personal   \n",
       "1823        still  success   RB                                        adverb   \n",
       "1824         miss  success  VBP  verb, present tense, not 3rd person singular   \n",
       "1825          the  success   DT                                    determiner   \n",
       "1826      smoking  success   NN                noun, common, singular or mass   \n",
       "\n",
       "     Punctuation  Stop_Word Word_Vocab       Onset      Offset  Duration  \\\n",
       "0                      True          I    0.012472    0.127781  0.115309   \n",
       "1                     False    reached    0.127781    0.493847  0.366067   \n",
       "2                      True       over    0.493847    0.960317  0.466470   \n",
       "3                      True        and    1.539002    1.661162  0.122160   \n",
       "4                     False   secretly    1.664915    2.377098  0.712183   \n",
       "...          ...        ...        ...         ...         ...       ...   \n",
       "1822                   True          I  590.470522  590.611418  0.140897   \n",
       "1823                  False      still  590.611418  590.999320  0.387902   \n",
       "1824                  False       miss  590.999320  591.188889  0.189569   \n",
       "1825                   True        the  591.188889  591.265763  0.076874   \n",
       "1826           .      False    smoking  591.265763  591.747619  0.481856   \n",
       "\n",
       "      Named_Entity  NWP_Candidate entropy_group accuracy_group  \n",
       "0            False          False           NaN            NaN  \n",
       "1            False          False           NaN            NaN  \n",
       "2            False          False           NaN            NaN  \n",
       "3            False          False           NaN            NaN  \n",
       "4            False           True          high            low  \n",
       "...            ...            ...           ...            ...  \n",
       "1822         False          False           NaN            NaN  \n",
       "1823         False          False           NaN            NaN  \n",
       "1824         False          False           NaN            NaN  \n",
       "1825         False          False           NaN            NaN  \n",
       "1826         False          False           NaN            NaN  \n",
       "\n",
       "[1827 rows x 14 columns]"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_ORDERS = 4\n",
    "CONSECUTIVE_SPACING = 10 \n",
    "\n",
    "# get baseline distribution of quadrants --> deviation threshold is the amount of error\n",
    "# we tolerate from the distribution in each order\n",
    "quadrant_distribution = get_quadrant_distributions(df_divide, df_selected.index).to_numpy()\n",
    "deviation_threshold = 0.05\n",
    "order_distributions = np.zeros((n_orders, 4))\n",
    "\n",
    "# find indices for presentation and set number of items each subject sees\n",
    "nwp_indices = sorted(df_selected.index)\n",
    "\n",
    "# Find lists with consecutive items violating our constraint\n",
    "while not (np.allclose(quadrant_distribution, order_distributions, atol=deviation_threshold)):\n",
    "\n",
    "    # randomly chunk all indices into N_ORDERS\n",
    "    subject_experiment_orders = random_chunks(nwp_indices, len(nwp_indices)//N_ORDERS, shuffle=True)\n",
    "    subject_experiment_orders = sort_consecutive_constraint(subject_experiment_orders, consecutive_spacing=CONSECUTIVE_SPACING)\n",
    "    \n",
    "    # now find distribution of each order\n",
    "    order_distributions = [get_quadrant_distributions(df_divide, order).to_numpy() for order in subject_experiment_orders]\n",
    "\n",
    "    # sometimes the randomized order makes a quadrant be dropped --> reset and try again\n",
    "    if not all([order.shape[-1] == 4 for order in order_distributions]):\n",
    "        order_distributions = np.zeros((N_ORDERS, 4))\n",
    "\n",
    "idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_spacing=CONSECUTIVE_SPACING)\n",
    "# now update df_preproc with our selected indices --> write out\n",
    "selected_idxs = df_selected.index\n",
    "df_final = df_preproc.copy()\n",
    "df_final.loc[selected_idxs, ['entropy_group', 'accuracy_group']] = df_selected[['entropy_group', 'accuracy_group']]\n",
    "\n",
    "df_final\n",
    "\n",
    "\n",
    "# for each of the experiment orders\n",
    "# for i, order in enumerate(subject_experiment_orders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "e32218e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[12, 64, 81, 100, 295, 317, 341, 368, 380, 404, 431, 470, 483, 504, 517, 572, 598, 609, 666, 681, 704, 716, 741, 774, 907, 981, 1020, 1035, 1068, 1094, 1141, 1174, 1191, 1211, 1227, 1239, 1269, 1281, 1318, 1336, 1376, 1388, 1408, 1430, 1445, 1462, 1473, 1485, 1525], [19, 75, 148, 190, 235, 255, 305, 331, 354, 390, 409, 451, 476, 496, 507, 540, 558, 588, 626, 648, 670, 687, 710, 731, 813, 842, 856, 874, 919, 948, 994, 1009, 1043, 1055, 1112, 1136, 1182, 1233, 1246, 1273, 1300, 1311, 1325, 1380, 1394, 1416, 1436, 1493, 1516], [9, 49, 85, 116, 154, 201, 221, 244, 272, 302, 383, 401, 421, 446, 462, 528, 576, 673, 693, 707, 724, 747, 779, 799, 833, 845, 903, 914, 935, 970, 986, 1004, 1125, 1147, 1178, 1202, 1230, 1251, 1264, 1284, 1369, 1421, 1433, 1454, 1476, 1488, 1507, 1539], [6, 46, 90, 111, 143, 163, 212, 239, 263, 299, 346, 361, 373, 395, 417, 441, 500, 547, 567, 580, 603, 645, 663, 677, 713, 727, 744, 828, 848, 868, 941, 1017, 1062, 1075, 1100, 1133, 1187, 1217, 1236, 1291, 1303, 1328, 1343, 1358, 1412, 1440, 1479, 1496]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "af8f9a2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "503870fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4,\n",
       "  30,\n",
       "  51,\n",
       "  70,\n",
       "  83,\n",
       "  106,\n",
       "  130,\n",
       "  144,\n",
       "  165,\n",
       "  202,\n",
       "  244,\n",
       "  257,\n",
       "  276,\n",
       "  344,\n",
       "  367,\n",
       "  406,\n",
       "  435,\n",
       "  453,\n",
       "  490,\n",
       "  539,\n",
       "  552,\n",
       "  574,\n",
       "  605,\n",
       "  639,\n",
       "  661,\n",
       "  699,\n",
       "  711,\n",
       "  748,\n",
       "  835,\n",
       "  848,\n",
       "  863,\n",
       "  881,\n",
       "  907,\n",
       "  950,\n",
       "  966,\n",
       "  1021,\n",
       "  1065,\n",
       "  1167,\n",
       "  1179,\n",
       "  1239,\n",
       "  1299,\n",
       "  1341,\n",
       "  1397,\n",
       "  1409,\n",
       "  1462,\n",
       "  1476,\n",
       "  1513,\n",
       "  1542,\n",
       "  1565,\n",
       "  1590,\n",
       "  1678,\n",
       "  1745,\n",
       "  1758,\n",
       "  1817],\n",
       " [64,\n",
       "  114,\n",
       "  136,\n",
       "  169,\n",
       "  208,\n",
       "  239,\n",
       "  261,\n",
       "  288,\n",
       "  341,\n",
       "  355,\n",
       "  372,\n",
       "  389,\n",
       "  413,\n",
       "  432,\n",
       "  484,\n",
       "  531,\n",
       "  548,\n",
       "  565,\n",
       "  589,\n",
       "  621,\n",
       "  650,\n",
       "  664,\n",
       "  675,\n",
       "  691,\n",
       "  826,\n",
       "  852,\n",
       "  889,\n",
       "  924,\n",
       "  959,\n",
       "  978,\n",
       "  994,\n",
       "  1037,\n",
       "  1062,\n",
       "  1076,\n",
       "  1099,\n",
       "  1130,\n",
       "  1151,\n",
       "  1190,\n",
       "  1203,\n",
       "  1222,\n",
       "  1259,\n",
       "  1318,\n",
       "  1389,\n",
       "  1451,\n",
       "  1481,\n",
       "  1497,\n",
       "  1551,\n",
       "  1601,\n",
       "  1634,\n",
       "  1658,\n",
       "  1673,\n",
       "  1732,\n",
       "  1772,\n",
       "  1783],\n",
       " [23,\n",
       "  38,\n",
       "  78,\n",
       "  99,\n",
       "  121,\n",
       "  151,\n",
       "  198,\n",
       "  251,\n",
       "  272,\n",
       "  319,\n",
       "  337,\n",
       "  382,\n",
       "  425,\n",
       "  445,\n",
       "  469,\n",
       "  504,\n",
       "  525,\n",
       "  544,\n",
       "  585,\n",
       "  601,\n",
       "  632,\n",
       "  670,\n",
       "  696,\n",
       "  736,\n",
       "  756,\n",
       "  769,\n",
       "  813,\n",
       "  830,\n",
       "  842,\n",
       "  896,\n",
       "  928,\n",
       "  943,\n",
       "  962,\n",
       "  975,\n",
       "  1001,\n",
       "  1027,\n",
       "  1110,\n",
       "  1122,\n",
       "  1160,\n",
       "  1175,\n",
       "  1217,\n",
       "  1255,\n",
       "  1352,\n",
       "  1384,\n",
       "  1419,\n",
       "  1523,\n",
       "  1557,\n",
       "  1624,\n",
       "  1646,\n",
       "  1669,\n",
       "  1688,\n",
       "  1710,\n",
       "  1722,\n",
       "  1762],\n",
       " [11,\n",
       "  34,\n",
       "  47,\n",
       "  61,\n",
       "  87,\n",
       "  109,\n",
       "  158,\n",
       "  268,\n",
       "  292,\n",
       "  306,\n",
       "  351,\n",
       "  363,\n",
       "  376,\n",
       "  396,\n",
       "  416,\n",
       "  441,\n",
       "  518,\n",
       "  535,\n",
       "  562,\n",
       "  595,\n",
       "  617,\n",
       "  685,\n",
       "  727,\n",
       "  742,\n",
       "  766,\n",
       "  786,\n",
       "  806,\n",
       "  822,\n",
       "  877,\n",
       "  921,\n",
       "  940,\n",
       "  955,\n",
       "  969,\n",
       "  997,\n",
       "  1016,\n",
       "  1054,\n",
       "  1068,\n",
       "  1081,\n",
       "  1118,\n",
       "  1183,\n",
       "  1207,\n",
       "  1249,\n",
       "  1290,\n",
       "  1345,\n",
       "  1375,\n",
       "  1428,\n",
       "  1447,\n",
       "  1459,\n",
       "  1471,\n",
       "  1516,\n",
       "  1649,\n",
       "  1666,\n",
       "  1694,\n",
       "  1738]]"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_experiment_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "0ea625bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[738], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/asynchrony/lib/python3.9/site-packages/numpy/lib/shape_base.py:1264\u001b[0m, in \u001b[0;36mtile\u001b[0;34m(A, reps)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39marray(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ndmin\u001b[38;5;241m=\u001b[39md)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# Note that no copy of zero-sized arrays is made. However since they\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# have no data there is no risk of an inadvertent overwrite.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (d \u001b[38;5;241m<\u001b[39m c\u001b[38;5;241m.\u001b[39mndim):\n\u001b[1;32m   1266\u001b[0m     tup \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m,)\u001b[38;5;241m*\u001b[39m(c\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39md) \u001b[38;5;241m+\u001b[39m tup\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "np.tile(test, (50, 1)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a428b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the items as a dictionary for passing out to aggregate\n",
    "quadrant_dist = {f'{labels[0]}-entropy_{labels[1]}-accuracy': round(len(df)/len(df_divide), 2) \n",
    "             for labels, df in df_divide.groupby(['entropy_group', 'accuracy_group'])}\n",
    "\n",
    "df_quadrants = pd.DataFrame.from_dict(quadrant_dist, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "477bdd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.15, 0.3 , 0.4 , 0.14]]),\n",
       " array([[0.13, 0.4 , 0.33, 0.14]]),\n",
       " array([[0.15, 0.34, 0.38, 0.12]]),\n",
       " array([[0.13, 0.33, 0.36, 0.18]])]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4031953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_random_orders(n_orders, n_participants_per_item, consecutive_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "170031b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_quadrant_distributions(df_divide, order).to_numpy().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f296b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12,\n",
       "  89,\n",
       "  95,\n",
       "  158,\n",
       "  169,\n",
       "  202,\n",
       "  282,\n",
       "  355,\n",
       "  390,\n",
       "  408,\n",
       "  425,\n",
       "  463,\n",
       "  496,\n",
       "  504,\n",
       "  535,\n",
       "  582,\n",
       "  597,\n",
       "  608,\n",
       "  691,\n",
       "  696,\n",
       "  742,\n",
       "  822,\n",
       "  904,\n",
       "  997,\n",
       "  1024,\n",
       "  1032,\n",
       "  1065,\n",
       "  1081,\n",
       "  1105,\n",
       "  1110,\n",
       "  1222,\n",
       "  1227,\n",
       "  1235,\n",
       "  1245,\n",
       "  1282,\n",
       "  1302,\n",
       "  1307,\n",
       "  1345,\n",
       "  1447,\n",
       "  1486,\n",
       "  1565,\n",
       "  1617,\n",
       "  1637,\n",
       "  1722,\n",
       "  1798],\n",
       " [17,\n",
       "  73,\n",
       "  99,\n",
       "  114,\n",
       "  204,\n",
       "  253,\n",
       "  261,\n",
       "  323,\n",
       "  329,\n",
       "  351,\n",
       "  531,\n",
       "  605,\n",
       "  610,\n",
       "  646,\n",
       "  661,\n",
       "  701,\n",
       "  738,\n",
       "  762,\n",
       "  795,\n",
       "  802,\n",
       "  813,\n",
       "  839,\n",
       "  896,\n",
       "  902,\n",
       "  955,\n",
       "  1103,\n",
       "  1136,\n",
       "  1162,\n",
       "  1259,\n",
       "  1290,\n",
       "  1371,\n",
       "  1411,\n",
       "  1443,\n",
       "  1448,\n",
       "  1459,\n",
       "  1484,\n",
       "  1513,\n",
       "  1551,\n",
       "  1607,\n",
       "  1701,\n",
       "  1710,\n",
       "  1735,\n",
       "  1747,\n",
       "  1754],\n",
       " [144,\n",
       "  151,\n",
       "  187,\n",
       "  198,\n",
       "  217,\n",
       "  251,\n",
       "  292,\n",
       "  324,\n",
       "  376,\n",
       "  475,\n",
       "  536,\n",
       "  548,\n",
       "  578,\n",
       "  606,\n",
       "  681,\n",
       "  713,\n",
       "  749,\n",
       "  786,\n",
       "  806,\n",
       "  873,\n",
       "  933,\n",
       "  940,\n",
       "  980,\n",
       "  987,\n",
       "  1044,\n",
       "  1087,\n",
       "  1102,\n",
       "  1154,\n",
       "  1172,\n",
       "  1231,\n",
       "  1255,\n",
       "  1278,\n",
       "  1327,\n",
       "  1354,\n",
       "  1367,\n",
       "  1409,\n",
       "  1438,\n",
       "  1451,\n",
       "  1571,\n",
       "  1579,\n",
       "  1610,\n",
       "  1629,\n",
       "  1655,\n",
       "  1765],\n",
       " [11,\n",
       "  23,\n",
       "  103,\n",
       "  145,\n",
       "  153,\n",
       "  161,\n",
       "  188,\n",
       "  227,\n",
       "  248,\n",
       "  332,\n",
       "  372,\n",
       "  385,\n",
       "  406,\n",
       "  453,\n",
       "  552,\n",
       "  571,\n",
       "  601,\n",
       "  650,\n",
       "  685,\n",
       "  699,\n",
       "  718,\n",
       "  766,\n",
       "  809,\n",
       "  817,\n",
       "  865,\n",
       "  901,\n",
       "  975,\n",
       "  1030,\n",
       "  1056,\n",
       "  1068,\n",
       "  1161,\n",
       "  1193,\n",
       "  1276,\n",
       "  1285,\n",
       "  1331,\n",
       "  1355,\n",
       "  1375,\n",
       "  1476,\n",
       "  1533,\n",
       "  1542,\n",
       "  1590,\n",
       "  1662,\n",
       "  1733,\n",
       "  1774]]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "43c33632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.13, 0.28, 0.38, 0.2 ]]), array([[0.2 , 0.35, 0.3 , 0.15]])]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ba1a568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.17, 0.35, 0.29, 0.19]]), array([[0.17, 0.28, 0.39, 0.16]])]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "351ef407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91,)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(test_orders[0]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
