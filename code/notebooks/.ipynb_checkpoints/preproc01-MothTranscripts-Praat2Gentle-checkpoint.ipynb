{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ede745",
   "metadata": {},
   "source": [
    "# Moth Transcripts to Gentle\n",
    "\n",
    "The Huth Moth transcripts are provided within Praat. There are two issues with this format:\n",
    "1. There is no joint transcript including punctuation (allowing us to present the next-word prediction framework)\n",
    "2. Our pipeline uses Gentle as its starting point to process files\n",
    "\n",
    "We load the Praat files and align it with a transcript generated through ChatGPT (adjusting mismatched words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6601b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import os, sys, glob\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praatio import textgrid as tgio\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from text_utils import strip_punctuation\n",
    "# from text_utils import get_pos_tags, get_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdc80473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_textgrid(praat_fn):\n",
    "    '''\n",
    "    Load a praat textgrid file using PraatIO\n",
    "    '''\n",
    "    \n",
    "    # things to remove from the textgrid (indicates laughing, chewing, pauses etc)\n",
    "    REMOVE_CHARACTERS = ['sp', 'br', 'lg', 'cg', 'ls', 'ns', 'sl', 'ig',\n",
    "                         '{sp}', '{br}', '{lg}', '{cg}', '{ls}', '{ns}', '{sl}', '{ig}', 'pause']\n",
    "    \n",
    "    # open the textgrid\n",
    "    tg = tgio.openTextgrid(praat_fn, includeEmptyIntervals=False, reportingMode=\"warning\") \n",
    "    \n",
    "    # remove entries of unwanted characters\n",
    "    for tier_name in tg.tierNames:\n",
    "        # get the current tier\n",
    "        tier = tg.getTier(tier_name)\n",
    "        \n",
    "        for x in tier.entries:\n",
    "            if x[-1].lower() in REMOVE_CHARACTERS:\n",
    "                tier.deleteEntry(x)\n",
    "\n",
    "#         for char in REMOVE_CHARACTERS:\n",
    "#             upper_set = set(tier.find(char.upper()))\n",
    "#             lower_set = set(tier.find(char.lower()))\n",
    "#             remove_idxs = sorted(upper_set.union(lower_set))\n",
    "\n",
    "#             # go through each index and remove\n",
    "#             for idx in remove_idxs:\n",
    "#                 try:\n",
    "#                     tier.deleteEntry(tier.entries[idx])\n",
    "#                 except:\n",
    "#                     print (idx)\n",
    "    \n",
    "#     # go through each entry at the word tier, remove the items\n",
    "#     words = [x for x in tg.getTier('word').entries if x[-1].lower() not in REMOVE_CHARACTERS]\n",
    "#     phones = [x for x in tg.getTier('phone').entries if x[-1].lower() not in REMOVE_CHARACTERS]\n",
    "#     words = tg.getTier('word').entries\n",
    "#     phones = tg.getTier('phone').entries\n",
    "    return tg\n",
    "\n",
    "def load_transcription(transcript_fn):\n",
    "    \n",
    "    with open(transcript_fn, 'r') as f: #open the file\n",
    "        contents = f.readlines() #put the lines to a variable (list).\n",
    "        \n",
    "    # get the transcription stripped of punctuation\n",
    "    words_transcribed = strip_punctuation(contents).split()\n",
    "    \n",
    "    return contents, words_transcribed\n",
    "\n",
    "def textgrid_to_gentle(praat_fn, transcript_fn):\n",
    "    '''\n",
    "    Transform Moth dataset textgrid files into gentle format\n",
    "    '''\n",
    "    \n",
    "    textgrid = load_clean_textgrid(praat_fn)\n",
    "    tg_words = textgrid.getTier('word')\n",
    "    \n",
    "    contents, words_transcribed = load_transcription(transcript_fn)\n",
    "    \n",
    "    assert (len(tg_words) == len(words_transcribed))\n",
    "    \n",
    "    # create the dictionary to store things in\n",
    "    # put the transcript in the raw form\n",
    "    align = {}\n",
    "    align['transcript'] = contents[0]\n",
    "    align['words'] = []\n",
    "    \n",
    "    # Taken from Kaldi metasentence tokenizer\n",
    "    # splits the transcript based on any punctuation besides for apostrophes and hyphens\n",
    "    regex_split_pattern = r'(\\w|\\.\\w|\\:\\w|\\â€™\\w|\\'\\w|\\-\\w)+'\n",
    "    \n",
    "    iterator = list(re.finditer(regex_split_pattern, ''.join(contents), re.UNICODE))\n",
    "    n_items = len(list(iterator))\n",
    "    \n",
    "    # make sure the iterator matches the length\n",
    "    assert (n_items == len(tg_words) == len(words_transcribed))\n",
    "    \n",
    "    # if all matches we're good to go\n",
    "    for word_info, m in zip(tg_words, iterator):\n",
    "        # span of the word in characters relative to the overall string\n",
    "        start_offset, end_offset = m.span()\n",
    "        word = m.group()\n",
    "        \n",
    "        # crop textgrid to the word\n",
    "        cropped_grid = textgrid.crop(cropStart=word_info[0], cropEnd=word_info[1], mode='truncated', rebaseToZero=False)\n",
    "        tg_phones = cropped_grid.getTier('phone').entries\n",
    "        word_phones = []\n",
    "\n",
    "        for phone_info in tg_phones:\n",
    "            phone = re.sub(r'\\d+', '', phone_info[-1])\n",
    "            duration = phone_info[1] - phone_info[0]\n",
    "            word_phones.append({'duration': duration, 'phone': phone})\n",
    "\n",
    "        word_align = {\n",
    "            'alignedWord': word.lower(),\n",
    "            \"case\": \"success\",\n",
    "            'word': word,\n",
    "            'start': word_info[0],\n",
    "            'end': word_info[1],\n",
    "            'phones': word_phones,\n",
    "            \"startOffset\": start_offset,\n",
    "            \"endOffset\": end_offset,\n",
    "        }\n",
    "        \n",
    "        align['words'].append(word_align)\n",
    "        \n",
    "    return align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0e456",
   "metadata": {},
   "source": [
    "## Set paths \n",
    "\n",
    "These are paths to the main directory and the stimulus directory\n",
    "\n",
    "CHANGE THE PATH BELOW TO MATCH YOUR DIRECTORY --> FinnLabTasks/transcript_alignment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d50ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/'\n",
    "datasets_dir = '/dartfs/rc/lab/F/FinnLab/datasets/'\n",
    "# stim_dir = os.path.join(datasets_dir, 'IBC/stimuli/lepetitprince/')\n",
    "\n",
    "# for prepping for onlin eexpt\n",
    "stim_dir = os.path.join(base_dir, 'stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed86e4c",
   "metadata": {},
   "source": [
    "## Load Praat files\n",
    "\n",
    "We first get all the filenames of TextGrid files within the stimulus directory. We also print out the number of files within this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35c06145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in dataset: 28\n"
     ]
    }
   ],
   "source": [
    "praat_fns = sorted(glob.glob(os.path.join(stim_dir, 'praat', '*.TextGrid')))\n",
    "\n",
    "print (f'Total files in dataset: {len(praat_fns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ae474",
   "metadata": {},
   "source": [
    "<b>Note:</b> This is <b>very</b> likely not to work on the first time. Follow the steps below to get the file to load!\n",
    "\n",
    "We are going to load a Praat TextGrid file. This will probably not work on the first time due to overlapping timestamps. To address this, do the following:\n",
    "1. Open the .TextGrid file in a text editor (e.g., TextEdit, SublimeText)\n",
    "2. Look at the Python error -- you will need to manually adjust these overlapping times. Copy the first number in the second parentheses:\n",
    "    - <b>Example error:</b> Two intervals in the same tier overlap in time: (START_1, END_1, sp) and (START_2, END_2, B)\n",
    "    - For this error, copy the number \"START_2\"\n",
    "3. Go to the text editor, and search (cmd + F) for the copied number (e.g., \"START_2\").\n",
    "4. Adjust the word/phoneme before's end time (e.g., END_1) to match the copied number (\"START_2\").\n",
    "5. Save the file and rerun the code\n",
    "6. Repeat for as many times until the file loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "244a517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulus name: myfirstdaywiththeyankees\n",
      "Successfully loaded Praat file!\n"
     ]
    }
   ],
   "source": [
    "# select a file number to load -- we then select that file from the list of alphabetized file names\n",
    "file_num = 16\n",
    "praat_fn = praat_fns[file_num]\n",
    "\n",
    "# now grab the current filename as a path -- print out only the filename (no extension)\n",
    "filepath = Path(praat_fn)\n",
    "stim_name = filepath.stem\n",
    "print (f'Stimulus name: {filepath.stem}')\n",
    "\n",
    "# attempt to load the praat file -- if this doesn't work, follow the steps above \n",
    "tg = tgio.openTextgrid(praat_fns[file_num], includeEmptyIntervals=False, reportingMode=\"warning\") \n",
    "\n",
    "print (f'Successfully loaded Praat file!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c0f2a",
   "metadata": {},
   "source": [
    "## Adjust the words to have punctuation\n",
    "\n",
    "After loading the transcript using Praat, we concatenate all the transcript words and pass it to ChatGPT to ensure punctuation. Then we need to go through comparing word by word making sure of the following:\n",
    "-  The new transcript matches the original number of words\n",
    "- Words are spelled correctly (as full words)\n",
    "\n",
    "This cell below will print out all the words of the TextGrid as a string. You will need to do the following:\n",
    "1. Open ChatGPT: https://chat.openai.com/chat\n",
    "2. Type the following instructions: \"Add punctuation and capitalization to the following but change nothing else:\"\n",
    "3. Copy and paste the transcript below <i>after</i> the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64d02c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY STORY IS ABOUT A NUMBER OF JOBS THAT SORT OF SHAPED MY LIFE AND SHAPED MY WHOLE DESTINY AND IT WAS A COUPLE OF JOBS I HAD UH DURING THE AGES OF TWENTY TO TWENTY ONE SO THE STORY BEGINS IN IN UH YES I WE YES I WAS EMPLOYED BACK THEN OK I DONT KNOW WHY YOU LAUGHED BUT ILL ILL ACCEPT THAT ITS A GOOD SIGN OK THINKING OUT LOUD HERE ALRIGHT SO IN NINETEEN EIGHTY FOUR I WAS A SOPHOMORE AT PRINCETON YOU CAN LAUGH AT THAT IF YOU LIKE NO ALRIGHT SO IN NINETEEN EIGHTY FOUR I WAS A SOPHOMORE AT PRINCETON I WANTED TO TAKE A YEAR OFF BECAUSE I HAD I HAD A VERY BAD REPUTATION AT SCHOOL I WAS DRINKING A LOT GETTING IN A LOT OF TROUBLE SO I WANTED TO TAKE A YEAR OFF ALSO I HAD JOINED THE ARMY TO PAY FOR PRINCETON AND I WAS LOOKING AT FOUR YEARS OF ACTIVE DUTY AFTER SCHOOL AND EIGHT YEARS OF RESERVE DUTY SO THIS WAS MY LAST CHANCE AT FREEDOM WAS AFTER S YOU KNOW WAS TAKING THIS YEAR OFF FROM SCHOOL LATER JUST FOR THE HARPERS CROWD I DID BECOME A CONSCIENTIOUS OBJECTOR AND SO I GOT OUT OF THE MILITARY SO AND AVOIDED THE GULF WAR UNLIKE MY FELLOW ROTC GUYS WHICH WAS A GOOD THING ALRIGHT SO BUT THIS WAS BEFORE I KNEW I WAS GETTING OUT OF THE ARMY THIS WAS GONNA BE MY YEAR OF FREEDOM I NEE I NEEDED TO MAKE MONEY THOUGH DURING THIS YEAR OFF AND A FRIEND OF MINE I KNOW ITS GOING TO SEEM ABSURD SUGGESTED THAT I BE A MODEL AND HE TOOK SOME PICTURES OF ME I KNOW I HAVE RINGS UNDER MY EYES THAT GO ALL THE WAY TO THE BACK OF MY SKULL AND I HAVE NO HAIR BUT AT TWENTY HE HAD IT IN HIS MIND THAT I SHOULD BE A MODEL SO HE TAKES SOME PICTURES OF ME AND I MUST ADMIT I I THOUGHT I WAS UGLY BUT I WAS VAIN ENOUGH TO TRY IT I DONT KNOW WHAT I WAS THINKING SO I GO I LOOK UP IN THE PRINCETON LIBRARY THEY HAD A MANHATTAN PHONEBOOK AND I LOOK UP MODELING AND I CHOOSE A MODELING AGENCY AND I GO INTO NEW YORK WITH THESE PICTURES THIS FELLOW TAKES THEM HE SAYS CAN I HOLD THESE OVERNIGHT I SAID YES AND UH THE NEXT MORNING HE CALLS ME COME INTO NEW YORK WE WANT TO SIGN YOU TO A CONTRACT HED SENT MY PICTURES TO THE PHOTOGRAPHER BRUCE WEBER WHO DOES THE ABERCORMBIE AND FITCH AND VANITY FAIR HOWEVER YOU SAY ABERCROMBIE WHATEVER AND UH THAT THIS FELLOW WANTED TO PHOTOGRAPH ME SO SUDDENLY WITH THIS SMALL AGENCY WHICH EXISTED FOR ABOUT A YEAR AND THEN THERE WAS A SCANDAL WITH YOUNG FEMALE MODELS FROM MINNESOTA CLOSED DOWN BUT SO SUDDENLY IM A MODEL AND IM GOING OFF TO THIS BRUCE WEBER PHOTOSHOOT AND I REMEMBER WE MET EARLY IN THE MORNING A BUNCH OF US A BUNCH OF MALE MODELS AND A VAN AND AS WERE GOING FOR TO UH THE HAMPTONS FOR THIS PHOTOSHOOT ITS LIKE SIX AM THIS ONE GUY LOOKS AROUND HES FROM TEXAS HE GOES HEY WERE ALL BLOND YOU KNOW JUST IT DAWNED ON HIM SORT OF UH AS KIND OF INTERESTING AND SO WE GET OUT TO THE HAMPTONS AND WERE AT THIS FARMHOUSE AND IT WAS LIKE A SCENE OUT OF CHRISTOPHER ISHERWOOD THE BERLIN STORIES ALL THESE BLONDE BOYS ABOUT TEN OF US RUNNING AROUND DOING PUSHUPS SO THAT OUR MUSCLES WOULD SWELL AND IN AND OUT OF THE POOL AND A BIG BUFFET AND EVERYTHING WAITING FOR THE LIGHT TO CHANGE AND UH SO I GET MY PICTURE TAKEN BY BRUCE WEBER WONDERFUL GUY VERY TALENTED OBVIOUSLY AND HE WANTED ME TO UH DROP MY SHORTS AND I FELT EMBARRASSED I DIDNT THINK I COULD DO THAT AND SO I ANYWAYS I HID EVERYTHING I I WASNT ABLE TO EXPOSE MYSELF SO ANYWAYS HE TOOK THESE PICTURES OF ME AND I HAVE ONE AND ILL UH IM GOING TO PASS IT OUT AS A SORT OF GIFT FOR YOU UH I ONLY HAVE A HUNDRED AND FIFTY OF THEM SO IF ONE PER TABLE BUT THIS IS THE BRUCE WEBER PICTURE OF ME AND UH I WAS THE UH ANYWAY THE PHYSIQUE WAS LOOKING GOOD THEN AND YOU COULD SEE I HAD HAIR ITS SORT OF DEPRESSING TO LOOK AT THAT NOW I DONT KNOW WHAT HAPPENED BUT UH ANYWAY SO I ENDED UP IN THE WHITNEY BIENNIAL I LIKE TO SAY TO MY ARTIST FRIENDS UH IVE BEEN IN THE BIENNIAL BUT IT WAS IN THIS PICTURE AND UH ON THE BACK OF THIS IS THE THING FROM MY UH NURSERY SCHOOL JUST SO YOU CAN STUDY THAT IF YOU LIKE SO SO PASS THOSE AROUND SO MAYBE ONE PER TABLE SINCE THERES ONLY A HUNDRED AND FIFTY BUT SO IT CAN GET TO THE BACK ALRIGHT SO HERE I AM A MODEL AND I GOT SEVERAL JOBS AND BUT AT THAT PHOTOSHOOT AT AT BRUCE WEBERS FARMHOUSE I MET A VERY PRETTY GIRL SHE WAS HIS ASSISTANT AND SHE GAVE ME HER PHONE NUMBER SO ABOUT A WEEK LATER IM DOING MY MODELING APPOINTMENTS WHICH ARE CALLED GO SEES WHICH WAS MADE VERY SIMPLE FOR THE MODELS IT WAS GO SEE SOMEONE AND SO THIS WAS WHAT THEY CALLED APPOI UH APPOINTMENTS WERE GO SEES SO IM ON A DAY OF GO SEES AND I FOUND MYSELF IN THE WEST VILLAGE AND UH RIGHT BY CORNELIA STREET AND I REMEMBERED THAT THAT GIRL LIVED ON CORNELIA STREET AND I MY HIGH SCHOOL LOVE WAS A GIRL NAMED CORNELIA AND SO SUDDENLY IT WAS ALL FUSING IN MY MIND SO I CALL HER AND UH I GET HER ROOMMATE AND I EXPLAIN THAT IM IN THE NEIGHBORHOOD AND IS I WONT SAY HER NAME WELL ILL SAY HER NAME CAUSE I CANT THINK OF A NEW ONE ILL I SAID IS ERIN HOME NO AND IM HER ROOMMATE AND I SAID OH IM RIGHT IN THE NEIGHBORHOOD SHE SAID WELL IF YOU WANNA COME UP AND WAIT FOR HER IT WAS LATE IN THE AFTERNOON SHE SHOULD BE HOME SOON I GO OK SO I GO UP TO HER APARTMENT AND I MEET THE ROOMMATE WHO WAS ABOUT THIRTY YEARS OLD AND VERY BEAUTIFUL AND VERY EXOTIC TO ME AT YOU KNOW I WAS TWENTY SHE WAS THIRTY AND UH SHE OFFERS ME A GLASS OF WINE WE END UP DRINKING UH TWO BOTTLES OF WINE NOT NOT ONLY IS MY LIFE BEEN SHAPED BY UH JOBS BUT ALSO BY UH LIQUOR AND SEX WHICH SHOULD APPEAL TO THE GLENFIDDICH PEOPLE SO UM SO WE DRINK SOME WINE WE DRINK LIKE TWO BOTTLES OF WINE THE FIRST GIRL NEVER COMES HOME AND I END UP MAKING LOVE TO THE ROOMMATE WHICH THESE THINGS HAPPENED BACK THEN AND UH SO WERE MAKING LOVE AND SHE WAS OLDER AND THIS WAS ALL VERY INTRIGUING TO ME AND AT A PENULTIMATE MOMENT THE MOMENT BEFORE AN ULTIMATE MOMENT I SAID TO HER I SAID CAN I HAVE AN ULTIMATE MOMENT I CAUSE I WASNT USING A CONDOM THIS WAS SORT OF BEFORE THE WHOLE CONDOM HYSTERIA AND AIDS HYSTERIA AND SO EVERYTHING WAS MORE ABOUT BIRTH CONTROL NOT YOU KNOW I I THOUGHT OF A BAD LINE EARLIER DEATH CONTROL SO THIS WAS ALL ABOUT BIRTH CONTROL AND SO I SAID UM CAN I HAVE AN ULTIMATE MOMENT AND SHE SAID OF COURSE YOU CAN IN A VERY SORT OF CONDESCENDING WAY WHICH WAS A BAD THING TO HEAR RIGHT AT A PENULTIMATE MOMENT BECAUSE I DONT KNOW IT DESTROYED MY CONFIDENCE AS A YOUNG LOVER I THOUGHT OH OF COURSE YOU CAN WHY DIDNT I KNOW THAT AND SO UH IT WRECKED MY ULTIMATE MOMENT A LITTLE SOME SHORT CIRCUITING HAPPENING BUT IT HAPPENED AND SO I THOUGHT I LEARNED A VALUABLE LESSON WHICH IS YOU NEVER ASK A WOMAN IF YOU CAN HAVE YOUR ULTIMATE MOMENT YOU JUST GO AHEAD AND HAVE IT YOU DONT ASK HER UNLESS SHE TELLS YOU BEFOREHAND THAT YOU KNOW YOU NEED TO BE CAREFUL OR SOMETHING SO I THOUGHT THIS WAS THE VALUABLE LESSON I LEARNED TO BE A LOTHARIO NEVER ASK SO I GO OFF TO EUROPE TO BE A MODEL BECAUSE IVE DONE VERY WELL I ENDED UP BEING PHOTOGRAPHED BY THIS GUY NAMED HORST AND I WAS IN BUS STOP ADS ALL OVER NEW YORK AND FERNANDO SANCHEZ LINGERIE IN A PAIR OF BIKINIS OR SOMETHING SO I GO OFF TO EUROPE TO MILAN TO BE A MODEL BUT I TRAVEL FOR ABOUT A MONTH AND A HALF AND I GET IN A TERRIBLE BAR FIGHT IN PARIS FRANCE AND I GET MY NOSE BROKEN AND MY LIPS SPLIT AND MY SKULL KNEED AND EVERYTHING GOT REALLY BEAT UP AND I WAS SUPPOSED TO REPORT TO MILAN MILANO WHATEVER FIVE DAYS LATER BUT MY NOSE WAS ALL PURPLE AND I THOUGHT YOU KNOW WHAT I WANTED TO BE A WRITER I THOUGHT A WRITER SHOULDNT BE A MODEL SO IM NOT GOING TO GO TO MILANO AND THATS NOT A DIGNIFIED JOB SO I BECAME A MALE AU PAIR IN PARIS IN THE UH MONTPARNASSE NEIGHBORHOOD SO I UH SO THAT WAS KIND OF A GOOD JOB BECAUSE THAT WAS RIGHT NEAR WHERE HEMINGWAY WOULD HANG OUT AND EVEN PASS THROUGH THE BAKERY THAT HE USED TO PASS THROUGH TO GET TO GERTRUDE STEIN AND SO I HAD MY TWO LITTLE CHARGES I THINK THEY WERE LIKE SIX AND THREE AND UH AND IT WAS WONDERFUL AND I WAS THE ONLY AU PAIR GARCON IN THE WHOLE QUARTER SO I WAS WITH ALL THESE SWEDISH AND GERMAN GIRLS AND IT WAS EVEN BETTER THAN BEING A MODEL AND GOING TO CORNELIA STREET SO I DID THAT FOR FIVE OR SO BLISSFUL MONTHS AND THEN I COME BACK TO THE STATES AND ITS LATE SPRING AND I GO VISIT SOME FRIENDS AT PRINCETON AND YET AGAIN I GET COMPLETELY DRUNK AND IN THE NEXT MORNING IM LYING ON THE FLOOR OF MY FRIENDS DORM ROOM AND IM TOTALLY DRUNK AND DESTROYED AND DEBAUCHED AND AH OH AND IM LYING ON THE HARD FLOOR AND THE SCHOOL NEWSPAPER COMES FLYING UNDERNEATH THE DOOR THERE WAS LIKE A BIT OF AN INCH COMES FLYING UNDER THE DOOR AND OPENS UP TO A PAGE TO THE CLASSIFIEDS AS IT SORT OF WAFTED ACROSS THE ROOM AND IT WAS RIGHT IN FRONT OF ME AND I LOOKED DOWN AND THERE WAS AN ADVERTISEMENT SAYING COME SPEND THE SUMMER IN NEW HAMPSHIRE AT CAMP THOREAU ON AN IDYLLIC LAKE AND I THOUGHT THIS IS WHAT I NEED I NEED TO STOP DRINKING IM ONLY TWENTY ONE BUT I ALREADY WANT TO STOP DRINKING IM SORRY GLENFIDDICH PEOPLE AND UH LET ME GO OFF TO NEW HAMPSHIRE TO CAMP THOREAU THATS WHAT I NEED TO DO SO I GO OFF TO CAMP THOREAU AND I GOT THE JOB BECAUSE ID BEEN AN AU PAIR SO I KNEW HOW TO DEAL WITH CHILDREN ID BEEN A CAMP COUNSELOR DURING THE SUMMERS IN NEW JERSEY AT A DAY CAMP SO I GET THE JOB I GO OFF TO CAMP THOREAU AND IM DOING PRETTY WELL AND THEN A FRIEND OF MINE SENDS ME LOLITA WHICH I HAD NEVER READ BEFORE WHICH IS NOT THE BEST SORT OF READING MATERIAL WHEN THERES LIKE TWELVE AND THIRTEEN YEAR OLD GIRLS RUNNING AROUND AND COMPLETELY DISTRACTED ME IN THE WRONG WAY AND UH SO ONE NIGHT I GOT VERY DRUNK AS ALWAYS AND ATE A SOME FOOD AND THE WHOLE NEXT DAY I WAS HUNG OVER FOR THE WHOLE DAY HAD A TERRIBLE HEADACHE INTO THE NIGHT AND I WAS SORT OF WALKING AROUND THE THE CAMP YOU KNOW TRYING TO CLEAR MY HEAD OF THIS HEADACHE THAT HAD LASTED ALL DAY AND I SEE A LIGHT ON AT THE UH ART COUNSELORS UH CABIN AND SO I GO TO SEE HER AND SHES IN HER SHE WAS THIRTY FOUR AND I SAID DO YOU HAVE AN ASPIRIN AND SHE YOU KNOW GETS ME AN ASPIRIN AND I HAD ADMIRED HER DOWN BY THE LAKE CAUSE I HAD NOTICED THAT YOU COULD SEE SOME PUBIS STICKING OUT FROM THE SIDES AND I THOUGHT THIS WAS A MORE APPROPRIATE TARGET FOR MY AFFECTIONS THAN THESE YOUNG GIRLS THAN THAT THAT NABOKOV INFLUENCE SO SHE GETS ME AN ASPIRIN AND WERE SORT OF CHATTING AND RIGHT IN THAT MOMENT A BAT FLIES RIGHT BETWEEN US AND WE BOTH SCREAM YOU KNOW OUT IN THE WOODS THERE IN NEW HAMPSHIRE AND I SAY WELL MAYBE IF WE AND ITS FLYING AROUND I SAY MAYBE IF WE TURN THE LIGHTS OFF I THINK BLATS ARE BL UH BATS ARE BLIND BUT IF WE TURN THE LIGHTS OFF ITLL SEE I DONT KNOW WHAT MY REASONING WAS SO I TURN THE LIGHTS OFF AND WE WAIT I TURN THE LIGHTS BACK ON NO BAT AND I FELT VERY MUCH YOU KNOW OH I REALLY AM A WOODSMAN A VERY MUCH A CAMP THOREAU FELLOW AND RIGHT AT THAT MOMENT OF HUBRIS AS ALWAYS THE BAT FLIES RIGHT BETWEEN US LIKE SOME SORT OF MAD CUPID GOES RIGHT BETWEEN US AND UH AND I TURN THE LIGHTS OFF AGAIN I PUT MY ARM AROUND HER AND ONE TOUCH LEADS TO ANOTHER I THINK BLAKE HAS A POEM ABOUT THAT SO ANYWAY UH WE END UP IN BED AND I REMEMBER THE RULE THAT I HAD LEARNED THE YEAR BEFORE WITH WITH ANOTHER OLDER WOMAN DO NOT ASK IF YOU CAN HAVE AN ULTIMATE MOMENT JUST GO AHEAD AND WHEN YOURE READY HAVE YOUR ULTIMATE MOMENT YOU KNOW APPROPRIATELY PERHAPS AFTER SHE MAY HAVE HAD ONE BUT I DONT KNOW IF I KNEW THAT AT TWENTY ONE SO I HAD I HAD TWO ULTIMATE MOMENTS THAT NIGHT IT WAS QUITE LOVELY AND UH BUT THE NEXT DAY IT TURNED OUT I WAS TERRIBLY ILL AND I ENDED UP HAVING SALMONELLA POISONING BECAUSE I HAD BEEN DRINKING THE YOU KNOW THE NIGHT BEFORE IN SOME BAR AND ATE SOMETHING OFF THE FLOOR WHO KNOWS WHAT BUT UM SO I HAD SALMONELLA POISONING ENDED UP IN DARTMOUTH HOSPITAL AND UH THE CAMP YEAR CAME TO AN END I NEVER MADE LOVE WITH THE ART COUNSELOR AGAIN BUT I YOU KNOW WE PARTED AMICABLY IT WAS VERY SWEET AND I GO BACK TO PRINCETON BECOME A CONSCIENTIOUS OBJECTOR I JUST I SOLD A NOVEL AFTER SENIOR YEAR I MOVED TO NEW YORK TO WORK ON THIS NOVEL AND I GET A LETTER FROM FAR AWAY UH IN THE UNITED STATES AND I OPEN IT UP AND IT TURNS OUT ITS A LETTER FROM THE ART COUNSELOR AND WITH THE LETTER IS A PICTURE OF A FIFTEEN MONTH OLD BABY BOY WITH RED HAIR AND BLUE EYES AND IM TOLD THAT ITS MY SON AND NINE MONTHS OF PREGNANCY FIFTEEN MONTHS OF HIS LIFE IT WAS EXACTLY TWO YEARS AGO AND I WAS LIKE OH MY GOD AND HE LOOKED EXACTLY LIKE I DID SO SURE ENOUGH THIS MOMENT THAT HAD BEGUN ON CORNELIA STREET AND OF COURSE YOU CAN AND THEN YOU KNOW AND THEN BEING AN AU PAIR AND GETTING A CAMP JOB AND ALL THIS DRINKING HAD LED TO ME LOOKING AT A A CHILD AND I WAS TWENTY THREE NOW AND SO I WENT TO WHERE UH THEY LIVED AND I BECAME HIS DAD AND UH IT TURNED OUT YOU KNOW ALL THIS AU PAIR AND COUNSELING HAD BEEN WONDERFUL TRAINING AND UH I DONT MEAN TO BE SENTIMENTAL BUT THATS WHY WE HAVE THE WORD CAUSE SOMETIMES IT HAPPENS SO I BECAME HIS DAD AND UM AND I HAVE TO SAY ITS THE BEST JOB IVE EVER HAD AND MY SON IS NOW SEVENTEEN HE COMES UP IN TWO DAYS TO BE WITH ME FOR TEN DAYS AND I SEE HIM ABOUT EVERY EIGHT WEEKS AND HES A BEAUTIFUL BOY AND WEVE HAD AN INCREDIBLE FATHER SON THING FOR FIFTEEN YEARS NOW SO THATS MY STORY AND AND IT ALL BEGAN WITH THESE JOBS AND SOMEHOW ENDED UP THERE RATHER BEAUTIFULLY SO ILL DO A HAIRY CALL\n"
     ]
    }
   ],
   "source": [
    "def get_textgrid_words(textgrid):\n",
    "    '''\n",
    "    Extracts the words in the textgrid to show in a legible format\n",
    "    '''\n",
    "    words = [strip_punctuation(x[-1]) for x in textgrid.getTier('word').entries]\n",
    "    return words\n",
    "\n",
    "# load the textgrid removing all enunciations\n",
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "\n",
    "# gets all the words in the textgrid as an interpretable string\n",
    "tg_words = get_textgrid_words(textgrid)\n",
    "print (' '.join(tg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b0cf1",
   "metadata": {},
   "source": [
    "## Create a transcript file\n",
    "\n",
    "ChatGPT will then print out a verion of the transcript with punctuation. However, we need to double-check that the words match the original transcript. After getting the transcript from ChatGPT:\n",
    "1. Go to the directory '/stimuli/transcripts/' \n",
    "2. Create a text file names \"STIMULUSNAME.txt\" (where STIMULUSNAME is the name of the stimulus - printed out above)\n",
    "3. Paste the transcript from ChatGPT into the text file\n",
    "\n",
    "You should now be able to load the file in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbb04d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_praat_to_transcript(words_original, words_transcribed):\n",
    "    '''\n",
    "    Compares words from TextGrid and ChatGPT transcript word by word\n",
    "    '''\n",
    "    \n",
    "    for i, (word_orig, word_transc) in enumerate(zip(words_original, words_transcribed)):\n",
    "        if word_orig.lower() != word_transc.lower():\n",
    "            print (f'Word index: {i}')\n",
    "            print (f'TextGrid word: {word_orig}')\n",
    "            print (f'Transcript word: {word_transc}')\n",
    "            print (f'Word context: {words_original[i-5:i+5]}')\n",
    "            break\n",
    "    \n",
    "    if i+1 == len(words_original):\n",
    "        print (f'Finished transcript!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25aa20",
   "metadata": {},
   "source": [
    "## Check the transcript with the original file\n",
    "\n",
    "Run the following cell to compare words from the TextGrid to words from the ChatGPT transcript.\n",
    "\n",
    "Sometimes words will be misaligned:\n",
    "- ChatGPT may have missed some words\n",
    "- The Praat words may be misspelled, or hyphenated words may have been treated separately (e.g., eighty-four --> eighty four)\n",
    "\n",
    "You will need to correct this in either 1) the transcript or 2) the Praat file and make note of the change within the tracking document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b6bd1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcript!\n"
     ]
    }
   ],
   "source": [
    "transcript_fn = os.path.join(stim_dir, 'transcripts', f'{stim_name}_transcript.txt')\n",
    "\n",
    "# load the textgrid and get all words\n",
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "words_original = get_textgrid_words(textgrid)\n",
    "\n",
    "# load the ChatGPT created transcript\n",
    "_, words_transcribed = load_transcription(transcript_fn)\n",
    "\n",
    "compare_praat_to_transcript(words_original, words_transcribed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a518f12",
   "metadata": {},
   "source": [
    "## Create a gentle align file from Praat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e074245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentle_stim_dir = os.path.join(stim_dir, 'gentle', stim_name)\n",
    "\n",
    "# if the directory does not exist, make the directory\n",
    "if not os.path.exists(gentle_stim_dir):\n",
    "    os.makedirs(gentle_stim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91240a9c",
   "metadata": {},
   "source": [
    "Now that the directory is created, we will do the following:\n",
    "- Write the aligned file to the directory\n",
    "- Move a copy of the stimulus audio to the directory\n",
    "- Move a copy of the transcript to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "819e0684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/stimuli/gentle/myfirstdaywiththeyankees/a.wav'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "praat_fn = praat_fns[file_num]\n",
    "transcript_fn = os.path.join(stim_dir, 'transcripts', f'{stim_name}_transcript.txt')\n",
    "\n",
    "# given the two files, creates a file in gentle aligned format\n",
    "align_json = textgrid_to_gentle(praat_fn, transcript_fn)\n",
    "\n",
    "# write the file out to the directory\n",
    "with open(os.path.join(gentle_stim_dir, 'align.json'), 'w') as f:\n",
    "    json.dump(align_json, f)\n",
    "    \n",
    "# copy the transcript file renaming it to \"transcript.txt\" matching gentle convention\n",
    "shutil.copyfile(\n",
    "    transcript_fn, \n",
    "    os.path.join(gentle_stim_dir, 'transcript.txt')\n",
    ")\n",
    "\n",
    "# copy the stimulus audio file renaming it to \"a.wav\" matching gentle convention\n",
    "shutil.copyfile(\n",
    "    os.path.join(stim_dir, 'audio', f'{stim_name}.wav'), \n",
    "    os.path.join(gentle_stim_dir, 'a.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f700b7",
   "metadata": {},
   "source": [
    "### old for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "id": "118b1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740\n",
      "2742\n"
     ]
    }
   ],
   "source": [
    "print (len(tg_words))\n",
    "print (len (words_transcribed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "id": "63bb53f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<praatio.data_classes.interval_tier.IntervalTier at 0x2b22e9c7f880>"
      ]
     },
     "execution_count": 1590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "id": "33d81c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'story',\n",
       " 'is',\n",
       " 'about',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'jobs',\n",
       " 'that',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'shaped',\n",
       " 'my',\n",
       " 'life',\n",
       " 'and',\n",
       " 'shaped',\n",
       " 'my',\n",
       " 'whole',\n",
       " 'destiny',\n",
       " 'And',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'jobs',\n",
       " 'I',\n",
       " 'had',\n",
       " 'uh',\n",
       " 'during',\n",
       " 'the',\n",
       " 'ages',\n",
       " 'of',\n",
       " 'twenty',\n",
       " 'to',\n",
       " 'twentyone',\n",
       " 'So',\n",
       " 'the',\n",
       " 'story',\n",
       " 'begins',\n",
       " 'in',\n",
       " 'in',\n",
       " 'uh',\n",
       " 'yes',\n",
       " 'I',\n",
       " 'we',\n",
       " 'yes',\n",
       " 'I',\n",
       " 'was',\n",
       " 'employed',\n",
       " 'back',\n",
       " 'then',\n",
       " 'Ok',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'why',\n",
       " 'you',\n",
       " 'laughed',\n",
       " 'but',\n",
       " 'Ill',\n",
       " 'Ill',\n",
       " 'accept',\n",
       " 'that',\n",
       " 'Its',\n",
       " 'a',\n",
       " 'good',\n",
       " 'sign',\n",
       " 'Ok',\n",
       " 'thinking',\n",
       " 'out',\n",
       " 'loud',\n",
       " 'here',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'in',\n",
       " 'nineteen',\n",
       " 'eightyfour',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'sophomore',\n",
       " 'at',\n",
       " 'Princeton',\n",
       " 'You',\n",
       " 'can',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'that',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'No',\n",
       " 'Alright',\n",
       " 'So',\n",
       " 'in',\n",
       " 'nineteen',\n",
       " 'eightyfour',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'sophomore',\n",
       " 'at',\n",
       " 'Princeton',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'year',\n",
       " 'off',\n",
       " 'because',\n",
       " 'I',\n",
       " 'had',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'reputation',\n",
       " 'at',\n",
       " 'school',\n",
       " 'I',\n",
       " 'was',\n",
       " 'drinking',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'getting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'trouble',\n",
       " 'So',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'year',\n",
       " 'off',\n",
       " 'Also',\n",
       " 'I',\n",
       " 'had',\n",
       " 'joined',\n",
       " 'the',\n",
       " 'army',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'Princeton',\n",
       " 'and',\n",
       " 'I',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'four',\n",
       " 'years',\n",
       " 'of',\n",
       " 'active',\n",
       " 'duty',\n",
       " 'after',\n",
       " 'school',\n",
       " 'and',\n",
       " 'eight',\n",
       " 'years',\n",
       " 'of',\n",
       " 'reserve',\n",
       " 'duty',\n",
       " 'So',\n",
       " 'this',\n",
       " 'was',\n",
       " 'my',\n",
       " 'last',\n",
       " 'chance',\n",
       " 'at',\n",
       " 'freedom',\n",
       " 'was',\n",
       " 'after',\n",
       " 's',\n",
       " 'you',\n",
       " 'know',\n",
       " 'was',\n",
       " 'taking',\n",
       " 'this',\n",
       " 'year',\n",
       " 'off',\n",
       " 'from',\n",
       " 'school',\n",
       " 'Later',\n",
       " 'just',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Harpers',\n",
       " 'crowd',\n",
       " 'I',\n",
       " 'did',\n",
       " 'become',\n",
       " 'a',\n",
       " 'conscientious',\n",
       " 'objector',\n",
       " 'and',\n",
       " 'so',\n",
       " 'I',\n",
       " 'got',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'military',\n",
       " 'So',\n",
       " 'and',\n",
       " 'avoided',\n",
       " 'the',\n",
       " 'Golf',\n",
       " 'War',\n",
       " 'unlike',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'ROTC',\n",
       " 'guys',\n",
       " 'which',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'but',\n",
       " 'this',\n",
       " 'was',\n",
       " 'before',\n",
       " 'I',\n",
       " 'knew',\n",
       " 'I',\n",
       " 'was',\n",
       " 'getting',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'army',\n",
       " 'This',\n",
       " 'was',\n",
       " 'gonna',\n",
       " 'be',\n",
       " 'my',\n",
       " 'year',\n",
       " 'of',\n",
       " 'freedom',\n",
       " 'I',\n",
       " 'nee',\n",
       " 'I',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'make',\n",
       " 'money',\n",
       " 'though',\n",
       " 'during',\n",
       " 'this',\n",
       " 'year',\n",
       " 'off',\n",
       " 'And',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'I',\n",
       " 'know',\n",
       " 'its',\n",
       " 'going',\n",
       " 'to',\n",
       " 'seem',\n",
       " 'absurd',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'I',\n",
       " 'be',\n",
       " 'a',\n",
       " 'model',\n",
       " 'And',\n",
       " 'he',\n",
       " 'took',\n",
       " 'some',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'I',\n",
       " 'know',\n",
       " 'I',\n",
       " 'have',\n",
       " 'rings',\n",
       " 'under',\n",
       " 'my',\n",
       " 'eyes',\n",
       " 'that',\n",
       " 'go',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'my',\n",
       " 'skull',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'no',\n",
       " 'hair',\n",
       " 'But',\n",
       " 'at',\n",
       " 'twenty',\n",
       " 'he',\n",
       " 'had',\n",
       " 'it',\n",
       " 'in',\n",
       " 'his',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'I',\n",
       " 'should',\n",
       " 'be',\n",
       " 'a',\n",
       " 'model',\n",
       " 'So',\n",
       " 'he',\n",
       " 'takes',\n",
       " 'some',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'I',\n",
       " 'must',\n",
       " 'admit',\n",
       " 'I',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'was',\n",
       " 'ugly',\n",
       " 'but',\n",
       " 'I',\n",
       " 'was',\n",
       " 'vain',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'try',\n",
       " 'it',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'what',\n",
       " 'I',\n",
       " 'was',\n",
       " 'thinking',\n",
       " 'So',\n",
       " 'I',\n",
       " 'go',\n",
       " 'I',\n",
       " 'look',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Princeton',\n",
       " 'library',\n",
       " 'they',\n",
       " 'had',\n",
       " 'a',\n",
       " 'Manhattan',\n",
       " 'phone',\n",
       " 'book',\n",
       " 'and',\n",
       " 'I',\n",
       " 'look',\n",
       " 'up',\n",
       " 'modeling',\n",
       " 'and',\n",
       " 'I',\n",
       " 'choose',\n",
       " 'a',\n",
       " 'modeling',\n",
       " 'agency',\n",
       " 'And',\n",
       " 'I',\n",
       " 'go',\n",
       " 'into',\n",
       " 'New',\n",
       " 'York',\n",
       " 'with',\n",
       " 'these',\n",
       " 'pictures',\n",
       " 'This',\n",
       " 'fellow',\n",
       " 'takes',\n",
       " 'them',\n",
       " 'He',\n",
       " 'says',\n",
       " 'Can',\n",
       " 'I',\n",
       " 'hold',\n",
       " 'these',\n",
       " 'overnight',\n",
       " 'I',\n",
       " 'said',\n",
       " 'yes',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'the',\n",
       " 'next',\n",
       " 'morning',\n",
       " 'he',\n",
       " 'calls',\n",
       " 'me',\n",
       " 'Come',\n",
       " 'into',\n",
       " 'New',\n",
       " 'York',\n",
       " 'we',\n",
       " 'want',\n",
       " 'to',\n",
       " 'sign',\n",
       " 'you',\n",
       " 'to',\n",
       " 'a',\n",
       " 'contract',\n",
       " 'Hed',\n",
       " 'sent',\n",
       " 'my',\n",
       " 'pictures',\n",
       " 'to',\n",
       " 'the',\n",
       " 'photographer',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'who',\n",
       " 'does',\n",
       " 'the',\n",
       " 'Abercormbie',\n",
       " 'and',\n",
       " 'Fitch',\n",
       " 'and',\n",
       " 'Vanity',\n",
       " 'Fair',\n",
       " 'However',\n",
       " 'you',\n",
       " 'say',\n",
       " 'Abercrombie',\n",
       " 'whatever',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'that',\n",
       " 'this',\n",
       " 'fellow',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'photograph',\n",
       " 'me',\n",
       " 'So',\n",
       " 'suddenly',\n",
       " 'with',\n",
       " 'this',\n",
       " 'small',\n",
       " 'agency',\n",
       " 'which',\n",
       " 'existed',\n",
       " 'for',\n",
       " 'about',\n",
       " 'a',\n",
       " 'year',\n",
       " 'and',\n",
       " 'then',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'scandal',\n",
       " 'with',\n",
       " 'young',\n",
       " 'female',\n",
       " 'models',\n",
       " 'from',\n",
       " 'Minnesota',\n",
       " 'closed',\n",
       " 'down',\n",
       " 'But',\n",
       " 'so',\n",
       " 'suddenly',\n",
       " 'Im',\n",
       " 'a',\n",
       " 'model',\n",
       " 'and',\n",
       " 'Im',\n",
       " 'going',\n",
       " 'off',\n",
       " 'to',\n",
       " 'this',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'And',\n",
       " 'I',\n",
       " 'remember',\n",
       " 'we',\n",
       " 'met',\n",
       " 'early',\n",
       " 'in',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'us',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'male',\n",
       " 'models',\n",
       " 'and',\n",
       " 'a',\n",
       " 'van',\n",
       " 'And',\n",
       " 'as',\n",
       " 'were',\n",
       " 'going',\n",
       " 'for',\n",
       " 'to',\n",
       " 'uh',\n",
       " 'the',\n",
       " 'Hamptons',\n",
       " 'for',\n",
       " 'this',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'its',\n",
       " 'like',\n",
       " 'six',\n",
       " 'am',\n",
       " 'this',\n",
       " 'one',\n",
       " 'guy',\n",
       " 'looks',\n",
       " 'around',\n",
       " 'hes',\n",
       " 'from',\n",
       " 'Texas',\n",
       " 'he',\n",
       " 'goes',\n",
       " 'Hey',\n",
       " 'were',\n",
       " 'all',\n",
       " 'blond',\n",
       " 'you',\n",
       " 'know',\n",
       " 'just',\n",
       " 'it',\n",
       " 'dawned',\n",
       " 'on',\n",
       " 'him',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'uh',\n",
       " 'as',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'interesting',\n",
       " 'And',\n",
       " 'so',\n",
       " 'we',\n",
       " 'get',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Hamptons',\n",
       " 'and',\n",
       " 'were',\n",
       " 'at',\n",
       " 'this',\n",
       " 'farmhouse',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'like',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'out',\n",
       " 'of',\n",
       " 'Christopher',\n",
       " 'Isherwood',\n",
       " 'The',\n",
       " 'Berlin',\n",
       " 'Stories',\n",
       " 'All',\n",
       " 'these',\n",
       " 'blond',\n",
       " 'boys',\n",
       " 'about',\n",
       " 'ten',\n",
       " 'of',\n",
       " 'us',\n",
       " 'running',\n",
       " 'around',\n",
       " 'doing',\n",
       " 'pushups',\n",
       " 'so',\n",
       " 'that',\n",
       " 'our',\n",
       " 'muscles',\n",
       " 'would',\n",
       " 'swell',\n",
       " 'and',\n",
       " 'in',\n",
       " 'and',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pool',\n",
       " 'and',\n",
       " 'a',\n",
       " 'big',\n",
       " 'buffet',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'the',\n",
       " 'light',\n",
       " 'to',\n",
       " 'change',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'so',\n",
       " 'I',\n",
       " 'get',\n",
       " 'my',\n",
       " 'picture',\n",
       " 'taken',\n",
       " 'by',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'Wonderful',\n",
       " 'guy',\n",
       " 'very',\n",
       " 'talented',\n",
       " 'obviously',\n",
       " 'And',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'me',\n",
       " 'to',\n",
       " 'uh',\n",
       " 'drop',\n",
       " 'my',\n",
       " 'shorts',\n",
       " 'and',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'embarrassed',\n",
       " 'I',\n",
       " 'didnt',\n",
       " 'think',\n",
       " 'I',\n",
       " 'could',\n",
       " 'do',\n",
       " 'that',\n",
       " 'And',\n",
       " 'so',\n",
       " 'I',\n",
       " 'anyways',\n",
       " 'I',\n",
       " 'hid',\n",
       " 'everything',\n",
       " 'I',\n",
       " 'I',\n",
       " 'wasnt',\n",
       " 'able',\n",
       " 'to',\n",
       " 'expose',\n",
       " 'myself',\n",
       " 'So',\n",
       " 'anyways',\n",
       " 'he',\n",
       " 'took',\n",
       " 'these',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'one',\n",
       " 'and',\n",
       " 'Ill',\n",
       " 'uh',\n",
       " 'Im',\n",
       " 'going',\n",
       " 'to',\n",
       " 'pass',\n",
       " 'it',\n",
       " 'out',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'you',\n",
       " 'Uh',\n",
       " 'I',\n",
       " 'only',\n",
       " 'have',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'of',\n",
       " 'them',\n",
       " 'so',\n",
       " 'if',\n",
       " 'one',\n",
       " 'per',\n",
       " 'table',\n",
       " 'But',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'I',\n",
       " 'was',\n",
       " 'the',\n",
       " 'uh',\n",
       " 'anyway',\n",
       " 'the',\n",
       " 'physique',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'good',\n",
       " 'then',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'see',\n",
       " 'I',\n",
       " 'had',\n",
       " 'hair',\n",
       " 'Its',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'depressing',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'that',\n",
       " 'now',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'but',\n",
       " 'uh',\n",
       " 'anyway',\n",
       " 'So',\n",
       " 'I',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Whitney',\n",
       " 'Biennial',\n",
       " 'I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'say',\n",
       " 'to',\n",
       " 'my',\n",
       " 'artist',\n",
       " 'friends',\n",
       " 'uh',\n",
       " 'Ive',\n",
       " 'been',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Biennial',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'in',\n",
       " 'this',\n",
       " 'picture',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'from',\n",
       " 'my',\n",
       " 'uh',\n",
       " 'nursery',\n",
       " 'school',\n",
       " 'just',\n",
       " 'so',\n",
       " 'you',\n",
       " 'can',\n",
       " 'study',\n",
       " 'that',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'So',\n",
       " 'ns',\n",
       " 'so',\n",
       " 'pass',\n",
       " 'those',\n",
       " 'around',\n",
       " 'so',\n",
       " 'maybe',\n",
       " 'one',\n",
       " 'per',\n",
       " 'table',\n",
       " 'since',\n",
       " 'theres',\n",
       " 'only',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'But',\n",
       " 'so',\n",
       " 'it',\n",
       " 'can',\n",
       " 'get',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'model',\n",
       " 'and',\n",
       " 'I',\n",
       " 'got',\n",
       " 'several',\n",
       " 'jobs',\n",
       " 'And',\n",
       " 'but',\n",
       " 'at',\n",
       " 'that',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'at',\n",
       " 'at',\n",
       " 'Bruce',\n",
       " 'Webers',\n",
       " 'farmhouse',\n",
       " 'I',\n",
       " 'met',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pretty',\n",
       " 'girl',\n",
       " 'She',\n",
       " 'was',\n",
       " 'his',\n",
       " 'assistant',\n",
       " 'and',\n",
       " 'she',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'her',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'So',\n",
       " 'about',\n",
       " 'a',\n",
       " 'week',\n",
       " 'later',\n",
       " 'Im',\n",
       " 'sp',\n",
       " 'doing',\n",
       " 'my',\n",
       " 'modeling',\n",
       " 'appointments',\n",
       " 'which',\n",
       " 'are',\n",
       " 'called',\n",
       " 'gosees',\n",
       " 'which',\n",
       " 'was',\n",
       " 'made',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'for',\n",
       " 'the',\n",
       " 'models',\n",
       " 'It',\n",
       " 'was',\n",
       " 'go',\n",
       " 'see',\n",
       " 'someone',\n",
       " 'And',\n",
       " 'so',\n",
       " 'this',\n",
       " 'was',\n",
       " 'what',\n",
       " 'they',\n",
       " 'called',\n",
       " 'appoi',\n",
       " 'uh',\n",
       " 'appointments',\n",
       " 'were',\n",
       " 'gosees',\n",
       " 'So',\n",
       " 'Im',\n",
       " 'on',\n",
       " 'a',\n",
       " 'day',\n",
       " 'of',\n",
       " 'gosees',\n",
       " 'and',\n",
       " 'I',\n",
       " 'found',\n",
       " 'myself',\n",
       " 'in',\n",
       " 'the',\n",
       " 'West',\n",
       " 'Village',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'right',\n",
       " 'by',\n",
       " 'Cornelia',\n",
       " 'Street',\n",
       " 'And',\n",
       " 'I',\n",
       " 'remembered',\n",
       " 'that',\n",
       " 'that',\n",
       " 'girl',\n",
       " 'lived',\n",
       " 'on',\n",
       " 'Cornelia',\n",
       " 'Street',\n",
       " 'And',\n",
       " 'I',\n",
       " 'my',\n",
       " 'high',\n",
       " 'school',\n",
       " 'love',\n",
       " 'was',\n",
       " 'a',\n",
       " 'girl',\n",
       " 'named',\n",
       " 'Cornelia',\n",
       " 'And',\n",
       " 'so',\n",
       " 'suddenly',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'fusing',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'So',\n",
       " 'I',\n",
       " 'call',\n",
       " 'her',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'I',\n",
       " 'get',\n",
       " 'her',\n",
       " 'roommate',\n",
       " 'and',\n",
       " 'I',\n",
       " 'explain',\n",
       " 'that',\n",
       " 'Im',\n",
       " 'in',\n",
       " 'the',\n",
       " 'neighborhood',\n",
       " 'and',\n",
       " 'is',\n",
       " 'I',\n",
       " 'wont',\n",
       " ...]"
      ]
     },
     "execution_count": 1591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_transcribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "id": "fd5e007c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1589], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m tg_words \u001b[38;5;241m=\u001b[39m textgrid\u001b[38;5;241m.\u001b[39mgetTier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m contents, words_transcribed \u001b[38;5;241m=\u001b[39m load_transcription(transcript_fn)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(tg_words) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(words_transcribed))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# create the dictionary to store things in\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# put the transcript in the raw form\u001b[39;00m\n\u001b[1;32m     10\u001b[0m align \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "tg_words = textgrid.getTier('word')\n",
    "\n",
    "contents, words_transcribed = load_transcription(transcript_fn)\n",
    "\n",
    "for tg_word, transcribed_word \n",
    "\n",
    "assert (len(tg_words) == len(words_transcribed))\n",
    "\n",
    "# create the dictionary to store things in\n",
    "# put the transcript in the raw form\n",
    "align = {}\n",
    "align['transcript'] = contents[0]\n",
    "align['words'] = []\n",
    "\n",
    "# Taken from Kaldi metasentence tokenizer\n",
    "# splits the transcript based on any punctuation besides for apostrophes and hyphens\n",
    "regex_split_pattern = r'(\\w|\\.\\w|\\:\\w|\\â€™\\w|\\'\\w|\\-\\w)+'\n",
    "\n",
    "iterator = list(re.finditer(regex_split_pattern, ''.join(contents), re.UNICODE))\n",
    "n_items = len(list(iterator))\n",
    "# make sure the iterator matches the length\n",
    "# assert (n_items == len(tg_words) == len(words_transcribed))\n",
    "\n",
    "\n",
    "## this block helps find what words are wrong\n",
    "for word_info, m in zip(tg_words, iterator):\n",
    "    \n",
    "    if word_info[-1].lower() != m.group().lower():\n",
    "        print (m)\n",
    "    \n",
    "sys.exit(0)\n",
    "\n",
    "# # if all matches we're good to go\n",
    "# for word_info, m in zip(tg_words, iterator):\n",
    "    \n",
    "#     # span of the word in characters relative to the overall string\n",
    "#     start_offset, end_offset = m.span()\n",
    "#     word = m.group()\n",
    "    \n",
    "#     # crop textgrid to the word\n",
    "#     cropped_grid = textgrid.crop(cropStart=word_info[0], cropEnd=word_info[1], mode='truncated', rebaseToZero=False)\n",
    "#     tg_phones = cropped_grid.getTier('phone').entries\n",
    "#     word_phones = []\n",
    "    \n",
    "#     for phone_info in tg_phones:\n",
    "#         phone = re.sub(r'\\d+', '', phone_info[-1])\n",
    "#         duration = phone_info[1] - phone_info[0]\n",
    "#         word_phones.append({'duration': duration, 'phone': phone})\n",
    "    \n",
    "#     word_align = {\n",
    "#         'alignedWord': word.lower(),\n",
    "#         \"case\": \"success\",\n",
    "#         'word': word,\n",
    "#         'start': word_info[0],\n",
    "#         'end': word_info[1],\n",
    "#         'phones': word_phones,\n",
    "#         \"startOffset\": start_offset,\n",
    "#         \"endOffset\": end_offset,\n",
    "#     }\n",
    "\n",
    "#     align['words'].append(word_align)\n",
    "\n",
    "# return align"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
