{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee711696",
   "metadata": {},
   "source": [
    "## Load modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.display import clear_output\n",
    "import os, sys, glob\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/utils/gentle')\n",
    "\n",
    "import gentle\n",
    "from narratives_utils import *\n",
    "from text_utils import get_pos_tags, get_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98153156",
   "metadata": {},
   "source": [
    "# Create function for loading transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dd292967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# generate the explained tags --> we will use these to make more sense of the outputs\n",
    "tags_explained = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "STOP_UTTERANCES = ['yes', 'well', 'oh', 'mhm', 'um', 'boom']\n",
    "\n",
    "STOP_WORDS.extend(STOP_UTTERANCES)\n",
    "\n",
    "# POS tag mapping, format: {Treebank tag (1st letter only): Wordnet}\n",
    "tagset_mapping = defaultdict(\n",
    "    lambda: 'n',   # defaults to noun\n",
    "    {\n",
    "        'N': 'n',  # noun types\n",
    "        'P': 'n',  # pronoun types, predeterminers\n",
    "        'V': 'v',  # verb types\n",
    "        'J': 'a',  # adjective types\n",
    "        'D': 'a',  # determiner\n",
    "        'R': 'r'   # adverb types\n",
    "    })\n",
    "\n",
    "def gentle_to_dataframe(align_fn, interpolate_missing_times=True, shift_onset=True):\n",
    "    \n",
    "    # load the alignment file\n",
    "    with open(align_fn, encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # grab the original transcript\n",
    "    transcript = data['transcript']\n",
    "    words_list = data['words']\n",
    "\n",
    "    # go and extract each word --> pos tagging here incorporates context\n",
    "    all_words = [word['word'] for word in words_list]\n",
    "    _, pos_tags = map(list,zip(*pos_tag(all_words)))\n",
    "\n",
    "    # go through each word\n",
    "    df_stack = []\n",
    "\n",
    "    for i, current_word in enumerate(words_list):\n",
    "        \n",
    "        tokens = word_tokenize(current_word['word'])\n",
    "        tag = tagset_mapping[pos_tags[i][0]]\n",
    "        lemmas = [lemmatizer.lemmatize(re.sub(\"[^a-zA-Z\\s-]+\", '', token.lower()), pos=tag) for token in tokens]\n",
    "        stop_word = all([lemma in STOP_WORDS for lemma in lemmas if lemma]) # evaluate if not empty string\n",
    "        \n",
    "        word_dict = {\n",
    "            'Word-Written': current_word['word'],\n",
    "            'POS': pos_tags[i], # extract pos_tag for the word\n",
    "            'POS-Definition': tags_explained[pos_tags[i]][0], # get the explained tag\n",
    "            'Punctuation': transcript[current_word['endOffset']:words_list[i+1]['startOffset']]\n",
    "                                      if i+1 < len(words_list) else transcript[current_word['endOffset']:], # punctuation following the word (use subsequent word)\n",
    "            'Stop-Word': stop_word, # true or false if a stopword\n",
    "        }\n",
    "\n",
    "        # make sure that we've aligned the word --> could also check its a word in the vocabulary\n",
    "        if 'alignedWord' in current_word:\n",
    "            aligned_dict = {\n",
    "                'Word-Vocab': current_word['alignedWord'],\n",
    "                'Onset': current_word['start']\n",
    "                'Offset': current_word['end'],\n",
    "                'Duration': current_word['end'] - current_word['start'], # calculate duration of the current word\n",
    "            }\n",
    "            word_dict.update(aligned_dict)\n",
    "        \n",
    "        df_stack.append(\n",
    "            pd.DataFrame(\n",
    "                word_dict,\n",
    "                index=[i],\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    df_stack = pd.concat(df_stack)\n",
    "    \n",
    "    if interpolate_missing_times:\n",
    "        df_stack['Onset'].interpolate()\n",
    "    \n",
    "    return df_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180b4d1",
   "metadata": {},
   "source": [
    "# Preprocessing steps to get punctuation aligned\n",
    "\n",
    "Currently, gentle removes punctuation (e.g., hyphens, quotations) from the transcript. We want to perform the following operations:\n",
    "1. Collapse over hyphenated words that make sense together\n",
    "2. Ignore named-entities (e.g., cities, people)\n",
    "3. Retain punctuation following words in the transcript for presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6474c",
   "metadata": {},
   "source": [
    "**Tory** run the following cells for these task names:\n",
    "- black\n",
    "- bronx\n",
    "- forgot\n",
    "- piemanpni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfe8b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'black' # replace this string with those above\n",
    "\n",
    "# set directories\n",
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/'\n",
    "# narratives_dir = '/dartfs/rc/lab/D/DBIC/DBIC/archive/narratives'\n",
    "\n",
    "gentle_dir = os.path.join(base_dir, 'stimuli', 'gentle')\n",
    "preproc_dir = os.path.join(base_dir, 'stimuli', 'preprocessed')\n",
    "\n",
    "task_out_dir = os.path.join(preproc_dir, task)\n",
    "\n",
    "if not os.path.exists(task_out_dir):\n",
    "    os.makedirs(task_out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fcc273",
   "metadata": {},
   "source": [
    "# Create the dataframe for the current task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5d22031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads the alignment file and parses variables of interest into dataframe\n",
    "df_task = gentle_to_dataframe(os.path.join(gentle_dir, task, 'align.json'), shift_onset=True)\n",
    "\n",
    "#save to file\n",
    "df_task_raw_fn = os.path.join(task_out_dir, f'{task}_transcript_raw-onset-shifted.csv')\n",
    "df_task.to_csv(df_task_raw_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e76f9bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word-Written</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS-Definition</th>\n",
       "      <th>Punctuation</th>\n",
       "      <th>Stop-Word</th>\n",
       "      <th>Word-Vocab</th>\n",
       "      <th>Onset</th>\n",
       "      <th>Offset</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>so</td>\n",
       "      <td>0.228000</td>\n",
       "      <td>0.623700</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>i</td>\n",
       "      <td>0.633083</td>\n",
       "      <td>1.247400</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>1.366167</td>\n",
       "      <td>2.277000</td>\n",
       "      <td>0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>2.280833</td>\n",
       "      <td>2.425500</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>junior</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective or numeral, ordinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>junior</td>\n",
       "      <td>2.431250</td>\n",
       "      <td>3.108600</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>in</td>\n",
       "      <td>3.113833</td>\n",
       "      <td>3.375900</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>college</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>college</td>\n",
       "      <td>3.381583</td>\n",
       "      <td>4.158000</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>when</td>\n",
       "      <td>WRB</td>\n",
       "      <td>Wh-adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>when</td>\n",
       "      <td>4.263333</td>\n",
       "      <td>4.969800</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>i</td>\n",
       "      <td>4.978167</td>\n",
       "      <td>5.039100</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>got</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>got</td>\n",
       "      <td>5.047583</td>\n",
       "      <td>5.276700</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>my</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>my</td>\n",
       "      <td>5.288917</td>\n",
       "      <td>5.534100</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>first</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective or numeral, ordinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>first</td>\n",
       "      <td>5.546750</td>\n",
       "      <td>6.276600</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>paying</td>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, present participle or gerund</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>paying</td>\n",
       "      <td>6.415500</td>\n",
       "      <td>7.583400</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>job</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>job</td>\n",
       "      <td>7.596167</td>\n",
       "      <td>8.217000</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>in</td>\n",
       "      <td>8.289167</td>\n",
       "      <td>8.840700</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>my</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>my</td>\n",
       "      <td>8.855583</td>\n",
       "      <td>9.107999</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>field</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>field</td>\n",
       "      <td>9.124999</td>\n",
       "      <td>9.890100</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>on</td>\n",
       "      <td>9.956750</td>\n",
       "      <td>10.395000</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>10.412500</td>\n",
       "      <td>10.513800</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>radio</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>radio</td>\n",
       "      <td>10.531500</td>\n",
       "      <td>11.246400</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>This</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>this</td>\n",
       "      <td>11.342000</td>\n",
       "      <td>11.850300</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>is</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>verb, present tense, 3rd person singular</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>is</td>\n",
       "      <td>11.870250</td>\n",
       "      <td>11.969099</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>not</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>not</td>\n",
       "      <td>11.989249</td>\n",
       "      <td>12.196800</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>an</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>an</td>\n",
       "      <td>12.217333</td>\n",
       "      <td>12.305700</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>internship</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>internship</td>\n",
       "      <td>12.326417</td>\n",
       "      <td>13.256099</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>I'm</td>\n",
       "      <td>NNP</td>\n",
       "      <td>noun, proper, singular</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>i'm</td>\n",
       "      <td>13.351749</td>\n",
       "      <td>13.899600</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>getting</td>\n",
       "      <td>VBG</td>\n",
       "      <td>verb, present participle or gerund</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>getting</td>\n",
       "      <td>13.923000</td>\n",
       "      <td>14.226300</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>14.250250</td>\n",
       "      <td>14.305500</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>check</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>.</td>\n",
       "      <td>False</td>\n",
       "      <td>check</td>\n",
       "      <td>14.329583</td>\n",
       "      <td>14.879700</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>It</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>it</td>\n",
       "      <td>15.151417</td>\n",
       "      <td>16.533000</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>16.560833</td>\n",
       "      <td>16.701299</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>a</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>a</td>\n",
       "      <td>16.729416</td>\n",
       "      <td>16.800299</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>country</td>\n",
       "      <td>16.835249</td>\n",
       "      <td>17.216100</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>and</td>\n",
       "      <td>17.245083</td>\n",
       "      <td>17.344799</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>western</td>\n",
       "      <td>JJ</td>\n",
       "      <td>adjective or numeral, ordinal</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>western</td>\n",
       "      <td>17.373999</td>\n",
       "      <td>17.730900</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>radio</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>radio</td>\n",
       "      <td>17.760750</td>\n",
       "      <td>18.126900</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>station</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>station</td>\n",
       "      <td>18.159083</td>\n",
       "      <td>18.671400</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>conjunction, coordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>and</td>\n",
       "      <td>19.021167</td>\n",
       "      <td>20.750400</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>my</td>\n",
       "      <td>PRP$</td>\n",
       "      <td>pronoun, possessive</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>my</td>\n",
       "      <td>20.785333</td>\n",
       "      <td>20.938500</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>job</td>\n",
       "      <td>NN</td>\n",
       "      <td>noun, common, singular or mass</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>job</td>\n",
       "      <td>20.973750</td>\n",
       "      <td>21.364200</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>though</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>though</td>\n",
       "      <td>21.401833</td>\n",
       "      <td>21.621600</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>it</td>\n",
       "      <td>PRP</td>\n",
       "      <td>pronoun, personal</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>it</td>\n",
       "      <td>21.658000</td>\n",
       "      <td>21.720600</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>21.758833</td>\n",
       "      <td>21.958200</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>only</td>\n",
       "      <td>RB</td>\n",
       "      <td>adverb</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>only</td>\n",
       "      <td>21.995167</td>\n",
       "      <td>22.482900</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "      <td>preposition or conjunction, subordinating</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>on</td>\n",
       "      <td>22.565750</td>\n",
       "      <td>22.948200</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>determiner</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>the</td>\n",
       "      <td>22.986833</td>\n",
       "      <td>23.057100</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>weekends</td>\n",
       "      <td>NNS</td>\n",
       "      <td>noun, common, plural</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>weekends</td>\n",
       "      <td>23.095917</td>\n",
       "      <td>23.859000</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>was</td>\n",
       "      <td>VBD</td>\n",
       "      <td>verb, past tense</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>was</td>\n",
       "      <td>23.979167</td>\n",
       "      <td>24.591600</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>\"to\" as preposition or infinitive marker</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>to</td>\n",
       "      <td>24.633000</td>\n",
       "      <td>24.740100</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>play</td>\n",
       "      <td>VB</td>\n",
       "      <td>verb, base form</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>play</td>\n",
       "      <td>24.781750</td>\n",
       "      <td>25.433100</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word-Written   POS                             POS-Definition Punctuation  \\\n",
       "0            So    RB                                     adverb               \n",
       "1             I   PRP                          pronoun, personal               \n",
       "2           was   VBD                           verb, past tense               \n",
       "3             a    DT                                 determiner               \n",
       "4        junior    JJ              adjective or numeral, ordinal               \n",
       "5            in    IN  preposition or conjunction, subordinating               \n",
       "6       college    NN             noun, common, singular or mass               \n",
       "7          when   WRB                                  Wh-adverb               \n",
       "8             I   PRP                          pronoun, personal               \n",
       "9           got   VBD                           verb, past tense               \n",
       "10           my  PRP$                        pronoun, possessive               \n",
       "11        first    JJ              adjective or numeral, ordinal               \n",
       "12       paying   VBG         verb, present participle or gerund               \n",
       "13          job    NN             noun, common, singular or mass               \n",
       "14           in    IN  preposition or conjunction, subordinating               \n",
       "15           my  PRP$                        pronoun, possessive               \n",
       "16        field    NN             noun, common, singular or mass               \n",
       "17           on    IN  preposition or conjunction, subordinating               \n",
       "18          the    DT                                 determiner               \n",
       "19        radio    NN             noun, common, singular or mass          .    \n",
       "20         This    DT                                 determiner               \n",
       "21           is   VBZ   verb, present tense, 3rd person singular               \n",
       "22          not    RB                                     adverb               \n",
       "23           an    DT                                 determiner               \n",
       "24   internship    NN             noun, common, singular or mass          ,    \n",
       "25          I'm   NNP                     noun, proper, singular               \n",
       "26      getting   VBG         verb, present participle or gerund               \n",
       "27            a    DT                                 determiner               \n",
       "28        check    NN             noun, common, singular or mass          .    \n",
       "29           It   PRP                          pronoun, personal               \n",
       "30          was   VBD                           verb, past tense               \n",
       "31            a    DT                                 determiner               \n",
       "32      country    NN             noun, common, singular or mass               \n",
       "33          and    CC                  conjunction, coordinating               \n",
       "34      western    JJ              adjective or numeral, ordinal               \n",
       "35        radio    NN             noun, common, singular or mass               \n",
       "36      station    NN             noun, common, singular or mass               \n",
       "37          and    CC                  conjunction, coordinating               \n",
       "38           my  PRP$                        pronoun, possessive               \n",
       "39          job    NN             noun, common, singular or mass          ,    \n",
       "40       though    IN  preposition or conjunction, subordinating               \n",
       "41           it   PRP                          pronoun, personal               \n",
       "42          was   VBD                           verb, past tense               \n",
       "43         only    RB                                     adverb               \n",
       "44           on    IN  preposition or conjunction, subordinating               \n",
       "45          the    DT                                 determiner               \n",
       "46     weekends   NNS                       noun, common, plural          ,    \n",
       "47          was   VBD                           verb, past tense               \n",
       "48           to    TO   \"to\" as preposition or infinitive marker               \n",
       "49         play    VB                            verb, base form               \n",
       "\n",
       "    Stop-Word  Word-Vocab      Onset     Offset  Duration  \n",
       "0        True          so   0.228000   0.623700      0.39  \n",
       "1        True           i   0.633083   1.247400      0.58  \n",
       "2        True         was   1.366167   2.277000      0.34  \n",
       "3        True           a   2.280833   2.425500      0.15  \n",
       "4       False      junior   2.431250   3.108600      0.68  \n",
       "5        True          in   3.113833   3.375900      0.27  \n",
       "6       False     college   3.381583   4.158000      0.79  \n",
       "7        True        when   4.263333   4.969800      0.23  \n",
       "8        True           i   4.978167   5.039100      0.07  \n",
       "9       False         got   5.047583   5.276700      0.24  \n",
       "10       True          my   5.288917   5.534100      0.24  \n",
       "11      False       first   5.546750   6.276600      0.73  \n",
       "12      False      paying   6.415500   7.583400      0.55  \n",
       "13      False         job   7.596167   8.217000      0.64  \n",
       "14       True          in   8.289167   8.840700      0.28  \n",
       "15       True          my   8.855583   9.107999      0.27  \n",
       "16      False       field   9.124999   9.890100      0.78  \n",
       "17       True          on   9.956750  10.395000      0.21  \n",
       "18       True         the  10.412500  10.513800      0.12  \n",
       "19      False       radio  10.531500  11.246400      0.74  \n",
       "20       True        this  11.342000  11.850300      0.15  \n",
       "21       True          is  11.870250  11.969099      0.12  \n",
       "22       True         not  11.989249  12.196800      0.23  \n",
       "23       True          an  12.217333  12.305700      0.11  \n",
       "24      False  internship  12.326417  13.256099      0.96  \n",
       "25       True         i'm  13.351749  13.899600      0.21  \n",
       "26      False     getting  13.923000  14.226300      0.33  \n",
       "27       True           a  14.250250  14.305500      0.08  \n",
       "28      False       check  14.329583  14.879700      0.58  \n",
       "29       True          it  15.151417  16.533000      0.19  \n",
       "30       True         was  16.560833  16.701299      0.17  \n",
       "31       True           a  16.729416  16.800299      0.10  \n",
       "32      False     country  16.835249  17.216100      0.38  \n",
       "33       True         and  17.245083  17.344799      0.13  \n",
       "34      False     western  17.373999  17.730900      0.39  \n",
       "35      False       radio  17.760750  18.126900      0.40  \n",
       "36      False     station  18.159083  18.671400      0.54  \n",
       "37       True         and  19.021167  20.750400      0.19  \n",
       "38       True          my  20.785333  20.938500      0.19  \n",
       "39      False         job  20.973750  21.364200      0.43  \n",
       "40      False      though  21.401833  21.621600      0.25  \n",
       "41       True          it  21.658000  21.720600      0.10  \n",
       "42       True         was  21.758833  21.958200      0.23  \n",
       "43       True        only  21.995167  22.482900      0.53  \n",
       "44       True          on  22.565750  22.948200      0.20  \n",
       "45       True         the  22.986833  23.057100      0.11  \n",
       "46      False    weekends  23.095917  23.859000      0.81  \n",
       "47       True         was  23.979167  24.591600      0.26  \n",
       "48       True          to  24.633000  24.740100      0.15  \n",
       "49      False        play  24.781750  25.433100      0.70  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c90f78",
   "metadata": {},
   "source": [
    "# Clean hyphens\n",
    "\n",
    "Run this cell. You will be presented with a hyphenated word and its place within a sentence. You will need to enter \"y\" or \"n\" depending on if the word is meant to be hyphenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hyphenated_words(df_task):\n",
    "    \n",
    "    print (\"You will see a hyphenated word. Enter \\'y' if the word is meant to be hyphenated or \\'n' if not.\\n\")\n",
    "    \n",
    "    hyphenated = df_task['Punctuation'].str.contains('-')\n",
    "    hyphenated_idxs = np.where(hyphenated)[0]\n",
    "    \n",
    "    for idx in hyphenated_idxs:\n",
    "        # grab the current row and following row from the dataframe\n",
    "        df_rows = df_task.iloc[idx:idx+2]\n",
    "        hyphenated_word = '-'.join(df_rows['Word-Written'])\n",
    "        \n",
    "        # establish some context for the word\n",
    "        precontext = ' '.join(df_task.iloc[idx-10:idx]['Word-Written'])\n",
    "        postcontext = ' '.join(df_task.iloc[idx+2:idx+10]['Word-Written'])\n",
    "        print (f'\\nContext: {precontext} ___ {postcontext}')\n",
    "        print (f'Word: {hyphenated_word}')\n",
    "\n",
    "        response = input()\n",
    "    \n",
    "        if response == 'y':\n",
    "            \n",
    "            hyphenated_entry = {\n",
    "                'Word-Written': hyphenated_word,\n",
    "                'POS': df_rows['POS'].to_list()[-1],\n",
    "                'POS-Definition': df_rows['POS-Definition'].to_list()[-1],\n",
    "                'Punctuation': df_rows['Punctuation'].to_list()[-1] ,\n",
    "                'Stop-Word': hyphenated_word.lower() in STOP_WORDS,\n",
    "                'Word-Vocab': hyphenated_word,\n",
    "                'Onset': df_rows['Onset'].to_list()[0],\n",
    "                'Offset': df_rows['Offset'].to_list()[-1],\n",
    "                'Duration': df_rows['Offset'].to_list()[-1] - df_rows['Onset'].to_list()[0]\n",
    "            }\n",
    "            \n",
    "            df_task.at[idx, :] = hyphenated_entry\n",
    "            df_task = df_task.drop(idx+1).reset_index(drop=True)\n",
    "\n",
    "            # we've dropped an index\n",
    "            hyphenated_idxs -= 1 \n",
    "            \n",
    "            print (f'Word updated to: {hyphenated_word}')\n",
    "        else:\n",
    "            # otherwise add padding on each side to ensure it's not hyphenated\n",
    "            df_task.at[idx, 'Punctuation'] =  ' - '\n",
    "            \n",
    "            hyphenated_word = ' - '.join(df_rows['Word-Written'])\n",
    "            print (f'Words separated to: {hyphenated_word}')\n",
    "        \n",
    "#     clear_output(wait=True)\n",
    "        \n",
    "df_task = df_task.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e2327b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will see a hyphenated word. Enter 'y' if the word is meant to be hyphenated or 'n' if not.\n",
      "\n",
      "\n",
      "Context: the week each Sunday It actually came on an album ___ so I had to take it out of\n",
      "Word: pre-recorded\n",
      "\n",
      "\n",
      "Context: of my dreams and people still don't know who I ___ of who I am So I hatch the\n",
      "Word: am-all\n",
      "\n",
      "\n",
      "Context: who I am So I hatch the secret plan This ___ didn't tell anybody about it I am going\n",
      "Word: mission-I\n",
      "\n",
      "\n",
      "Context: Essence Yes you get it Essence Magazine of course targets ___ women and so I think surely now they\n",
      "Word: African-American\n",
      "\n",
      "\n",
      "Context: and they're from Mississippi My siblings all speak this way ___ talent no training no classes none of that\n",
      "Word: God-given\n",
      "\n",
      "\n",
      "Context: the time I lived in Kansas City Missouri but my ___ at the time had never heard of them\n",
      "Word: co-host\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hyphenated = df_task['Punctuation'].str.contains('-')\n",
    "hyphenated_idxs = np.where(hyphenated)[0]\n",
    "\n",
    "for idx in hyphenated_idxs:\n",
    "    # grab the current row and following row from the dataframe\n",
    "    df_rows = df_task.iloc[idx:idx+2]\n",
    "    hyphenated_word = '-'.join(df_rows['Word-Written'])\n",
    "    precontext = ' '.join(df_task.iloc[idx-10:idx]['Word-Written'])\n",
    "    postcontext = ' '.join(df_task.iloc[idx+2:idx+10]['Word-Written'])\n",
    "    print (f'\\nContext: {precontext} ___ {postcontext}')\n",
    "    print (f'Word: {hyphenated_word}')\n",
    "    \n",
    "    response = input()\n",
    "    \n",
    "    if response == 'y':\n",
    "        \n",
    "        hyphenated_entry = {\n",
    "            'Word-Written': hyphenated_word,\n",
    "            'POS': df_rows['POS'].to_list()[-1],\n",
    "            'POS-Definition': df_rows['POS-Definition'].to_list()[-1],\n",
    "            'Punctuation': df_rows['Punctuation'].to_list()[-1] ,\n",
    "            'Stop-Word': hyphenated_word.lower() in STOP_WORDS,\n",
    "            'Word-Vocab': hyphenated_word,\n",
    "            'Onset': df_rows['Onset'].to_list()[0],\n",
    "            'Offset': df_rows['Offset'].to_list()[-1],\n",
    "            'Duration': df_rows['Offset'].to_list()[-1] - df_rows['Onset'].to_list()[0]\n",
    "        }\n",
    "        print (df_rows['Punctuation'].to_list()[-1] )\n",
    "#         print (hyphenated_entry['Punctuation'])\n",
    "        df_task.at[idx, :] = hyphenated_entry\n",
    "        df_task = df_task.drop(idx+1).reset_index(drop=True)\n",
    "        \n",
    "        # we've dropped an index\n",
    "        hyphenated_idxs -= 1 \n",
    "    else:\n",
    "        # otherwise add padding on each side to ensure it's not hyphenated\n",
    "        df_task.at[idx, 'Punctuation'] =  ' - '\n",
    "        \n",
    "#     clear_output(wait=True)\n",
    "        \n",
    "df_task = df_task.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d4b3b4",
   "metadata": {},
   "source": [
    "# Clean named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1fd873e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will see a potential named entity (e.g., person, place). Enter 'y' if the word is or refers to a named entity and 'n' otherwise.\n",
      "\n",
      "\n",
      "Context: to play the top country hits of the week each ___ It actually came on an album pre recorded so\n",
      "Word: Sunday\n",
      "\n",
      "\n",
      "Context: my moment I get to read the weather live for ___ City Missouri Then I played commercials and while the\n",
      "Word: Jefferson\n",
      "\n",
      "\n",
      "Context: moment I get to read the weather live for Jefferson ___ Missouri Then I played commercials and while the commercials\n",
      "Word: City\n",
      "\n",
      "\n",
      "Context: I get to read the weather live for Jefferson City ___ Then I played commercials and while the commercials were\n",
      "Word: Missouri\n",
      "\n",
      "\n",
      "Context: break ends My career would eventually bring me here to ___ Louis It's the largest market I had ever been\n",
      "Word: St\n",
      "\n",
      "\n",
      "Context: ends My career would eventually bring me here to St ___ It's the largest market I had ever been in\n",
      "Word: Louis\n",
      "\n",
      "\n",
      "Context: events I promise you it was actually here at the ___ in St Louis Missouri a young man approached me\n",
      "Word: Sheldon\n",
      "\n",
      "\n",
      "Context: promise you it was actually here at the Sheldon in ___ Louis Missouri a young man approached me outside on\n",
      "Word: St\n",
      "\n",
      "\n",
      "Context: you it was actually here at the Sheldon in St ___ Missouri a young man approached me outside on the\n",
      "Word: Louis\n",
      "\n",
      "\n",
      "Context: it was actually here at the Sheldon in St Louis ___ a young man approached me outside on the sidewalk\n",
      "Word: Missouri\n",
      "\n",
      "\n",
      "Context: about this debate apparently that had been going on in ___ Louis when I first started here People told him\n",
      "Word: St\n",
      "\n",
      "\n",
      "Context: this debate apparently that had been going on in St ___ when I first started here People told him Oh\n",
      "Word: Louis\n",
      "\n",
      "\n",
      "Context: going on in St Louis when I first started here ___ told him Oh no no she's white man she's\n",
      "Word: People\n",
      "\n",
      "\n",
      "Context: first hint is me discussing an article I'd read in ___ magazine Anybody Essence Yes you get it Essence Magazine\n",
      "Word: Essence\n",
      "\n",
      "\n",
      "Context: is me discussing an article I'd read in Essence magazine ___ Essence Yes you get it Essence Magazine of course\n",
      "Word: Anybody\n",
      "\n",
      "\n",
      "Context: me discussing an article I'd read in Essence magazine Anybody ___ Yes you get it Essence Magazine of course targets\n",
      "Word: Essence\n",
      "\n",
      "\n",
      "Context: read in Essence magazine Anybody Essence Yes you get it ___ Magazine of course targets African American women and so\n",
      "Word: Essence\n",
      "\n",
      "\n",
      "Context: in Essence magazine Anybody Essence Yes you get it Essence ___ of course targets African American women and so I\n",
      "Word: Magazine\n",
      "\n",
      "\n",
      "Context: now they know They know They got to know now ___ put to a rest So I am now all\n",
      "Word: Debate\n",
      "\n",
      "\n",
      "Context: at the grocery store I'm chasing my kids at the ___ Louis zoo yelling at them people are recognizing my\n",
      "Word: St\n",
      "\n",
      "\n",
      "Context: the grocery store I'm chasing my kids at the St ___ zoo yelling at them people are recognizing my voice\n",
      "Word: Louis\n",
      "\n",
      "\n",
      "Context: Have you had any training She said Well I said ___ My father speaks this way my mother speaks this\n",
      "Word: Nope\n",
      "\n",
      "\n",
      "Context: this way my mother speaks this way and they're from ___ My siblings all speak this way God given talent\n",
      "Word: Mississippi\n",
      "\n",
      "\n",
      "Context: and they're from Mississippi My siblings all speak this way ___ given talent no training no classes none of that\n",
      "Word: God\n",
      "\n",
      "\n",
      "Context: engage in a drop the mic moment It's closer to ___ now when I drop this hint and I describe\n",
      "Word: Thanksgiving\n",
      "\n",
      "\n",
      "Context: the cornbread the night before and then you soak it ___ dressing and I thought boom mic moment they got\n",
      "Word: Cornbread\n",
      "\n",
      "\n",
      "Context: with these two women These two petite older black women ___ hair quaffed One had a single strand of pearls\n",
      "Word: Gray\n",
      "\n",
      "\n",
      "Context: woman making her mark on this big radio station in ___ Louis But I'm looking at them They have seen\n",
      "Word: St\n",
      "\n",
      "\n",
      "Context: making her mark on this big radio station in St ___ But I'm looking at them They have seen and\n",
      "Word: Louis\n",
      "\n",
      "\n",
      "Context: one I knew of from the time I lived in ___ City Missouri but my co host at the time\n",
      "Word: Kansas\n",
      "\n",
      "\n",
      "Context: I knew of from the time I lived in Kansas ___ Missouri but my co host at the time had\n",
      "Word: City\n",
      "\n",
      "\n",
      "Context: knew of from the time I lived in Kansas City ___ but my co host at the time had never\n",
      "Word: Missouri\n",
      "\n",
      "\n",
      "Context: Well I I am that woman You are I’m I'm ___ Daniel And and there it is again He's looking\n",
      "Word: Carol\n",
      "\n",
      "\n",
      "Context: I I am that woman You are I’m I'm Carol ___ And and there it is again He's looking at\n",
      "Word: Daniel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"You will see a potential named entity (e.g., person, place). Enter \\'y' if the word is or refers to a named entity and \\'n' otherwise.\\n\")\n",
    "    # fix instructions?\n",
    "named_entities = pd.Series(df_task['POS'] == 'NNP') & pd.Series(df_task['Stop-Word'] == False)\n",
    "named_entity_idxs = np.where(named_entities)[0]\n",
    "df_task['Named-Entity'] = False\n",
    "\n",
    "for idx in named_entity_idxs:\n",
    "    # grab the current row and following row from the dataframe\n",
    "    df_rows = df_task.iloc[idx]\n",
    "    ne_word = df_rows['Word-Written']\n",
    "    precontext = ' '.join(df_task.iloc[idx-10:idx]['Word-Written'])\n",
    "    postcontext = ' '.join(df_task.iloc[idx+1:idx+10]['Word-Written'])\n",
    "    print (f'\\nContext: {precontext} ___ {postcontext}')\n",
    "    print (f'Word: {ne_word}')\n",
    "    \n",
    "    response = input()\n",
    "    \n",
    "    if response == 'y':\n",
    "        df_task.at[idx, 'Named-Entity'] = True\n",
    "    \n",
    "#     clear_output(wait=True)\n",
    "\n",
    "df_task = df_task.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20f834d",
   "metadata": {},
   "source": [
    "# Add a column indiciating what words to have people predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "15cc95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidates are those that aren't named entities or stop-words\n",
    "\n",
    "if task == 'nwp_practice_trial':\n",
    "    practice_indices = df_task['Word-Written'].isin(['practice', 'recording', 'word'])\n",
    "    df_task['NWP-Candidate'] =  practice_indices\n",
    "elif task == 'example_trial':\n",
    "    example_indices = df_task['Word-Written'].isin(['fox', 'lazy'])\n",
    "    df_task['NWP-Candidate'] =  example_indices\n",
    "else:\n",
    "    df_task['NWP-Candidate'] = pd.Series(df_task['Named-Entity'] == False) & pd.Series(df_task['Stop-Word'] == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0e3bfd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junior\n",
      "college\n",
      "got\n",
      "first\n",
      "paying\n",
      "job\n",
      "field\n",
      "radio\n",
      "internship\n",
      "getting\n",
      "check\n",
      "country\n",
      "western\n",
      "radio\n",
      "station\n",
      "job\n",
      "though\n",
      "weekends\n",
      "play\n",
      "top\n",
      "country\n",
      "hits\n",
      "week\n",
      "Sunday\n",
      "actually\n",
      "came\n",
      "album\n",
      "pre\n",
      "recorded\n",
      "take\n",
      "sleeve\n",
      "put\n",
      "turntable\n",
      "put\n",
      "needle\n",
      "side\n",
      "one\n",
      "caring\n",
      "scratch\n",
      "let\n",
      "part\n",
      "one\n",
      "play\n",
      "moment\n",
      "coming\n",
      "training\n",
      "coming\n",
      "side\n",
      "one\n",
      "would\n",
      "end\n",
      "lift\n",
      "needle\n",
      "moment\n",
      "get\n",
      "read\n",
      "weather\n",
      "live\n",
      "Jefferson\n",
      "City\n",
      "Missouri\n",
      "played\n",
      "commercials\n",
      "commercials\n",
      "playing\n",
      "flip\n",
      "album\n",
      "put\n",
      "back\n",
      "turntable\n",
      "put\n",
      "needle\n",
      "back\n",
      "side\n",
      "two\n",
      "let\n",
      "part\n",
      "two\n",
      "top\n",
      "country\n",
      "hits\n",
      "week\n",
      "play\n",
      "know\n",
      "getting\n",
      "degree\n",
      "communications\n",
      "broadcasting\n",
      "teach\n",
      "kinds\n",
      "stuff\n",
      "teach\n",
      "flip\n",
      "album\n",
      "record\n",
      "time\n",
      "commercial\n",
      "break\n",
      "ends\n",
      "career\n",
      "would\n",
      "eventually\n",
      "bring\n",
      "St\n",
      "Louis\n",
      "largest\n",
      "market\n",
      "ever\n",
      "excited\n",
      "nervous\n",
      "trying\n",
      "get\n",
      "know\n",
      "community\n",
      "going\n",
      "every\n",
      "fundraiser\n",
      "every\n",
      "event\n",
      "think\n",
      "one\n",
      "events\n",
      "promise\n",
      "actually\n",
      "Sheldon\n",
      "St\n",
      "Louis\n",
      "Missouri\n",
      "young\n",
      "man\n",
      "approached\n",
      "outside\n",
      "sidewalk\n",
      "black\n",
      "said\n",
      "knew\n",
      "black\n",
      "keep\n",
      "mind\n",
      "heard\n",
      "pretty\n",
      "much\n",
      "whole\n",
      "adult\n",
      "life\n",
      "sound\n",
      "white\n",
      "heard\n",
      "heard\n",
      "sound\n",
      "white\n",
      "sound\n",
      "like\n",
      "white\n",
      "girl\n",
      "heard\n",
      "never\n",
      "ever\n",
      "heard\n",
      "knew\n",
      "black\n",
      "tells\n",
      "debate\n",
      "apparently\n",
      "going\n",
      "St\n",
      "Louis\n",
      "first\n",
      "started\n",
      "People\n",
      "told\n",
      "white\n",
      "man\n",
      "white\n",
      "sounds\n",
      "white\n",
      "white\n",
      "convinced\n",
      "never\n",
      "met\n",
      "black\n",
      "turns\n",
      "right\n",
      "black\n",
      "whole\n",
      "debate\n",
      "sort\n",
      "messed\n",
      "head\n",
      "little\n",
      "bit\n",
      "thought\n",
      "major\n",
      "market\n",
      "fantastic\n",
      "job\n",
      "dreams\n",
      "people\n",
      "still\n",
      "don't\n",
      "know\n",
      "hatch\n",
      "secret\n",
      "plan\n",
      "mission\n",
      "didn't\n",
      "tell\n",
      "anybody\n",
      "going\n",
      "start\n",
      "dropping\n",
      "hints\n",
      "air\n",
      "people\n",
      "people\n",
      "know\n",
      "first\n",
      "hint\n",
      "discussing\n",
      "article\n",
      "read\n",
      "Essence\n",
      "magazine\n",
      "Anybody\n",
      "Essence\n",
      "get\n",
      "Essence\n",
      "Magazine\n",
      "course\n",
      "targets\n",
      "African\n",
      "American\n",
      "women\n",
      "think\n",
      "surely\n",
      "know\n",
      "know\n",
      "got\n",
      "know\n",
      "Debate\n",
      "put\n",
      "rest\n",
      "town\n",
      "people\n",
      "beginning\n",
      "recognize\n",
      "voice\n",
      "grocery\n",
      "store\n",
      "chasing\n",
      "kids\n",
      "St\n",
      "Louis\n",
      "zoo\n",
      "yelling\n",
      "people\n",
      "recognizing\n",
      "voice\n",
      "gonna\n",
      "drop\n",
      "another\n",
      "hint\n",
      "air\n",
      "enjoying\n",
      "secret\n",
      "mission\n",
      "described\n",
      "trying\n",
      "make\n",
      "mother's\n",
      "collard\n",
      "greens\n",
      "got\n",
      "know\n",
      "got\n",
      "know\n",
      "black\n",
      "woman\n",
      "One\n",
      "day\n",
      "one\n",
      "many\n",
      "events\n",
      "education\n",
      "panel\n",
      "discussion\n",
      "small\n",
      "group\n",
      "people\n",
      "talking\n",
      "older\n",
      "white\n",
      "gentleman\n",
      "walks\n",
      "says\n",
      "speak\n",
      "people\n",
      "could\n",
      "feel\n",
      "said\n",
      "could\n",
      "feel\n",
      "stomach\n",
      "rising\n",
      "heart\n",
      "throat\n",
      "said\n",
      "people\n",
      "referring\n",
      "knew\n",
      "meant\n",
      "know\n",
      "meant\n",
      "wanted\n",
      "say\n",
      "mouth\n",
      "wanted\n",
      "say\n",
      "said\n",
      "know\n",
      "black\n",
      "people\n",
      "really\n",
      "gurgling\n",
      "threatening\n",
      "come\n",
      "spewing\n",
      "mouth\n",
      "embarrassed\n",
      "people\n",
      "standing\n",
      "around\n",
      "hearing\n",
      "angry\n",
      "dare\n",
      "diminish\n",
      "indignant\n",
      "even\n",
      "could\n",
      "say\n",
      "another\n",
      "word\n",
      "says\n",
      "training\n",
      "said\n",
      "said\n",
      "Nope\n",
      "father\n",
      "speaks\n",
      "way\n",
      "mother\n",
      "speaks\n",
      "way\n",
      "Mississippi\n",
      "siblings\n",
      "speak\n",
      "way\n",
      "God\n",
      "given\n",
      "talent\n",
      "training\n",
      "classes\n",
      "none\n",
      "little\n",
      "old\n",
      "still\n",
      "feeling\n",
      "secret\n",
      "mission\n",
      "decide\n",
      "engage\n",
      "drop\n",
      "mic\n",
      "moment\n",
      "closer\n",
      "Thanksgiving\n",
      "drop\n",
      "hint\n",
      "describe\n",
      "one\n",
      "mother's\n",
      "recipes\n",
      "cornbread\n",
      "dressing\n",
      "took\n",
      "care\n",
      "air\n",
      "say\n",
      "bread\n",
      "white\n",
      "bread\n",
      "dressing\n",
      "cornbread\n",
      "dressing\n",
      "make\n",
      "cornbread\n",
      "night\n",
      "soak\n",
      "Cornbread\n",
      "dressing\n",
      "thought\n",
      "mic\n",
      "moment\n",
      "got\n",
      "know\n",
      "go\n",
      "home\n",
      "know\n",
      "black\n",
      "know\n",
      "black\n",
      "air\n",
      "one\n",
      "day\n",
      "remembering\n",
      "conversation\n",
      "two\n",
      "women\n",
      "two\n",
      "petite\n",
      "older\n",
      "black\n",
      "women\n",
      "Gray\n",
      "hair\n",
      "quaffed\n",
      "One\n",
      "single\n",
      "strand\n",
      "pearls\n",
      "touch\n",
      "makeup\n",
      "one\n",
      "took\n",
      "hand\n",
      "pulled\n",
      "close\n",
      "leaned\n",
      "could\n",
      "hear\n",
      "said\n",
      "proud\n",
      "got\n",
      "lump\n",
      "throat\n",
      "knew\n",
      "meant\n",
      "looking\n",
      "young\n",
      "talented\n",
      "black\n",
      "woman\n",
      "making\n",
      "mark\n",
      "big\n",
      "radio\n",
      "station\n",
      "St\n",
      "Louis\n",
      "looking\n",
      "seen\n",
      "experienced\n",
      "ever\n",
      "see\n",
      "experience\n",
      "see\n",
      "experience\n",
      "looking\n",
      "history\n",
      "face\n",
      "feel\n",
      "One\n",
      "day\n",
      "debate\n",
      "air\n",
      "talking\n",
      "controversy\n",
      "organization\n",
      "one\n",
      "many\n",
      "decided\n",
      "walk\n",
      "onto\n",
      "highway\n",
      "road\n",
      "interstate\n",
      "stop\n",
      "traffic\n",
      "shut\n",
      "protesting\n",
      "lack\n",
      "minority\n",
      "jobs\n",
      "construction\n",
      "one\n",
      "organizations\n",
      "involved\n",
      "one\n",
      "knew\n",
      "time\n",
      "lived\n",
      "Kansas\n",
      "City\n",
      "Missouri\n",
      "co\n",
      "host\n",
      "time\n",
      "never\n",
      "heard\n",
      "truly\n",
      "dismissive\n",
      "truly\n",
      "frustrated\n",
      "thinking\n",
      "movement\n",
      "moment\n",
      "moment\n",
      "either\n",
      "educate\n",
      "chastise\n",
      "enlighten\n",
      "think\n",
      "gonna\n",
      "three\n",
      "also\n",
      "thinking\n",
      "also\n",
      "worried\n",
      "still\n",
      "kind\n",
      "new\n",
      "still\n",
      "young\n",
      "woman\n",
      "black\n",
      "pregnant\n",
      "worried\n",
      "stereotype\n",
      "angry\n",
      "black\n",
      "woman\n",
      "yet\n",
      "also\n",
      "worried\n",
      "don't\n",
      "speak\n",
      "enough\n",
      "black\n",
      "issues\n",
      "thinking\n",
      "dive\n",
      "right\n",
      "said\n",
      "know\n",
      "yellow\n",
      "pages\n",
      "look\n",
      "right\n",
      "call\n",
      "around\n",
      "years\n",
      "took\n",
      "calls\n",
      "ended\n",
      "conversation\n",
      "turned\n",
      "mics\n",
      "left\n",
      "studio\n",
      "weeks\n",
      "ago\n",
      "gentleman\n",
      "came\n",
      "house\n",
      "service\n",
      "call\n",
      "repair\n",
      "windshield\n",
      "car\n",
      "right\n",
      "driveway\n",
      "met\n",
      "outside\n",
      "said\n",
      "know\n",
      "friend\n",
      "lives\n",
      "told\n",
      "news\n",
      "lady\n",
      "lives\n",
      "know\n",
      "said\n",
      "pausing\n",
      "feeling\n",
      "pride\n",
      "trepidation\n",
      "said\n",
      "woman\n",
      "Carol\n",
      "Daniel\n",
      "looking\n",
      "sizing\n",
      "sized\n",
      "driveway\n",
      "thought\n",
      "think\n",
      "white\n",
      "surprised\n",
      "find\n",
      "black\n",
      "thought\n",
      "debate\n",
      "apparently\n",
      "debate\n",
      "still\n",
      "form\n",
      "fashion\n",
      "sure\n",
      "ever\n",
      "really\n",
      "anything\n",
      "lot\n",
      "stake\n",
      "lot\n",
      "juggle\n",
      "life\n",
      "lot\n",
      "weight\n",
      "life\n",
      "try\n",
      "let\n",
      "weigh\n",
      "know\n",
      "deserve\n",
      "deserve\n",
      "job\n",
      "don't\n",
      "job\n",
      "sound\n",
      "white\n",
      "job\n",
      "good\n",
      "Thank\n"
     ]
    }
   ],
   "source": [
    "for word in df_task[df_task['NWP-Candidate']]['Word-Written']:\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815430ca",
   "metadata": {},
   "source": [
    "# Save the file as both CSV and JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0bca007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to file\n",
    "df_task_preproc_fn = os.path.join(task_out_dir, f'{task}_transcript_preprocessed-shifted-onset')\n",
    "\n",
    "df_task.columns = df_task.columns.str.replace(\"-\", \"_\")\n",
    "df_task.to_csv(f'{df_task_preproc_fn}.csv', index=False)\n",
    "df_task.to_json(f'{df_task_preproc_fn}.json', orient='records') #, lines=True) #, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80e0a2",
   "metadata": {},
   "source": [
    "# Organize into randomized sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "id": "21a93d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/code/utils/')\n",
    "\n",
    "from randomization_utils import create_balanced_orders, sort_consecutive_constraint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "id": "fd1dfb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "\n",
    "## functions modified from https://stackoverflow.com/questions/93353/create-many-constrained-random-permutation-of-a-list\n",
    "def get_pool(items, n_elements_per_subject, use_each_times):\n",
    "    pool = {}\n",
    "    for n in items:\n",
    "        pool[n] = use_each_times\n",
    "    \n",
    "    return pool\n",
    "\n",
    "def rebalance(ret, pool, n_elements_per_subject):\n",
    "    max_item = None\n",
    "    max_times = None\n",
    "    \n",
    "    for item, times in pool.items():\n",
    "        if max_times is None:\n",
    "            max_item = item\n",
    "            max_times = times\n",
    "        elif times > max_times:\n",
    "            max_item = item\n",
    "            max_times = times\n",
    "    \n",
    "    next_item, times = max_item, max_times\n",
    "\n",
    "    candidates = []\n",
    "    for i in range(len(ret)):\n",
    "        item = ret[i]\n",
    "\n",
    "        if next_item not in item:\n",
    "            candidates.append( (item, i) )\n",
    "    \n",
    "    swap, swap_index = random.choice(candidates)\n",
    "\n",
    "    swapi = []\n",
    "    for i in range(len(swap)):\n",
    "        if swap[i] not in pool:\n",
    "            swapi.append( (swap[i], i) )\n",
    "    \n",
    "    which, i = random.choice(swapi)\n",
    "    \n",
    "    pool[next_item] -= 1\n",
    "    pool[swap[i]] = 1\n",
    "    swap[i] = next_item\n",
    "\n",
    "    ret[swap_index] = swap\n",
    "\n",
    "def create_balanced_orders(items, n_elements_per_subject, use_each_times, consecutive_limit=2,  error=1):\n",
    "    '''\n",
    "    Returns a set of unique lists under the constraints of \n",
    "    - n_elements_per_subject (must be less than items)\n",
    "    - use_each_times: number of times each item should be seen across subjects\n",
    "\n",
    "    Together these define the number of subjects\n",
    "\n",
    "    '''\n",
    "\n",
    "    n_subjects = math.ceil((use_each_times * len(items)) / n_elements_per_subject)\n",
    "\n",
    "    print (f'Creating orders for {n_subjects} subjects')\n",
    "\n",
    "    pool = get_pool(items, n_elements_per_subject, use_each_times)\n",
    "    \n",
    "    ret = []\n",
    "    while len(pool.keys()) > 0:\n",
    "        while len(pool.keys()) < n_elements_per_subject:\n",
    "            rebalance(ret, pool, n_elements_per_subject)\n",
    "        \n",
    "        selections = sorted(random.sample(pool.keys(), n_elements_per_subject))\n",
    "        \n",
    "        for i in selections:\n",
    "            pool[i] -= 1\n",
    "            if pool[i] == 0:\n",
    "                del pool[i]\n",
    "\n",
    "        ret.append( selections )\n",
    "        \n",
    "        unique, counts = np.unique(ret, return_counts=True)\n",
    "        \n",
    "        if all(np.logical_and(counts <= use_each_times + error, counts >= use_each_times)):\n",
    "               break\n",
    "    return ret\n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "\n",
    "def get_consecutive_list_idxs(orders, consecutive_length):\n",
    "    \n",
    "    # Find lists with consecutive items violating our constraint\n",
    "    idxs = np.where([np.any(np.asarray(list(map(len, consecutive(order)))) >= consecutive_length) for order in orders])[0]\n",
    "    \n",
    "    return idxs\n",
    "\n",
    "def sort_consecutive_constraint(orders, consecutive_length=3):\n",
    "    \n",
    "    # Get sets of all orders\n",
    "    all_order_idxs = np.arange(len(orders))\n",
    "    \n",
    "    # Find lists with consecutive items violating our constraint\n",
    "    consecutive_order_idxs = get_consecutive_list_idxs(orders, consecutive_length)\n",
    "    \n",
    "    while len(consecutive_order_idxs):\n",
    "\n",
    "        for order_idx in consecutive_order_idxs:\n",
    "            # Select the current list violating the constraint\n",
    "            current_list = np.asarray(orders[order_idx])\n",
    "\n",
    "            random_list_options = np.setdiff1d(all_order_idxs, order_idx)\n",
    "\n",
    "            # Find all sets of consecutive items in the current list --> find their lengths\n",
    "            consecutive_items = consecutive(current_list)\n",
    "            consecutive_lengths = np.asarray(list(map(len, consecutive_items)))\n",
    "\n",
    "            # Find sets of slices that violate the constraint\n",
    "            violations = np.where(consecutive_lengths >= consecutive_length)[0]\n",
    "\n",
    "            for violation in violations:\n",
    "                # Select items that need to be swapped --> these will be swapped into a randomly selected list\n",
    "                swap_items = consecutive_items[violation][1::2]\n",
    "\n",
    "                for item in swap_items:\n",
    "                    swap_idx = np.where(current_list == item)[0]\n",
    "\n",
    "                    # Select a random other list\n",
    "                    random_list_idx = random.choice(random_list_options)\n",
    "                    random_list = np.asarray(orders[random_list_idx])\n",
    "\n",
    "                    # Find choices not within our current list\n",
    "                    swap_choices = np.setdiff1d(random_list, current_list)\n",
    "\n",
    "                    # Select a random choice\n",
    "                    choice = random.choice(swap_choices)\n",
    "                    \n",
    "                    # Make sure we didn't violate our constraint again with either list\n",
    "                    while (\n",
    "                        np.isin(choice,current_list) or \n",
    "                        np.isin(item,random_list)\n",
    "                    ):\n",
    "                        \n",
    "                        # Select a random other list\n",
    "                        random_list_idx = random.choice(random_list_options)\n",
    "                        random_list = np.asarray(orders[random_list_idx])\n",
    "\n",
    "                        # Find choices not within our current list\n",
    "                        swap_choices = np.setdiff1d(random_list, current_list)\n",
    "\n",
    "                        # Select a random choice\n",
    "                        choice = random.choice(swap_choices)\n",
    "                    \n",
    "                    # Find the index to swap to\n",
    "                    choice_idx = np.where(random_list == choice)[0]\n",
    "\n",
    "                    # Swap the two items\n",
    "                    current_list[swap_idx] = choice\n",
    "                    random_list[choice_idx] = item\n",
    "\n",
    "                    # Set them in the overall orders\n",
    "                    orders[order_idx] = sorted(current_list)\n",
    "                    orders[random_list_idx] = sorted(random_list)\n",
    "                    \n",
    "        # Find lists with consecutive items violating our constraint\n",
    "        consecutive_order_idxs = get_consecutive_list_idxs(orders, consecutive_length)\n",
    "    return orders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "id": "f7d9eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'black'\n",
    "\n",
    "EXPERIMENT_NAME = 'pilot_version02'\n",
    "\n",
    "percent_sampled = 0.25 # number of items to sample for each subject\n",
    "n_counts_per_item = 25 # number of times items are seen across subjects\n",
    "\n",
    "# set directories\n",
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/'\n",
    "preproc_dir = os.path.join(base_dir, 'stimuli', 'preprocessed')\n",
    "task_out_dir = os.path.join(base_dir, 'stimuli', 'presentation_orders', EXPERIMENT_NAME, task)\n",
    "\n",
    "if not os.path.exists(task_out_dir):\n",
    "    os.makedirs(task_out_dir)\n",
    "\n",
    "# load preprocessed transcript\n",
    "df_task_preproc_fn = os.path.join(preproc_dir, task, f'{task}_transcript_preprocessed')\n",
    "df_preproc = pd.read_csv(f'{df_task_preproc_fn}.csv')\n",
    "\n",
    "# find indices for presentation and set number of items each subject sees\n",
    "nwp_indices = np.where(df_preproc['NWP_Candidate'])[0]\n",
    "n_items_per_subject = round(len(nwp_indices) * percent_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8adcc1",
   "metadata": {},
   "source": [
    "Calculate percent consecutive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "id": "a67c2ccd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2903752039151713"
      ]
     },
     "execution_count": 963,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.diff(nwp_indices) == 1) / len(nwp_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eecfdb9",
   "metadata": {},
   "source": [
    "create experiment structure for subjects --> sort the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "59663215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating orders for 101 subjects\n",
      "101\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "subject_experiment_orders = create_balanced_orders(items=nwp_indices, n_elements_per_subject=n_items_per_subject, use_each_times=n_counts_per_item, error=1)\n",
    "subject_experiment_orders = list(map(sorted, subject_experiment_orders))\n",
    "\n",
    "# Find lists with consecutive items violating our constraint\n",
    "idxs = get_consecutive_list_idxs(subject_experiment_orders, consecutive_length=2)\n",
    "print (len(idxs))\n",
    "\n",
    "orders = sort_consecutive_constraint(subject_experiment_orders, consecutive_length=2)\n",
    "\n",
    "# Find lists with consecutive items violating our constraint\n",
    "idxs = get_consecutive_list_idxs(orders, consecutive_length=2)\n",
    "print (len(idxs))\n",
    "# test = sort_consecutive_constraint(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619433f",
   "metadata": {},
   "source": [
    "Make sure no counts are less than the number we wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "3d89bb02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counts per word: 61300%\n"
     ]
    }
   ],
   "source": [
    "uniq, counts = np.unique(orders, return_counts=True)\n",
    "counts_per_word = np.sum(counts < n_counts_per_item)\n",
    "\n",
    "print (f'All counts per word: {np.sum(counts >= n_counts_per_item) / len(counts)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf569bd",
   "metadata": {},
   "source": [
    "Then check that all orders are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "7a9ac194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique orders: 100.0%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = Counter(tuple(o) for o in orders)\n",
    "\n",
    "unique_orders = np.sum([v for k, v in counts.items()]) / len(counts)\n",
    "\n",
    "print (f'Unique orders: {unique_orders*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffcf0f",
   "metadata": {},
   "source": [
    "Then make sure all items are at least 2 apart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "id": "c47ce4dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consecutive constraint: 100.0%\n"
     ]
    }
   ],
   "source": [
    "orders_meeting_consecutive = np.sum([(np.all(np.diff(order) >= 2)) for order in orders]) / len(orders)\n",
    "print (f'Consecutive constraint: {orders_meeting_consecutive*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30bd3c7",
   "metadata": {},
   "source": [
    "Lastly write files out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "aad17baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavioral/stimuli/presentation_orders/black'"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "87953868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/narratives/lib/python3.7/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "for i, order in enumerate(orders):\n",
    "    # find indices not selected for the current subject and set to false\n",
    "    df_subject = df_preproc.copy()\n",
    "    unselected = np.setdiff1d(nwp_indices, order)\n",
    "    df_subject['NWP_Candidate'].loc[unselected] = False\n",
    "\n",
    "    sub_fn = os.path.join(task_out_dir, f'sub-{str(i+1).zfill(5)}_task-{task}.json')\n",
    "    df_subject.to_json(sub_fn, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
