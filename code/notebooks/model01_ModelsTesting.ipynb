{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0196f0-7320-4327-83ca-f2ccdc3d0f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /dartfs/rc/lab/F/FinnLab/tommy/models/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import glob\n",
    "\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/utils/')\n",
    "sys.path.append('/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/joint-clm-prosody/')\n",
    "\n",
    "from config import *\n",
    "from src.data.prominence_regression_datamodule import ProminenceRegressionDataModule\n",
    "from src.models.joint_clm_prosody import ProsodyCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c469ed-c693-4b39-9232-265f2e7312fd",
   "metadata": {},
   "source": [
    "## Create PyTorch Lightning DataModule --> similar to pt dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161fa68c-d410-454b-8d00-e1341d7ba516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPT2 tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloader: padding with token id: 50256\n",
      "Loading data from /dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/code/modeling/joint-clm-prosody/data/helsinki-prosody/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 109791/109791 [01:19<00:00, 1374.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 7217/109791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing samples: 100%|██████████| 12199/12199 [00:08<00:00, 1462.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed 768/12199\n"
     ]
    }
   ],
   "source": [
    "modeling_dir = os.path.join(BASE_DIR, 'code/modeling/joint-clm-prosody')\n",
    "\n",
    "data_module = ProminenceRegressionDataModule(\n",
    "    dataset_name = 'helsinki_prominence',\n",
    "    data_dir = os.path.join(modeling_dir, 'data/heglsinki-prosody/data'),\n",
    "    train_file = 'train_360.txt',\n",
    "    val_file = 'dev.txt',\n",
    "    test_file = 'test.txt',\n",
    "    model_name = \"gpt2\",\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "data_module.setup(stage=\"fit\")\n",
    "\n",
    "# for batch in data_module.train_dataloader():\n",
    "#     sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6276d59b-1943-43d8-87da-bf755bd700d3",
   "metadata": {},
   "source": [
    "### Get all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "693e4728-a9e6-47d2-b3f0-b698b91e9ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62179396-82d8-4625-95d5-6560b0fdeb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "\n",
    "for batch in data_module.train_dataloader():\n",
    "    all_labels.append(batch['tokenized_labels'].flatten())\n",
    "all_labels = torch.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4dd3c-a870-4d4d-a422-57305f109f0a",
   "metadata": {},
   "source": [
    "## Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23795dca-d747-4816-9b89-5def56e3186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b93658d6-5b74-492b-be40-7dd7ad3bac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Huggingface model.\n",
      "Loading pretrained model\n",
      "Using joint loss\n"
     ]
    }
   ],
   "source": [
    "model = ProsodyCausalLM(model_name=\"gpt2\", pretrained=True)\n",
    "# outputs = test.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca745ad-f26f-4ef9-8656-be7d17e66576",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, outputs = model.step(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc0833b-6e7e-451d-84b5-0f2d29a0841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Normal(outputs['mu'], outputs['var'].squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2b5613-cfa7-4595-8c6c-07ebf0604b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, labels, mask = model.get_shifted_labels(\n",
    "    logits=outputs['logits'],\n",
    "    labels=batch['input_ids'],\n",
    "    mask=batch['attention_mask']\n",
    ")\n",
    "\n",
    "# logits = outputs['logits']\n",
    "# labels=batch['input_ids']\n",
    "# mask=batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "588c108c-c666-412d-8410-d392ecf2943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(logits, -1)\n",
    "correct = (preds == labels) * mask.bool()\n",
    "\n",
    "correct = torch.sum(correct)\n",
    "total = torch.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b7be494-7f1d-4008-b814-f5919b611f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = outputs['dist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2e8b994-6c80-4a49-b844-0c02940555a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prosody labels\n",
    "labels = batch['tokenized_labels']\n",
    "dist = outputs['dist']\n",
    "\n",
    "loss_mask = batch['loss_mask']\n",
    "\n",
    "mu = outputs['mu']\n",
    "var = outputs['var'].squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c59cd1f5-9b4e-442a-825a-088f1e11d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.gamma import Gamma\n",
    "\n",
    "shifted_labels = labels[..., 1:].contiguous().view(-1)\n",
    "shifted_mu = mu[..., :-1].contiguous() #.view(-1)\n",
    "shifted_var = var[..., :-1].contiguous()#. view(-1)\n",
    "shifted_mask = loss_mask[..., 1:].contiguous().view(-1)\n",
    "\n",
    "dist = Gamma(shifted_mu, shifted_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de4640c8-2362-4c9b-992d-d9a689c758db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9840e+00, -9.9900e+02,  1.1410e+00,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00],\n",
       "        [ 1.8000e-02,  2.0110e+00, -9.9900e+02,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00],\n",
       "        [ 2.1030e+00,  2.2330e+00,  7.1000e-01,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00],\n",
       "        ...,\n",
       "        [ 1.4910e+00,  1.4910e+00,  1.9500e-01,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00],\n",
       "        [ 2.4880e+00, -9.9900e+02,  1.0660e+00,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00],\n",
       "        [-9.9900e+02, -9.9900e+02,  9.9400e-01,  ..., -1.0000e+00,\n",
       "         -1.0000e+00, -1.0000e+00]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['tokenized_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aaab940b-2e5b-4ebf-8584-3e2d1b17aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Then , across the pale glimmering of sand , Henriot saw a figure moving .',\n",
       " 'The captain , who had so long been a cause of so much discomfort , was gone where the wicked cease from troubling .',\n",
       " 'This first book of his brought him into notice , and served as an introduction to Tycho and to Galileo .',\n",
       " \"Here's what I found at the very site of that final shipwreck !\",\n",
       " 'They stopped and stared at me .',\n",
       " \"'Oh , indeed ! ' said mrs Rogers graciously ; for she was the lodger , and her servant was in waiting , so she was more gracious than intimate , in right of her position .\",\n",
       " \"Well , what of it ? It's not my fault . And he began thinking about the next day .\",\n",
       " 'Let us seek David on the hillsides , tending his flocks with loving care .',\n",
       " 'But he heard a rustling in the branches , and a golden apple fell into his hand .',\n",
       " 'So , comrades , said Myles at last , what shall we do now ?',\n",
       " 'To acquire languages , departed or living in spite of such obstinacies as he now knew them inherently to possess , was a herculean performance which gradually led him on to a greater interest in it than in the presupposed patent process .',\n",
       " 'What is the matter with you ?',\n",
       " \"'There are no women here , ' I said .\",\n",
       " 'I would certainly come with you , replied the Queen , but I am afraid that I cannot walk backwards .',\n",
       " 'An old fright ought to realise she is a fright !',\n",
       " 'I remember the study was close , and I came to get cool .',\n",
       " 'In the mid afternoon of this same day Kennicott was called into the country .',\n",
       " 'But even among savages the processes are much more complicated .',\n",
       " 'The Search after Happiness , a Tale , august first eighteen twenty nine .',\n",
       " 'There now , here they are .',\n",
       " \"Don't you know me ?\",\n",
       " 'Brigitte had been seen in the market place betimes that morning , and , wonderful to relate , she had bought the one hare to be had .',\n",
       " \"She can see at a glance that Lord Warburton isn't .\",\n",
       " 'IS THIS MADNESS ?',\n",
       " 'And that kind of thing is a little overwhelming at short range .',\n",
       " 'For it is true , that late learners cannot so well take the ply ; except it be in some minds , that have not suffered themselves to fix , but have kept themselves open , and prepared to receive continual amendment , which is exceeding rare .',\n",
       " \"Interest in one's own social figure is to some extent a material interest , for other men's love or aversion is a principle read into their acts ; and a social animal like man is dependent on other men's acts for his happiness .\",\n",
       " 'Find out who did that .',\n",
       " \"'How is old Betty Barnes ? '\",\n",
       " 'Respect for a person is properly only respect for the law of honesty , etc of which he gives us an example .',\n",
       " 'Harry , like all the educated boys of the South , honored and admired its public men .',\n",
       " \"'What am I to do ? ' she asked , when , after clapping her hands , the old woman appeared before her .\"]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cde30175-9f91-4ac0-be5f-699db24a22a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.984,\n",
       "  None,\n",
       "  1.141,\n",
       "  0.0,\n",
       "  1.745,\n",
       "  0.858,\n",
       "  0.003,\n",
       "  1.229,\n",
       "  None,\n",
       "  1.309,\n",
       "  0.596,\n",
       "  0.07,\n",
       "  0.954,\n",
       "  0.57,\n",
       "  None],\n",
       " [0.018,\n",
       "  2.011,\n",
       "  None,\n",
       "  0.061,\n",
       "  0.587,\n",
       "  1.907,\n",
       "  0.867,\n",
       "  0.47,\n",
       "  0.067,\n",
       "  1.827,\n",
       "  0.037,\n",
       "  1.096,\n",
       "  0.189,\n",
       "  1.041,\n",
       "  None,\n",
       "  0.545,\n",
       "  1.721,\n",
       "  0.12,\n",
       "  0.034,\n",
       "  1.686,\n",
       "  1.96,\n",
       "  0.898,\n",
       "  0.183,\n",
       "  None],\n",
       " [2.103,\n",
       "  2.233,\n",
       "  0.71,\n",
       "  0.112,\n",
       "  0.866,\n",
       "  1.541,\n",
       "  0.603,\n",
       "  0.056,\n",
       "  0.842,\n",
       "  None,\n",
       "  0.195,\n",
       "  2.098,\n",
       "  0.113,\n",
       "  0.029,\n",
       "  1.691,\n",
       "  0.044,\n",
       "  1.588,\n",
       "  0.212,\n",
       "  0.018,\n",
       "  0.718,\n",
       "  None],\n",
       " [0.424,\n",
       "  0.033,\n",
       "  0.29,\n",
       "  2.272,\n",
       "  0.028,\n",
       "  0.0,\n",
       "  1.688,\n",
       "  0.823,\n",
       "  0.086,\n",
       "  0.33,\n",
       "  1.032,\n",
       "  0.605,\n",
       "  None],\n",
       " [0.508, 2.466, 0.003, 1.42, 0.0, 0.301, None],\n",
       " [None,\n",
       "  None,\n",
       "  2.403,\n",
       "  None,\n",
       "  None,\n",
       "  4.155,\n",
       "  0.32,\n",
       "  0.487,\n",
       "  0.404,\n",
       "  None,\n",
       "  0.971,\n",
       "  0.165,\n",
       "  1.14,\n",
       "  0.145,\n",
       "  0.0,\n",
       "  None,\n",
       "  1.249,\n",
       "  0.465,\n",
       "  0.0,\n",
       "  2.27,\n",
       "  0.562,\n",
       "  0.0,\n",
       "  None,\n",
       "  1.075,\n",
       "  0.351,\n",
       "  0.75,\n",
       "  0.026,\n",
       "  0.293,\n",
       "  1.181,\n",
       "  0.008,\n",
       "  None,\n",
       "  0.789,\n",
       "  0.282,\n",
       "  0.725,\n",
       "  0.012,\n",
       "  0.072,\n",
       "  None],\n",
       " [2.181,\n",
       "  None,\n",
       "  0.332,\n",
       "  0.068,\n",
       "  0.021,\n",
       "  None,\n",
       "  0.931,\n",
       "  2.765,\n",
       "  0.006,\n",
       "  0.123,\n",
       "  None,\n",
       "  0.02,\n",
       "  0.062,\n",
       "  1.165,\n",
       "  0.482,\n",
       "  0.835,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.495,\n",
       "  None],\n",
       " [0.01,\n",
       "  0.527,\n",
       "  1.162,\n",
       "  0.487,\n",
       "  0.005,\n",
       "  0.0,\n",
       "  2.705,\n",
       "  None,\n",
       "  0.091,\n",
       "  0.333,\n",
       "  0.215,\n",
       "  0.008,\n",
       "  0.262,\n",
       "  2.826,\n",
       "  None],\n",
       " [0.038,\n",
       "  0.0,\n",
       "  0.697,\n",
       "  0.028,\n",
       "  0.886,\n",
       "  0.081,\n",
       "  0.0,\n",
       "  1.287,\n",
       "  None,\n",
       "  0.07,\n",
       "  0.016,\n",
       "  2.44,\n",
       "  2.013,\n",
       "  0.23,\n",
       "  0.166,\n",
       "  0.166,\n",
       "  1.798,\n",
       "  None],\n",
       " [2.136,\n",
       "  None,\n",
       "  0.924,\n",
       "  None,\n",
       "  0.23,\n",
       "  0.927,\n",
       "  0.0,\n",
       "  1.291,\n",
       "  None,\n",
       "  0.678,\n",
       "  0.262,\n",
       "  0.014,\n",
       "  0.4,\n",
       "  1.398,\n",
       "  None],\n",
       " [0.754,\n",
       "  1.382,\n",
       "  1.317,\n",
       "  None,\n",
       "  1.15,\n",
       "  0.569,\n",
       "  0.687,\n",
       "  0.697,\n",
       "  1.078,\n",
       "  0.0,\n",
       "  0.285,\n",
       "  1.027,\n",
       "  0.689,\n",
       "  0.004,\n",
       "  0.903,\n",
       "  0.075,\n",
       "  0.145,\n",
       "  0.694,\n",
       "  0.0,\n",
       "  1.679,\n",
       "  None,\n",
       "  0.243,\n",
       "  0.059,\n",
       "  1.284,\n",
       "  0.951,\n",
       "  0.226,\n",
       "  1.483,\n",
       "  0.27,\n",
       "  0.021,\n",
       "  2.421,\n",
       "  0.233,\n",
       "  0.0,\n",
       "  1.928,\n",
       "  0.296,\n",
       "  0.181,\n",
       "  0.0,\n",
       "  1.202,\n",
       "  0.092,\n",
       "  0.022,\n",
       "  1.382,\n",
       "  1.234,\n",
       "  0.317,\n",
       "  None],\n",
       " [2.379, 0.087, 0.0, 1.863, 0.0, 0.061, None],\n",
       " [None, 0.856, 0.0, 0.191, 0.083, None, None, 0.242, 0.0, None],\n",
       " [0.526,\n",
       "  0.158,\n",
       "  2.656,\n",
       "  0.024,\n",
       "  1.047,\n",
       "  0.089,\n",
       "  None,\n",
       "  0.533,\n",
       "  0.0,\n",
       "  0.198,\n",
       "  None,\n",
       "  0.685,\n",
       "  0.02,\n",
       "  0.207,\n",
       "  2.806,\n",
       "  0.363,\n",
       "  0.07,\n",
       "  2.348,\n",
       "  1.136,\n",
       "  1.453,\n",
       "  None],\n",
       " [0.235, 3.166, 0.482, 0.737, 0.0, 0.82, 0.841, 0.282, 0.0, 0.492, None],\n",
       " [1.528,\n",
       "  0.45,\n",
       "  0.14,\n",
       "  1.807,\n",
       "  0.039,\n",
       "  1.58,\n",
       "  None,\n",
       "  0.449,\n",
       "  0.063,\n",
       "  1.652,\n",
       "  0.049,\n",
       "  0.296,\n",
       "  2.26,\n",
       "  None],\n",
       " [0.984,\n",
       "  0.0,\n",
       "  1.578,\n",
       "  0.682,\n",
       "  0.0,\n",
       "  0.131,\n",
       "  1.024,\n",
       "  0.394,\n",
       "  2.974,\n",
       "  0.199,\n",
       "  0.893,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  1.245,\n",
       "  None],\n",
       " [0.548, 0.066, 1.552, 1.15, 0.036, 2.835, 0.01, 0.601, 0.122, 0.677, None],\n",
       " [0.049,\n",
       "  0.493,\n",
       "  0.153,\n",
       "  2.789,\n",
       "  None,\n",
       "  0.013,\n",
       "  2.892,\n",
       "  None,\n",
       "  1.899,\n",
       "  0.205,\n",
       "  0.325,\n",
       "  0.006,\n",
       "  1.209,\n",
       "  None],\n",
       " [2.451, 0.052, None, 0.657, 1.17, 0.305, None],\n",
       " [0.349, 0.007, 0.008, 2.785, None],\n",
       " [1.376,\n",
       "  0.124,\n",
       "  0.03,\n",
       "  1.208,\n",
       "  0.0,\n",
       "  0.03,\n",
       "  0.541,\n",
       "  0.359,\n",
       "  1.82,\n",
       "  0.148,\n",
       "  0.255,\n",
       "  None,\n",
       "  2.859,\n",
       "  None,\n",
       "  1.291,\n",
       "  0.062,\n",
       "  1.952,\n",
       "  None,\n",
       "  0.853,\n",
       "  0.0,\n",
       "  1.08,\n",
       "  0.021,\n",
       "  0.495,\n",
       "  2.103,\n",
       "  0.0,\n",
       "  0.053,\n",
       "  0.902,\n",
       "  None],\n",
       " [0.084, 0.522, 0.339, 0.0, 0.171, 1.95, 0.071, 0.46, 0.648, 1.992, None],\n",
       " [2.227, 0.256, 0.311, None],\n",
       " [0.932,\n",
       "  1.817,\n",
       "  1.96,\n",
       "  0.0,\n",
       "  0.436,\n",
       "  0.0,\n",
       "  0.019,\n",
       "  0.317,\n",
       "  1.445,\n",
       "  0.0,\n",
       "  0.356,\n",
       "  0.194,\n",
       "  None],\n",
       " [1.491,\n",
       "  0.0,\n",
       "  0.534,\n",
       "  2.67,\n",
       "  None,\n",
       "  0.405,\n",
       "  2.576,\n",
       "  0.384,\n",
       "  0.674,\n",
       "  1.053,\n",
       "  0.047,\n",
       "  0.397,\n",
       "  0.036,\n",
       "  1.283,\n",
       "  None,\n",
       "  2.09,\n",
       "  0.006,\n",
       "  0.411,\n",
       "  0.001,\n",
       "  1.582,\n",
       "  0.032,\n",
       "  None,\n",
       "  0.084,\n",
       "  0.046,\n",
       "  0.845,\n",
       "  1.032,\n",
       "  0.713,\n",
       "  0.027,\n",
       "  1.486,\n",
       "  None,\n",
       "  0.027,\n",
       "  0.21,\n",
       "  1.972,\n",
       "  1.504,\n",
       "  2.344,\n",
       "  None,\n",
       "  0.064,\n",
       "  2.152,\n",
       "  0.176,\n",
       "  0.558,\n",
       "  1.433,\n",
       "  1.263,\n",
       "  None,\n",
       "  0.636,\n",
       "  0.266,\n",
       "  2.071,\n",
       "  1.042,\n",
       "  None],\n",
       " [1.699,\n",
       "  0.51,\n",
       "  0.382,\n",
       "  0.608,\n",
       "  1.359,\n",
       "  1.23,\n",
       "  0.543,\n",
       "  0.0,\n",
       "  1.124,\n",
       "  0.765,\n",
       "  0.144,\n",
       "  1.958,\n",
       "  0.644,\n",
       "  None,\n",
       "  0.06,\n",
       "  1.853,\n",
       "  0.463,\n",
       "  1.383,\n",
       "  1.315,\n",
       "  1.875,\n",
       "  0.649,\n",
       "  0.019,\n",
       "  2.04,\n",
       "  0.175,\n",
       "  0.255,\n",
       "  0.007,\n",
       "  0.701,\n",
       "  None,\n",
       "  0.31,\n",
       "  0.0,\n",
       "  1.839,\n",
       "  0.816,\n",
       "  0.26,\n",
       "  0.401,\n",
       "  0.137,\n",
       "  2.462,\n",
       "  0.214,\n",
       "  1.962,\n",
       "  0.245,\n",
       "  1.255,\n",
       "  0.249,\n",
       "  0.0,\n",
       "  0.808,\n",
       "  None],\n",
       " [2.754, 0.613, 0.27, 0.589, 0.397, None],\n",
       " [None, 2.439, 0.0, 0.249, 0.711, None, None],\n",
       " [1.491,\n",
       "  0.195,\n",
       "  0.0,\n",
       "  0.962,\n",
       "  0.024,\n",
       "  1.137,\n",
       "  1.715,\n",
       "  0.299,\n",
       "  0.008,\n",
       "  0.277,\n",
       "  2.372,\n",
       "  0.0,\n",
       "  1.28,\n",
       "  None,\n",
       "  0.475,\n",
       "  0.263,\n",
       "  0.671,\n",
       "  0.0,\n",
       "  0.47,\n",
       "  0.296,\n",
       "  0.048,\n",
       "  2.142,\n",
       "  None],\n",
       " [2.488,\n",
       "  None,\n",
       "  1.066,\n",
       "  0.515,\n",
       "  0.0,\n",
       "  0.894,\n",
       "  0.388,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.256,\n",
       "  None,\n",
       "  2.265,\n",
       "  0.181,\n",
       "  2.068,\n",
       "  0.142,\n",
       "  0.467,\n",
       "  1.412,\n",
       "  None],\n",
       " [None,\n",
       "  0.994,\n",
       "  0.0,\n",
       "  0.244,\n",
       "  0.831,\n",
       "  None,\n",
       "  None,\n",
       "  3.746,\n",
       "  0.759,\n",
       "  None,\n",
       "  0.118,\n",
       "  None,\n",
       "  0.519,\n",
       "  0.079,\n",
       "  0.734,\n",
       "  0.075,\n",
       "  None,\n",
       "  0.607,\n",
       "  0.0,\n",
       "  0.714,\n",
       "  0.346,\n",
       "  0.638,\n",
       "  0.532,\n",
       "  None]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['original_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d135e915-8959-490d-b8e8-91bc88de2d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_text', 'tokenized_text', 'original_labels', 'tokenized_labels', 'input_ids', 'loss_mask', 'attention_mask', 'word_to_tokens'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d37ce1ae-6cdc-445a-9e28-4e9d683afdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_labels = shifted_labels * shifted_mask + 1e-4  # add small constant for numerical stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d2a880b-d686-436e-8617-aba87a3419c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = -dist.log_prob(shifted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c1668cf3-f92d-4d1a-a340-18ae1f3d5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_nll = nll * shifted_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a891e568-2c53-4ce5-abb4-352c0ef474f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = masked_nll.sum() / shifted_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b026d8e-be4c-4cd6-9abd-e556661675cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1118, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4d22410-5615-4cc8-8466-58c5525e159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6996, 0.0008, 0.4233,  ..., 0.4181, 0.4164, 0.4170],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec31a3f5-daa8-45db-8414-ea6f3fa42c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels * loss_mask + 1e-4  # add small constant for numerical stability\n",
    "nll = -dist.log_prob(labels)\n",
    "\n",
    "# mask loss\n",
    "masked_nll = nll * loss_mask\n",
    "masked_nll_mean = masked_nll.sum() / loss_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d2ad38f-0d1d-4d15-bfe7-c6aa53456b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96d583ab-264e-4691-9d6a-c44c31263993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1053, -0.0000,  1.6620,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [-0.8416,  2.1437, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 2.2663,  3.1142,  1.2448,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        ...,\n",
       "        [ 1.4299,  1.6123,  1.0655,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [ 2.4956, -0.0000,  3.3014,  ..., -0.0000, -0.0000, -0.0000],\n",
       "        [-0.0000, -0.0000,  1.9905,  ..., -0.0000, -0.0000, -0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfbf195f-ee03-4f04-a05b-7d22506dcbf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 43])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['loss_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "11246245-1a98-44ec-b2f2-388425375330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4254, 5.9616, 3.6534,  ..., 1.2453, 1.2425, 1.2350],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['var'].view(-1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "74c25d02-9fc2-4345-baec-3d61b0c51176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(5e-5 == 0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29d3229e-f54e-4134-a8a8-96fee1c6e348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.1849, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['prosody_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323b7c2-631a-43f9-a024-52a7b13365f0",
   "metadata": {},
   "source": [
    "### Manually perform forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47cb3598-6684-449e-a6d3-effbbb7e64a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized labels = prosody values\n",
    "input_embeds = test.get_input_embeddings(\n",
    "    input_ids = batch['input_ids'], \n",
    "    prosody_values = batch['tokenized_labels']\n",
    ")\n",
    "\n",
    "# get outputs from causal LM\n",
    "outputs = test.model.transformer(\n",
    "    inputs_embeds=input_embeds, \n",
    "    attention_mask=batch[\"attention_mask\"], \n",
    ")\n",
    "\n",
    "# get the logits for predicting each item in the sequence\n",
    "logits = test.model.lm_head(outputs.last_hidden_state)\n",
    "\n",
    "# get prosody predictions\n",
    "preds = test.regressor(outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925d14c-2512-4f9e-a208-10fcb7a909fb",
   "metadata": {},
   "source": [
    "### Get distribution over the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0816d01f-9cea-43a7-9856-912a89e396d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split last dimension into mu and var\n",
    "mu, var = torch.chunk(preds, chunks=2, dim=-1)\n",
    "\n",
    "# ensure positivity of var + add a small constant for numerical stability\n",
    "var = F.softplus(var)\n",
    "var = (var + eps).squeeze(-1)\n",
    "\n",
    "# have to squeeze the last dimension due to chunking\n",
    "if test.output_activation is not None:\n",
    "    mu = test.output_activation(mu.squeeze(-1))\n",
    "\n",
    "# Gamma distribution with concentration mu and rate var\n",
    "mu = F.softplus(mu)\n",
    "dist = Gamma(mu, var)\n",
    "preds = dist.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd95a2a-1c74-4e24-8562-bd44c631f455",
   "metadata": {},
   "source": [
    "## Test natural output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cb123b62-b3c4-4510-b063-480981fbf862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0272, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.prosody_loss(batch, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bb175141-5ed9-45c2-9b3a-c72f2ef8f2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prosody labels\n",
    "labels = batch['tokenized_labels']\n",
    "dist = outputs['dist']\n",
    "loss_mask = batch[\"loss_mask\"]  # ignore padded sequence in loss\n",
    "\n",
    "\n",
    "\n",
    "# log likelihood of labels given the distribution\n",
    "labels = labels * loss_mask + 1e-4  # add small constant for numerical stability\n",
    "nll = -dist.log_prob(labels)\n",
    "\n",
    "# mask loss\n",
    "masked_nll = nll * loss_mask\n",
    "masked_nll_mean = masked_nll.sum() / loss_mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6f5d7c16-5436-4fea-a162-6c2afb7818d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.5416, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_nll_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688417bc-2aa3-4495-9c77-efe6c4e17e6d",
   "metadata": {},
   "source": [
    "## Test PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac8b34c-13f4-48ed-9f4c-b6dfa1138842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "6104bd7b-25e2-45bb-b92c-7943c8d88ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8943fe6-3a46-4648-8334-ba50b6197afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    'r': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'lora_dropout': 0.05,\n",
    "    'bias': \"none\",\n",
    "    'task_type': \"CAUSAL_LM\",\n",
    "    'base_model_name_or_path': 'gpt2',\n",
    "    'modules_to_save': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9ff2e24-e1ce-4483-8d43-b28dc19a541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,187,200 || all params: 163,627,008 || trainable%: 23.9491\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(**lora_config)\n",
    "model = get_peft_model(test.model, config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "1a872a15-ae34-4bff-abfd-7ad6b638c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # peft\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    base_model_name_or_path=model_name,\n",
    "    # modules_to_save=[\"wte\", \"lm_head\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(test.model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "40af7fa8-bc50-4d44-8e88-9bc8dd22b6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"gpt2\"\n",
    "\n",
    "# # loading base model and resizing embedding layers\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# # gradient checkpointing enabling\n",
    "# model.enable_input_require_grads()\n",
    "# model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# # peft\n",
    "# config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     base_model_name_or_path=model_name,\n",
    "#     modules_to_save=[\"wte\"] \n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, config)\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "e7a2cdef-db6a-4755-8c53-e35145a2f82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-trainable layer: model.transformer.wte.weight\n",
      "Non-trainable layer: model.transformer.wpe.weight\n",
      "Non-trainable layer: model.transformer.h.0.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.0.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.0.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.0.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.0.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.0.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.0.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.0.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.0.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.0.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.0.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.0.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.1.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.1.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.1.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.1.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.1.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.1.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.1.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.1.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.1.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.1.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.1.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.1.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.2.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.2.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.2.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.2.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.2.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.2.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.2.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.2.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.2.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.2.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.2.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.2.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.3.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.3.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.3.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.3.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.3.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.3.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.3.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.3.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.3.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.3.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.3.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.3.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.4.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.4.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.4.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.4.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.4.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.4.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.4.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.4.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.4.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.4.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.4.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.4.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.5.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.5.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.5.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.5.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.5.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.5.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.5.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.5.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.5.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.5.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.5.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.5.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.6.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.6.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.6.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.6.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.6.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.6.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.6.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.6.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.6.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.6.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.6.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.6.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.7.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.7.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.7.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.7.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.7.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.7.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.7.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.7.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.7.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.7.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.7.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.7.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.8.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.8.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.8.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.8.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.8.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.8.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.8.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.8.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.8.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.8.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.8.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.8.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.9.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.9.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.9.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.9.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.9.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.9.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.9.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.9.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.9.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.9.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.9.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.9.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.10.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.10.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.10.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.10.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.10.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.10.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.10.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.10.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.10.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.10.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.10.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.10.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.11.ln_1.weight\n",
      "Non-trainable layer: model.transformer.h.11.ln_1.bias\n",
      "Non-trainable layer: model.transformer.h.11.attn.c_attn.base_layer.weight\n",
      "Non-trainable layer: model.transformer.h.11.attn.c_attn.base_layer.bias\n",
      "Trainable layer: model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "Trainable layer: model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "Non-trainable layer: model.transformer.h.11.attn.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.11.attn.c_proj.bias\n",
      "Non-trainable layer: model.transformer.h.11.ln_2.weight\n",
      "Non-trainable layer: model.transformer.h.11.ln_2.bias\n",
      "Non-trainable layer: model.transformer.h.11.mlp.c_fc.weight\n",
      "Non-trainable layer: model.transformer.h.11.mlp.c_fc.bias\n",
      "Non-trainable layer: model.transformer.h.11.mlp.c_proj.weight\n",
      "Non-trainable layer: model.transformer.h.11.mlp.c_proj.bias\n",
      "Non-trainable layer: model.transformer.ln_f.weight\n",
      "Non-trainable layer: model.transformer.ln_f.bias\n",
      "Trainable layer: prosody_embed.weight\n",
      "Trainable layer: regressor.weight\n",
      "Trainable layer: regressor.bias\n"
     ]
    }
   ],
   "source": [
    "# Inspect the trainable parameters\n",
    "for name, param in test.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable layer: {name}\")\n",
    "    else:\n",
    "        print(f\"Non-trainable layer: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "b9c81b2d-fc00-49a1-86af-a9cc176a4ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight\n",
      "transformer.wpe.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.bias\n",
      "transformer.h.0.attn.c_attn.base_layer.weight\n",
      "transformer.h.0.attn.c_attn.base_layer.bias\n",
      "transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.bias\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.bias\n",
      "transformer.h.1.attn.c_attn.base_layer.weight\n",
      "transformer.h.1.attn.c_attn.base_layer.bias\n",
      "transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.bias\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.bias\n",
      "transformer.h.2.attn.c_attn.base_layer.weight\n",
      "transformer.h.2.attn.c_attn.base_layer.bias\n",
      "transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.bias\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.bias\n",
      "transformer.h.3.attn.c_attn.base_layer.weight\n",
      "transformer.h.3.attn.c_attn.base_layer.bias\n",
      "transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.bias\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.bias\n",
      "transformer.h.4.attn.c_attn.base_layer.weight\n",
      "transformer.h.4.attn.c_attn.base_layer.bias\n",
      "transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.bias\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.bias\n",
      "transformer.h.5.attn.c_attn.base_layer.weight\n",
      "transformer.h.5.attn.c_attn.base_layer.bias\n",
      "transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.bias\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.bias\n",
      "transformer.h.6.attn.c_attn.base_layer.weight\n",
      "transformer.h.6.attn.c_attn.base_layer.bias\n",
      "transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.bias\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.bias\n",
      "transformer.h.7.attn.c_attn.base_layer.weight\n",
      "transformer.h.7.attn.c_attn.base_layer.bias\n",
      "transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.bias\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.bias\n",
      "transformer.h.8.attn.c_attn.base_layer.weight\n",
      "transformer.h.8.attn.c_attn.base_layer.bias\n",
      "transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.bias\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.bias\n",
      "transformer.h.9.attn.c_attn.base_layer.weight\n",
      "transformer.h.9.attn.c_attn.base_layer.bias\n",
      "transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.bias\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.bias\n",
      "transformer.h.10.attn.c_attn.base_layer.weight\n",
      "transformer.h.10.attn.c_attn.base_layer.bias\n",
      "transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.bias\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.bias\n",
      "transformer.h.11.attn.c_attn.base_layer.weight\n",
      "transformer.h.11.attn.c_attn.base_layer.bias\n",
      "transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.bias\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "transformer.ln_f.weight\n",
      "transformer.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in test.model.named_parameters():\n",
    "    print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e3fb5915-424a-48e2-aa08-77017f43b54c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[408], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mtrainable:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mprint\u001b[39m (layer\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/prosody/lib/python3.10/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "for layer in test.model.layers:\n",
    "    if layer.trainable:\n",
    "        print (layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "09672b9e-2bc9-4184-9420-25eed0daacdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TaskType.FEATURE_EXTRACTION: 'FEATURE_EXTRACTION'>"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskType.FEATURE_EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "545b0a9a-8a8d-49de-8fbd-956f05dfde76",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['This is a']\n",
    "# inputs = [f'{ins} {tokenizer.mask_token}' for ins in inputs]\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "# outputs = model(**inputs, output_hidden_states=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
