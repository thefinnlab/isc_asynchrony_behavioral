{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c91df5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npredictor = WhisperNextWordPredictor()\\n\\n# Example timestamp_words list\\ntimestamp_words = [\\n    (0.0, \"hello\"),\\n    (0.5, \"how\"),\\n    (1.0, \"are\"),\\n    (1.5, \"you\")\\n]\\n\\n# Get logits for each word\\nresults = predictor.get_next_word_logits(\"path/to/audio.wav\", timestamp_words)\\n\\n# Get top 5 predictions for each word\\nfor result in results:\\n    print(f\"\\nPredictions after \\'{result[\\'word\\']}\\' at {result[\\'timestamp\\']}s:\")\\n    predictions = predictor.get_top_k_predictions(result[\\'next_word_logits\\'])\\n    for word, prob in predictions:\\n        print(f\"{word}: {prob:.3f}\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperModel, WhisperModel, WhisperForCausalLM\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "class WhisperNextWordPredictor:\n",
    "    def __init__(self, model_name=\"openai/whisper-base\"):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.processor = WhisperProcessor.from_pretrained(model_name)\n",
    "        self.model =  WhisperForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "    def get_next_word_logits(self, audio_path, timestamp_words):\n",
    "        \"\"\"\n",
    "        Extract next-word prediction logits at each timestamp in the transcript.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to audio file\n",
    "            timestamp_words: List of (timestamp, word) tuples\n",
    "        \n",
    "        Returns:\n",
    "            List of (word, timestamp, next_word_logits) tuples\n",
    "        \"\"\"\n",
    "        # Load and process audio\n",
    "        audio_features = self.processor.feature_extractor(\n",
    "            self._load_audio(audio_path), \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        \n",
    "        # Calculate frames per second for the model\n",
    "        # Whisper uses a 25ms frame stride\n",
    "        frames_per_second = 1 / 0.025  # 40 frames per second\n",
    "        \n",
    "        results = []\n",
    "        for i, (timestamp, word) in enumerate(timestamp_words):\n",
    "            # Create attention mask for audio up to current timestamp\n",
    "            # Convert timestamp to frame index\n",
    "            current_frame = int(timestamp * frames_per_second)\n",
    "            attention_mask = torch.ones_like(audio_features)\n",
    "            attention_mask[:, :, current_frame:] = 0  # Mask future frames\n",
    "            \n",
    "            # Get encoder hidden states with masked audio\n",
    "            encoder_outputs = self.model.get_encoder()(\n",
    "                audio_features,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            print (attention_mask.shape)\n",
    "            \n",
    "            # Convert previous words to input ids\n",
    "            previous_words = [w for _, w in timestamp_words[:i+1]]\n",
    "\n",
    "            tokens = self.processor.tokenizer(\n",
    "                \" \".join(previous_words),\n",
    "                return_tensors=\"pt\",\n",
    "                device=self.device\n",
    "            ) #.input_ids.to(self.device)\n",
    "            \n",
    "\n",
    "            # Get decoder outputs\n",
    "            decoder_outputs = self.model.get_decoder()(\n",
    "                input_ids=tokens['input_ids'],\n",
    "                encoder_hidden_states=encoder_outputs[0],\n",
    "                attention_mask=tokens['attention_mask']  # Remove channel dimension\n",
    "            )\n",
    "            \n",
    "            # Get logits for next token prediction\n",
    "            next_token_logits = self.model.proj_out(decoder_outputs[0][:, -1, :])\n",
    "            \n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'timestamp': timestamp,\n",
    "                'next_word_logits': next_token_logits.detach().cpu().numpy()\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def get_top_k_predictions(self, logits, k=5):\n",
    "        \"\"\"\n",
    "        Get top k predicted next words from logits\n",
    "        \n",
    "        Args:\n",
    "            logits: Logits array from get_next_word_logits\n",
    "            k: Number of top predictions to return\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1)\n",
    "        top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "        \n",
    "        predictions = []\n",
    "        for prob, idx in zip(top_k_probs[0], top_k_indices[0]):\n",
    "            word = self.processor.tokenizer.decode([idx])\n",
    "            predictions.append((word, prob.item()))\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def _load_audio(self, audio_path):\n",
    "        \"\"\"\n",
    "        Load and preprocess audio file to match Whisper's expected input format.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to the audio file\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Audio waveform resampled to 16kHz\n",
    "        \"\"\"\n",
    "        # Load audio file\n",
    "        # Whisper expects 16kHz mono audio\n",
    "        try:\n",
    "            waveform, sample_rate = librosa.load(\n",
    "                audio_path,\n",
    "                sr=16000,  # Whisper expects 16kHz\n",
    "                mono=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading audio file: {str(e)}\")\n",
    "            \n",
    "        # Check if audio is too short\n",
    "        if len(waveform) == 0:\n",
    "            raise ValueError(\"Audio file is empty\")\n",
    "            \n",
    "        # Normalize audio to float32 range [-1, 1]\n",
    "        if not np.isfinite(waveform).all():\n",
    "            raise ValueError(\"Audio file contains invalid values (inf or nan)\")\n",
    "            \n",
    "        waveform = librosa.util.normalize(waveform)\n",
    "        \n",
    "        return waveform\n",
    "\n",
    "# Complete example usage:\n",
    "\"\"\"\n",
    "predictor = WhisperNextWordPredictor()\n",
    "\n",
    "# Example timestamp_words list\n",
    "timestamp_words = [\n",
    "    (0.0, \"hello\"),\n",
    "    (0.5, \"how\"),\n",
    "    (1.0, \"are\"),\n",
    "    (1.5, \"you\")\n",
    "]\n",
    "\n",
    "# Get logits for each word\n",
    "results = predictor.get_next_word_logits(\"path/to/audio.wav\", timestamp_words)\n",
    "\n",
    "# Get top 5 predictions for each word\n",
    "for result in results:\n",
    "    print(f\"\\nPredictions after '{result['word']}' at {result['timestamp']}s:\")\n",
    "    predictions = predictor.get_top_k_predictions(result['next_word_logits'])\n",
    "    for word, prob in predictions:\n",
    "        print(f\"{word}: {prob:.3f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ab79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from config import *\n",
    "from tommy_utils import nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e97c61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'wheretheressmoke'\n",
    "\n",
    "# load the preprocessed file --> this has next-word-candidates selected\n",
    "stim_preprocessed_fn = os.path.join(BASE_DIR, 'stimuli/preprocessed', task, f'{task}_transcript-preprocessed.csv')\n",
    "df_preproc = pd.read_csv(stim_preprocessed_fn)\n",
    "\n",
    "# remap for our functions\n",
    "df_preproc = df_preproc.rename(columns={'Word_Written': 'word', 'Punctuation': 'punctuation'})\n",
    "\n",
    "# create a list of indices that we will iterate through to sample the transcript\n",
    "segments = nlp.get_segment_indices(n_words=len(df_preproc), window_size=25)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "accb568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/dark_matter/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "audio_fn = os.path.join(BASE_DIR, f'stimuli/audio/{task}.wav')\n",
    "\n",
    "timestamp_words = [tuple(df_preproc.loc[i, ['Onset', 'word']]) for i in range(10)]\n",
    "\n",
    "predictor = WhisperNextWordPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4c1e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n",
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n",
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'device': 'cpu'} not recognized.\n",
      "Keyword arguments {'device': 'cpu'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 3000])\n",
      "torch.Size([1, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "results = predictor.get_next_word_logits(audio_fn, timestamp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4172a7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions after 'I' at 0.0124716553288s:\n",
      "<|endoftext|>: 0.518\n",
      ".: 0.271\n",
      "!: 0.023\n",
      "?: 0.017\n",
      " (: 0.017\n",
      "\n",
      "Predictions after 'reached' at 0.1277805392196221s:\n",
      "<|endoftext|>: 0.422\n",
      ".: 0.124\n",
      "?: 0.043\n",
      ",: 0.025\n",
      " (: 0.017\n",
      "\n",
      "Predictions after 'over' at 0.4938471714535156s:\n",
      "<|endoftext|>: 0.374\n",
      "?: 0.141\n",
      ".: 0.063\n",
      "': 0.015\n",
      ";: 0.014\n",
      "\n",
      "Predictions after 'and' at 1.53900226757s:\n",
      "<|transcribe|>: 0.838\n",
      "<|translate|>: 0.065\n",
      "<|endoftext|>: 0.054\n",
      " I: 0.018\n",
      " and: 0.005\n",
      "\n",
      "Predictions after 'secretly' at 1.6649148365141804s:\n",
      " my: 0.061\n",
      "<|endoftext|>: 0.043\n",
      "?: 0.037\n",
      ".: 0.033\n",
      "': 0.022\n",
      "\n",
      "Predictions after 'undid' at 2.41700680272s:\n",
      " my: 0.208\n",
      "<|endoftext|>: 0.062\n",
      " and: 0.048\n",
      " I: 0.040\n",
      " of: 0.037\n",
      "\n",
      "Predictions after 'my' at 2.9010308719513387s:\n",
      " and: 0.127\n",
      "<|endoftext|>: 0.084\n",
      " —: 0.036\n",
      " I: 0.029\n",
      " of: 0.028\n",
      "\n",
      "Predictions after 'seatbelt' at 3.091175727917867s:\n",
      " and: 0.271\n",
      "<|endoftext|>: 0.128\n",
      " when: 0.042\n",
      " And: 0.038\n",
      "—: 0.033\n",
      "\n",
      "Predictions after 'and' at 4.53740830352817s:\n",
      " when: 0.197\n",
      " and: 0.162\n",
      "<|endoftext|>: 0.081\n",
      " --: 0.025\n",
      " und: 0.023\n",
      "\n",
      "Predictions after 'when' at 4.6619047619s:\n",
      " I: 0.069\n",
      "<|endoftext|>: 0.057\n",
      " when: 0.055\n",
      " and: 0.052\n",
      " of: 0.035\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"\\nPredictions after '{result['word']}' at {result['timestamp']}s:\")\n",
    "    predictions = predictor.get_top_k_predictions(result['next_word_logits'])\n",
    "    for word, prob in predictions:\n",
    "        print(f\"{word}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050bfacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dartfs/rc/lab/F/FinnLab/tommy/conda/envs/dark_matter/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1553e0105952493ab997f4c96a973f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbd9a75ac194c24b986dc4a547ed871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e653be13c5374452beb83357f9cc2183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cf8ad4431e49eeb2811bc3f927b069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4a713ed4db4766acdf5e852465c851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b875ebca5244d76865148bb0f71ecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f6957848764db3b6e5b5904adea6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a748ff85b63c45f79048bdc84f2ab797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5707ebb175490ab590c058523b5c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0ff5c827394064b79eaba66a0cda21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbd66e2d95f4fa495954b3c8b362de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Example timestamp_words list\n",
    "# timestamp_words = [\n",
    "#     (0.0, \"hello\"),\n",
    "#     (0.5, \"how\"),\n",
    "#     (1.0, \"are\"),\n",
    "#     (1.5, \"you\")\n",
    "# ]\n",
    "\n",
    "# # Get logits for each word\n",
    "# results = predictor.get_next_word_logits(\"path/to/audio.wav\", timestamp_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877cad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # we don't need to get the last word\n",
    "# for i, segment in enumerate(segments):\n",
    "\n",
    "#     ground_truth_index = segment[-1] + 1\n",
    "#     ground_truth_word = df_preproc.loc[ground_truth_index, 'word']\n",
    "    \n",
    "#     # also keep track of the current ground truth word\n",
    "#     inputs = nlp.transcript_to_input(df_preproc, segment, add_punctuation=True)\t\t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
