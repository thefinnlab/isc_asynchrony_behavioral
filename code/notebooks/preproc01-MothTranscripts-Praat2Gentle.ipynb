{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ede745",
   "metadata": {},
   "source": [
    "# Moth Transcripts to Gentle\n",
    "\n",
    "The Huth Moth transcripts are provided within Praat. There are two issues with this format:\n",
    "1. There is no joint transcript including punctuation (allowing us to present the next-word prediction framework)\n",
    "2. Our pipeline uses Gentle as its starting point to process files\n",
    "\n",
    "We load the Praat files and align it with a transcript generated through ChatGPT (adjusting mismatched words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6601b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import os, sys, glob\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from praatio import textgrid as tgio\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "sys.path.append('../utils/')\n",
    "\n",
    "from text_utils import strip_punctuation\n",
    "# from text_utils import get_pos_tags, get_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc80473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_textgrid(praat_fn):\n",
    "    '''\n",
    "    Load a praat textgrid file using PraatIO\n",
    "    '''\n",
    "    \n",
    "    # things to remove from the textgrid (indicates laughing, chewing, pauses etc)\n",
    "    REMOVE_CHARACTERS = ['sp', 'br', 'lg', 'cg', 'ls', 'ns', 'sl', 'ig',\n",
    "                         '{sp}', '{br}', '{lg}', '{cg}', '{ls}', '{ns}', '{sl}', '{ig}', 'pause']\n",
    "    \n",
    "    # open the textgrid\n",
    "    tg = tgio.openTextgrid(praat_fn, includeEmptyIntervals=False, reportingMode=\"warning\") \n",
    "    \n",
    "    # remove entries of unwanted characters\n",
    "    for tier_name in tg.tierNames:\n",
    "        # get the current tier\n",
    "        tier = tg.getTier(tier_name)\n",
    "        \n",
    "        for x in tier.entries:\n",
    "            if x[-1].lower() in REMOVE_CHARACTERS:\n",
    "                tier.deleteEntry(x)\n",
    "\n",
    "#         for char in REMOVE_CHARACTERS:\n",
    "#             upper_set = set(tier.find(char.upper()))\n",
    "#             lower_set = set(tier.find(char.lower()))\n",
    "#             remove_idxs = sorted(upper_set.union(lower_set))\n",
    "\n",
    "#             # go through each index and remove\n",
    "#             for idx in remove_idxs:\n",
    "#                 try:\n",
    "#                     tier.deleteEntry(tier.entries[idx])\n",
    "#                 except:\n",
    "#                     print (idx)\n",
    "    \n",
    "#     # go through each entry at the word tier, remove the items\n",
    "#     words = [x for x in tg.getTier('word').entries if x[-1].lower() not in REMOVE_CHARACTERS]\n",
    "#     phones = [x for x in tg.getTier('phone').entries if x[-1].lower() not in REMOVE_CHARACTERS]\n",
    "#     words = tg.getTier('word').entries\n",
    "#     phones = tg.getTier('phone').entries\n",
    "    return tg\n",
    "\n",
    "def load_transcription(transcript_fn):\n",
    "    \n",
    "    with open(transcript_fn, 'r') as f: #open the file\n",
    "        contents = f.readlines() #put the lines to a variable (list).\n",
    "        \n",
    "    # get the transcription stripped of punctuation\n",
    "    words_transcribed = strip_punctuation(contents).split()\n",
    "    \n",
    "    return contents, words_transcribed\n",
    "\n",
    "def textgrid_to_gentle(praat_fn, transcript_fn):\n",
    "    '''\n",
    "    Transform Moth dataset textgrid files into gentle format\n",
    "    '''\n",
    "    \n",
    "    textgrid = load_clean_textgrid(praat_fn)\n",
    "    tg_words = textgrid.getTier('word')\n",
    "    \n",
    "    contents, words_transcribed = load_transcription(transcript_fn)\n",
    "    \n",
    "    assert (len(tg_words) == len(words_transcribed))\n",
    "    \n",
    "    # create the dictionary to store things in\n",
    "    # put the transcript in the raw form\n",
    "    align = {}\n",
    "    align['transcript'] = contents[0]\n",
    "    align['words'] = []\n",
    "    \n",
    "    # Taken from Kaldi metasentence tokenizer\n",
    "    # splits the transcript based on any punctuation besides for apostrophes and hyphens\n",
    "    regex_split_pattern = r'(\\w|\\.\\w|\\:\\w|\\â€™\\w|\\'\\w|\\-\\w)+'\n",
    "    \n",
    "    iterator = list(re.finditer(regex_split_pattern, ''.join(contents), re.UNICODE))\n",
    "    n_items = len(list(iterator))\n",
    "    \n",
    "    # make sure the iterator matches the length\n",
    "    assert (n_items == len(tg_words) == len(words_transcribed))\n",
    "    \n",
    "    # if all matches we're good to go\n",
    "    for word_info, m in zip(tg_words, iterator):\n",
    "        # span of the word in characters relative to the overall string\n",
    "        start_offset, end_offset = m.span()\n",
    "        word = m.group()\n",
    "        \n",
    "        # crop textgrid to the word\n",
    "        cropped_grid = textgrid.crop(cropStart=word_info[0], cropEnd=word_info[1], mode='truncated', rebaseToZero=False)\n",
    "        tg_phones = cropped_grid.getTier('phone').entries\n",
    "        word_phones = []\n",
    "\n",
    "        for phone_info in tg_phones:\n",
    "            phone = re.sub(r'\\d+', '', phone_info[-1])\n",
    "            duration = phone_info[1] - phone_info[0]\n",
    "            word_phones.append({'duration': duration, 'phone': phone})\n",
    "\n",
    "        word_align = {\n",
    "            'alignedWord': word.lower(),\n",
    "            \"case\": \"success\",\n",
    "            'word': word,\n",
    "            'start': word_info[0],\n",
    "            'end': word_info[1],\n",
    "            'phones': word_phones,\n",
    "            \"startOffset\": start_offset,\n",
    "            \"endOffset\": end_offset,\n",
    "        }\n",
    "        \n",
    "        align['words'].append(word_align)\n",
    "        \n",
    "    return align"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0e456",
   "metadata": {},
   "source": [
    "## Set paths \n",
    "\n",
    "These are paths to the main directory and the stimulus directory\n",
    "\n",
    "CHANGE THE PATH BELOW TO MATCH YOUR DIRECTORY --> FinnLabTasks/transcript_alignment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d50ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/'\n",
    "datasets_dir = '/dartfs/rc/lab/F/FinnLab/datasets/'\n",
    "# stim_dir = os.path.join(datasets_dir, 'IBC/stimuli/lepetitprince/')\n",
    "\n",
    "# for prepping for onlin eexpt\n",
    "stim_dir = os.path.join(base_dir, 'stimuli')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed86e4c",
   "metadata": {},
   "source": [
    "## Load Praat files\n",
    "\n",
    "We first get all the filenames of TextGrid files within the stimulus directory. We also print out the number of files within this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35c06145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files in dataset: 29\n"
     ]
    }
   ],
   "source": [
    "praat_fns = sorted(glob.glob(os.path.join(stim_dir, 'praat', '*.TextGrid')))\n",
    "\n",
    "print (f'Total files in dataset: {len(praat_fns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875ae474",
   "metadata": {},
   "source": [
    "<b>Note:</b> This is <b>very</b> likely not to work on the first time. Follow the steps below to get the file to load!\n",
    "\n",
    "We are going to load a Praat TextGrid file. This will probably not work on the first time due to overlapping timestamps. To address this, do the following:\n",
    "1. Open the .TextGrid file in a text editor (e.g., TextEdit, SublimeText)\n",
    "2. Look at the Python error -- you will need to manually adjust these overlapping times. Copy the first number in the second parentheses:\n",
    "    - <b>Example error:</b> Two intervals in the same tier overlap in time: (START_1, END_1, sp) and (START_2, END_2, B)\n",
    "    - For this error, copy the number \"START_2\"\n",
    "3. Go to the text editor, and search (cmd + F) for the copied number (e.g., \"START_2\").\n",
    "4. Adjust the word/phoneme before's end time (e.g., END_1) to match the copied number (\"START_2\").\n",
    "5. Save the file and rerun the code\n",
    "6. Repeat for as many times until the file loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244a517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stimulus name: wheretheressmoke\n",
      "Successfully loaded Praat file!\n"
     ]
    }
   ],
   "source": [
    "# select a file number to load -- we then select that file from the list of alphabetized file names\n",
    "file_num = -1\n",
    "praat_fn = praat_fns[file_num]\n",
    "\n",
    "# now grab the current filename as a path -- print out only the filename (no extension)\n",
    "filepath = Path(praat_fn)\n",
    "stim_name = filepath.stem\n",
    "print (f'Stimulus name: {filepath.stem}')\n",
    "\n",
    "# attempt to load the praat file -- if this doesn't work, follow the steps above \n",
    "tg = tgio.openTextgrid(praat_fns[file_num], includeEmptyIntervals=False, reportingMode=\"warning\") \n",
    "\n",
    "print (f'Successfully loaded Praat file!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c0f2a",
   "metadata": {},
   "source": [
    "## Adjust the words to have punctuation\n",
    "\n",
    "After loading the transcript using Praat, we concatenate all the transcript words and pass it to ChatGPT to ensure punctuation. Then we need to go through comparing word by word making sure of the following:\n",
    "-  The new transcript matches the original number of words\n",
    "- Words are spelled correctly (as full words)\n",
    "\n",
    "This cell below will print out all the words of the TextGrid as a string. You will need to do the following:\n",
    "1. Open ChatGPT: https://chat.openai.com/chat\n",
    "2. Type the following instructions: \"Add punctuation and capitalization to the following but change nothing else:\"\n",
    "3. Copy and paste the transcript below <i>after</i> the instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d02c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I REACHED OVER AND SECRETLY UNDID MY SEATBELT AND WHEN HIS FOOT HIT THE BRAKE AT THE RED LIGHT I FLUNG OPEN THE DOOR AND I RAN I HAD NO SHOES ON I WAS CRYING I HAD NO WALLET BUT I WAS OKAY BECAUSE I HAD MY CIGARETTES AND I DIDNT WANT ANY PART OF FREEDOM IF I DIDNT HAVE MY CIGARETTES WHEN YOU LIVE WITH SOMEONE WHO HAS A TEMPER A VERY BAD TEMPER A VERY VERY BAD TEMPER YOU LEARN TO PLAY AROUND THAT YOU LEARN THIS TIME ILL PLAY POSSUM AND NEXT TIME ILL JUST BE REAL NICE OR ILL SAY YES TO EVERYTHING OR YOU MAKE YOURSELF SCARCE OR YOU RUN AND THIS WAS ONE OF THE TIMES WHEN YOU JUST RUN AND AS I WAS RUNNING I THOUGHT THIS WAS A GREAT PLACE TO JUMP OUT BECAUSE THERE WERE BIG LAWNS AND THERE WERE CULDESACS AND SOMETIMES HE WOULD COME AFTER ME AND DRIVE AND YELL STUFF AT ME TO GET BACK IN GET BACK IN AND I WAS LIKE NO IM OUT OF HERE THIS IS GREAT AND I WENT AND HID BEHIND A CABANA AND HE LEFT AND I HAD MY CIGARETTES AND UH I STARTED TO WALK IN THIS BEAUTIFUL NEIGHBORHOOD IT WAS TENTHIRTY AT NIGHT AND IT WAS SILENT AND LOVELY AND THERE WAS NO SOUND EXCEPT FOR SPRINKLERS CH CH CH CH CH CH CH CH AND I WAS ENJOYING MYSELF AND ENJOYING THE ABSENCE OF ANGER AND ENJOYING THESE FEW HOURS I KNEW ID HAVE OF FREEDOM AND JUST TO PERFECT IT I THOUGHT ILL HAVE A SMOKE AND THEN IT OCCURRED TO ME WITH HORRIFYING SPEED I DONT HAVE A LIGHT JUST THEN AS IF IN ANSWER I SEE A FIGURE UP AHEAD WHO IS THAT ITS NOT HIM OKAY THEY DONT HAVE A DOG WHO IS THAT WHAT UH WHAT ARE THEY DOING OUT ON THIS SUBURBAN STREET AND THE PERSON COMES CLOSER AND I COULD SEE ITS A WOMAN AND THEN I CAN SEE SHE HAS HER HANDS IN HER FACE OH SHES CRYING AND THEN SHE SEES ME AND SHE COMPOSES HERSELF AND SHE GETS CLOSER AND I SEE SHE HAS NO SHOES ON SHE HAS NO SHOES ON AND SHES CRYING AND SHES OUT ON THE STREET STREET I RECOGNIZE HER THOUGH IVE NEVER MET HER AND JUST AS SHE PASSES ME SHE SAYS YOU GOT A CIGARETTE AND I SAY YOU GOT A LIGHT AND SHE SAYS DAMN I HOPE SO AND THEN FIRST SHE DIGS INTO HER CUTOFFS IN THE FRONT NOTHING AND THEN DIGS IN THE BACK AND THEN SHE HAS THIS VEST ON THAT HAS FIFTY MILLION LITTLE POCKETS ON IT AND SHES CHECKING AND CHECKING AND ITS LOOKING BAD ITS LOOKING VERY BAD SHE DIGS BACK IN THE FRONT AGAIN DEEP DEEP AND SHE PULLS OUT A PACK OF MATCHES THAT HAD BEEN LAUNDERED AT LEAST ONCE UGH WE OPEN IT UP AND THERE IS ONE MATCH INSIDE OKAY OH MY GOD THIS TAKES ON ITS LIKE NASA NOW WE GOT TO LIKE OH HOW ARE WE GONNA DO IT OKAY AND WE WE HUNKER DOWN WE CROUCH ON THE GROUND AND WHERES THE WIND COMING FROM WERE STOPPING I TAKE OUT MY CIGARETTES LETS GET THE CIGARETTES READY OH MY BRAND SHE SAYS NOT SURPRISING AND WE BOTH HAVE OUR CIGARETTES AT THE READY SHE STRIKES ONCE NOTHING SHE STRIKES AGAIN YES FIRE PUFF INHALE MM SWEET KISS OF THAT CIGARETTE AND WE SIT THERE AND WERE LOVING THE NICOTINE AND WE BOTH NEED THIS RIGHT NOW I CAN TELL THE NIGHTS BEEN TOUGH IMMEDIATELY WE START TO REMINISCE ABOUT OUR THIRTYSECOND RELATIONSHIP I DIDNT THINK THAT WAS GONNA HAPPEN ME NEITHER OH MAN THAT WAS CLOSE OH IM SO LUCKY I SAW YOU YEAH THEN SHE SURPRISES ME BY SAYING WHAT WAS THE FIGHT ABOUT AND I SAY WHA WHAT ARE THEY ALL ABOUT AND SHE SAID I KNOW WHAT YOU MEAN SHE SAID WAS IT A BAD ONE AND AND I SAID YOU KNOW LIKE MEDIUM SHE SAID OH AND WE START TO TRADE STORIES ABOUT OUR LIVES WERE BOTH FROM UP NORTH WERE BOTH KIND OF NEWISH TO THE NEIGHBORHOOD THIS IS IN FLORIDA WE BOTH WENT TO COLLEGE NOT GREAT COLLEGES BUT MAN WE GRADUATED AND IM ACTUALLY FINDING MYSELF A LITTLE JEALOUS OF HER BECAUSE SHE HAS THIS REALLY COOL JOB WASHING DOGS SHE HAD HORSES BACK HOME AND SHE REALLY LOVES ANIMALS AND SHE WANTS TO BE A VET AND IM LIKE MAN YOURE HALFWAY THERE IM A WAITRESS AT AN ICE CREAM PARLOR SO UM THATS NOT I DONT KNOW WHERE I WANT TO BE BUT I KNOW ITS NOT THAT AND THEN IT GETS A LITTLE DEEPER AND WE SHARE SOME OTHER STUFF ABOUT WHAT OUR LIVES ARE LIKE THINGS THAT I CANT EVER TELL PEOPLE AT HOME THIS GIRL I CAN TELL HER THE REALLY UGLY STUFF AND SHE STILL UNDERSTANDS HOW IT CAN STILL BE PRETTY SHE UNDERSTANDS LIKE HOW NICE HES GONNA BE WHEN I GET HOME AND HOW SWEET THATLL BE WE ARE CHAINSMOKING OFF EACH OTHER OH THATS ALMOST OUT COME ON AND WE WE GO THROUGH THIS ENTIRE PACK UNTIL ITS GONE AND THEN I SAY YOU KNOW WHAT UH THIS IS A LITTLE FUNNY BUT YOURE GONNA HAVE TO SHOW ME THE WAY TO GET HOME BECAUSE ALTHOUGH IM TWENTYTHREE YEARS OLD I DONT HAVE MY DRIVERS LICENSE YET AND I JUST JUMPED OUT RIGHT WHEN I NEEDED TO AND SHE SAYS WELL WHY DONT YOU COME BACK TO MY HOUSE AND ILL GIVE YOU A RIDE I SAY OKAY GREAT AND WE START WALKING AND UH WE GET TO THIS UM LOTS OF UH LIGHTS AND UH THE ROADS ARE GETTING WIDER AND WIDER AND THERES MORE CARS AND I SEE UM LOTS OF STORES YOU KNOW LAUNDROMATS AND DOLLAR STORES AND EMERGECENTERS AND THEN WE CROSS OVER US ONE AND UH SHE LEADS ME TO SOME PLACE AND I THINK NO BUT YES CARLS EFFICIENCY APARTMENTS THIS GIRL LIVES THERE AND ITS HORRIBLE AND ITS LIT UP SO BRIGHT JUST TO ILLUMINATE THE HORRIBLENESS OF IT ITS THE KIND OF PLACE WHERE YOU DRIVE YOUR CAR RIGHT UP AND THE DOORS RIGHT THERE AND THERES FIFTY MILLION CIGARETTE BUTTS OUTSIDE AND THERES LIKE DOORS ONE THROUGH SEVEN AND YOU KNOW BEHIND EVERY SINGLE DOOR THERES SOME HORRIBLE MISERY GOING ON THERES SOMEONE CRYING OR DRUNK OR LONELY OR CRUEL AND I THINK OH GOD SHE LIVES HERE HOW AWFUL WE GO TO THE DOOR DOOR NUMBER FOUR AND SHE VERY VERY QUIETLY KEYS IN AS SOON AS THE DOOR OPENS I HEAR THE BLARE OF TELEVISION COME OUT AND ON THE BLUE LIGHT OF THE TELEVISION THE SMOKE OF A HUNDRED CIGARETTES IN THAT LITTLE CRACK OF LIGHT AND I HEAR THE MAN AND HE SAYS WHERE WERE YOU AND SHE SAYS NEVER MIND IM BACK AND HE SAYS YOU ALRIGHT AND SHE SAYS YEAH IM ALRIGHT AND THEN SHE TURNS TO ME AND SAYS YOU WANT A BEER AND HE SAYS WHO THE FUCK IS THAT AND SHE PULLS ME OVER AND HE SEES ME AND HE SAYS OH HEY IM NOT A THREAT JUST THEN HE TAKES A DRAG OF HIS CIGARETTE A VERY HARD HARD DRAG YOU KNOW THE KIND THAT MAKES THE END OF IT REALLY HEAT UP HOT HOT HOT AND LONG AND ITS A LITTLE SCARY AND I FOLLOW THE CIGARETTE DOWN BECAUSE IM AFRAID OF THAT HEAD FALLING OFF AND IM SURPRISED WHEN I SEE IN THE CROOK OF HIS ARM A LITTLE BOY SLEEPING A TODDLER AND I THINK AND JUST THEN THE GIRL REACHES UNDERNEATH THE BED AND TAKES OUT A CARTON AND SHE TAPS OUT THE LAST S PACK OF CIGARETTES IN THERE AND ON THE WAY UP SHE KISSES THE LITTLE BOY AND THEN SHE KISSES THE MAN AND THE MAN SAYS AGAIN YOU ALRIGHT AND SHE SAYS YEAH IM JUST GONNA GO OUT AND SMOKE WITH HER AND SO WE GO OUTSIDE AND SIT AMONGST THE CIGARETTE BUTTS AND SMOKE AND I SAY WOW THATS YOUR LITTLE BOY AND SHE SAYS YEAH ISNT HE BEAUTIFUL AND I SAY YEAH HE IS HE IS BEAUTIFUL HES MY LIGHT HE KEEPS ME GOING SHE SAYS WE FINISH OUR CIGARETTES SHE FINISHES HER BEER I DONT HAVE A BEER BECAUSE I CANT GO HOME WITH BEER ON MY BREATH AND SHE GOES INSIDE TO GET THE KEYS SHE TAKES TOO LONG IN THERE GETTING THE KEYS AND I THINK SOMETHING MUST BE WRONG AND SHE COMES OUT AND SHE SAYS LOOK IM REALLY SORRY BUT UM LIKE WE DONT HAVE ANY GAS IN THE CAR ITS ALREADY ON E AND HE NEEDS TO GET TO WORK IN THE MORNING AND UM I YOU KNOW I IM GONNA BE WALK TO WORK AS IT IS SO WHAT I DID WAS THOUGH HERE LOOK I DREW OUT THIS MAP FOR YOU AND YOURE REALLY YOURE LIKE A MILE AND A HALF FROM HOME AND UM IF YOU WALK THREE STREETS OVER YOULL BE BACK ON THAT PRETTY STREET AND YOU JUST TAKE THAT AND YOULL BE FINE AND SHE ALSO HAS WRAPPED UP IN TOILET PAPER SEVEN CIGARETTES FOR ME A THIRD OF HER PACK I NOTE AND A NEW PACK OF MATCHES AND SHE TELLS ME GOODBYE AND THAT WAS GREAT TO MEET YOU AND HOW LUCKY AND THAT WAS FUN AND YOU KNOW LETS BE FRIENDS AND I SAY YEAH OKAY AND I WALK AWAY BUT I KIND OF KNOW WERE NOT GONNA BE FRIENDS I MIGHT NOT EVER SEE HER AGAIN AND I KIND OF KNOW I DONT THINK SHES EVER GOING TO BE A VET AND I CROSS AND I WALK AWAY AND MAYBE THIS WOULDVE SEEMED LIKE A VISIT FROM MY POSSIBLE FUTURE AND SCARY BUT IT KIND OF DOES THE OPPOSITE ON THE WALK HOME IM LIKE MAN THAT WAS REALLY GRIM OVER THERE AND IM GOING HOME NOW TO MY NICE BOYFRIEND AND HE IS GONNA BE SO EXTRA HAPPY TO SEE ME AND WE HAVE A ONEBEDROOM APARTMENT AND WE HAVE TWO TREES AND THERES A YARD AND WE HAVE THIS JAR IN THE KITCHEN WHERE THERES LIKE LOOSE MONEY THAT WE CAN USE FOR ANYTHING LIKE WE WOULD NEVER EVER RUN OUT OF GAS AND UM I DONT HAVE A BABY YOU KNOW SO I CAN LEAVE WHENEVER I WANT I SMOKED ALL SEVEN CIGARETTES ON THE WAY HOME AND PEOPLE WHO HAVE NEVER SMOKED CIGARETTES JUST THINK ICK DISGUSTING AND POISON BUT UNLESS YOUVE HAD THEM AND HELD THEM DEAR YOU DONT KNOW HOW GREAT THEY CAN BE AND WHAT FRIENDS AND COMFORT AND KINSHIP THEY CAN BRING IT TOOK ME A LONG TIME TO QUIT THAT BOYFRIEND AND THEN TO QUIT SMOKING AND UH SOMETIMES I STILL MISS THE SMOKING\n"
     ]
    }
   ],
   "source": [
    "def get_textgrid_words(textgrid):\n",
    "    '''\n",
    "    Extracts the words in the textgrid to show in a legible format\n",
    "    '''\n",
    "    words = [strip_punctuation(x[-1]) for x in textgrid.getTier('word').entries]\n",
    "    return words\n",
    "\n",
    "# load the textgrid removing all enunciations\n",
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "\n",
    "# gets all the words in the textgrid as an interpretable string\n",
    "tg_words = get_textgrid_words(textgrid)\n",
    "print (' '.join(tg_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b0cf1",
   "metadata": {},
   "source": [
    "## Create a transcript file\n",
    "\n",
    "ChatGPT will then print out a verion of the transcript with punctuation. However, we need to double-check that the words match the original transcript. After getting the transcript from ChatGPT:\n",
    "1. Go to the directory '/stimuli/transcripts/' \n",
    "2. Create a text file names \"STIMULUSNAME.txt\" (where STIMULUSNAME is the name of the stimulus - printed out above)\n",
    "3. Paste the transcript from ChatGPT into the text file\n",
    "\n",
    "You should now be able to load the file in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb04d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_praat_to_transcript(words_original, words_transcribed):\n",
    "    '''\n",
    "    Compares words from TextGrid and ChatGPT transcript word by word\n",
    "    '''\n",
    "    \n",
    "    for i, (word_orig, word_transc) in enumerate(zip(words_original, words_transcribed)):\n",
    "        if word_orig.lower() != word_transc.lower():\n",
    "            print (f'Word index: {i}')\n",
    "            print (f'TextGrid word: {word_orig}')\n",
    "            print (f'Transcript word: {word_transc}')\n",
    "            print (f'Word context: {words_original[i-5:i+5]}')\n",
    "            break\n",
    "    \n",
    "    if i+1 == len(words_original):\n",
    "        print (f'Finished transcript!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25aa20",
   "metadata": {},
   "source": [
    "## Check the transcript with the original file\n",
    "\n",
    "Run the following cell to compare words from the TextGrid to words from the ChatGPT transcript.\n",
    "\n",
    "Sometimes words will be misaligned:\n",
    "- ChatGPT may have missed some words\n",
    "- The Praat words may be misspelled, or hyphenated words may have been treated separately (e.g., eighty-four --> eighty four)\n",
    "\n",
    "You will need to correct this in either 1) the transcript or 2) the Praat file and make note of the change within the tracking document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bd1a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished transcript!\n"
     ]
    }
   ],
   "source": [
    "transcript_fn = os.path.join(stim_dir, 'transcripts', f'{stim_name}_transcript.txt')\n",
    "\n",
    "# load the textgrid and get all words\n",
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "words_original = get_textgrid_words(textgrid)\n",
    "\n",
    "# load the ChatGPT created transcript\n",
    "_, words_transcribed = load_transcription(transcript_fn)\n",
    "\n",
    "compare_praat_to_transcript(words_original, words_transcribed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a518f12",
   "metadata": {},
   "source": [
    "## Create a gentle align file from Praat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e074245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gentle_stim_dir = os.path.join(stim_dir, 'gentle', stim_name)\n",
    "\n",
    "# if the directory does not exist, make the directory\n",
    "if not os.path.exists(gentle_stim_dir):\n",
    "    os.makedirs(gentle_stim_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91240a9c",
   "metadata": {},
   "source": [
    "Now that the directory is created, we will do the following:\n",
    "- Write the aligned file to the directory\n",
    "- Move a copy of the stimulus audio to the directory\n",
    "- Move a copy of the transcript to the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "819e0684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/stimuli/gentle/wheretheressmoke/a.wav'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "praat_fn = praat_fns[file_num]\n",
    "transcript_fn = os.path.join(stim_dir, 'transcripts', f'{stim_name}_transcript.txt')\n",
    "\n",
    "# given the two files, creates a file in gentle aligned format\n",
    "align_json = textgrid_to_gentle(praat_fn, transcript_fn)\n",
    "\n",
    "# write the file out to the directory\n",
    "with open(os.path.join(gentle_stim_dir, 'align.json'), 'w') as f:\n",
    "    json.dump(align_json, f)\n",
    "    \n",
    "# copy the transcript file renaming it to \"transcript.txt\" matching gentle convention\n",
    "shutil.copyfile(\n",
    "    transcript_fn, \n",
    "    os.path.join(gentle_stim_dir, 'transcript.txt')\n",
    ")\n",
    "\n",
    "# copy the stimulus audio file renaming it to \"a.wav\" matching gentle convention\n",
    "shutil.copyfile(\n",
    "    os.path.join(stim_dir, 'audio', f'{stim_name}.wav'), \n",
    "    os.path.join(gentle_stim_dir, 'a.wav')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f700b7",
   "metadata": {},
   "source": [
    "### old for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9a476278-4a21-4569-bf6f-8733f946dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(f'/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/stimuli/preprocessed/howtodraw/howtodraw_transcript-preprocessed.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3caeb396-0ad4-4208-a195-532d21473b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'/dartfs/rc/lab/F/FinnLab/tommy/isc_asynchrony_behavior/stimuli/preprocessed/howtodraw/howtodraw_transcript-preprocessed.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "id": "118b1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2740\n",
      "2742\n"
     ]
    }
   ],
   "source": [
    "print (len(tg_words))\n",
    "print (len (words_transcribed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1590,
   "id": "63bb53f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<praatio.data_classes.interval_tier.IntervalTier at 0x2b22e9c7f880>"
      ]
     },
     "execution_count": 1590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1591,
   "id": "33d81c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'story',\n",
       " 'is',\n",
       " 'about',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'jobs',\n",
       " 'that',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'shaped',\n",
       " 'my',\n",
       " 'life',\n",
       " 'and',\n",
       " 'shaped',\n",
       " 'my',\n",
       " 'whole',\n",
       " 'destiny',\n",
       " 'And',\n",
       " 'it',\n",
       " 'was',\n",
       " 'a',\n",
       " 'couple',\n",
       " 'of',\n",
       " 'jobs',\n",
       " 'I',\n",
       " 'had',\n",
       " 'uh',\n",
       " 'during',\n",
       " 'the',\n",
       " 'ages',\n",
       " 'of',\n",
       " 'twenty',\n",
       " 'to',\n",
       " 'twentyone',\n",
       " 'So',\n",
       " 'the',\n",
       " 'story',\n",
       " 'begins',\n",
       " 'in',\n",
       " 'in',\n",
       " 'uh',\n",
       " 'yes',\n",
       " 'I',\n",
       " 'we',\n",
       " 'yes',\n",
       " 'I',\n",
       " 'was',\n",
       " 'employed',\n",
       " 'back',\n",
       " 'then',\n",
       " 'Ok',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'why',\n",
       " 'you',\n",
       " 'laughed',\n",
       " 'but',\n",
       " 'Ill',\n",
       " 'Ill',\n",
       " 'accept',\n",
       " 'that',\n",
       " 'Its',\n",
       " 'a',\n",
       " 'good',\n",
       " 'sign',\n",
       " 'Ok',\n",
       " 'thinking',\n",
       " 'out',\n",
       " 'loud',\n",
       " 'here',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'in',\n",
       " 'nineteen',\n",
       " 'eightyfour',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'sophomore',\n",
       " 'at',\n",
       " 'Princeton',\n",
       " 'You',\n",
       " 'can',\n",
       " 'laugh',\n",
       " 'at',\n",
       " 'that',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'No',\n",
       " 'Alright',\n",
       " 'So',\n",
       " 'in',\n",
       " 'nineteen',\n",
       " 'eightyfour',\n",
       " 'I',\n",
       " 'was',\n",
       " 'a',\n",
       " 'sophomore',\n",
       " 'at',\n",
       " 'Princeton',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'year',\n",
       " 'off',\n",
       " 'because',\n",
       " 'I',\n",
       " 'had',\n",
       " 'I',\n",
       " 'had',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'reputation',\n",
       " 'at',\n",
       " 'school',\n",
       " 'I',\n",
       " 'was',\n",
       " 'drinking',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'getting',\n",
       " 'in',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'trouble',\n",
       " 'So',\n",
       " 'I',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'take',\n",
       " 'a',\n",
       " 'year',\n",
       " 'off',\n",
       " 'Also',\n",
       " 'I',\n",
       " 'had',\n",
       " 'joined',\n",
       " 'the',\n",
       " 'army',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'for',\n",
       " 'Princeton',\n",
       " 'and',\n",
       " 'I',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'four',\n",
       " 'years',\n",
       " 'of',\n",
       " 'active',\n",
       " 'duty',\n",
       " 'after',\n",
       " 'school',\n",
       " 'and',\n",
       " 'eight',\n",
       " 'years',\n",
       " 'of',\n",
       " 'reserve',\n",
       " 'duty',\n",
       " 'So',\n",
       " 'this',\n",
       " 'was',\n",
       " 'my',\n",
       " 'last',\n",
       " 'chance',\n",
       " 'at',\n",
       " 'freedom',\n",
       " 'was',\n",
       " 'after',\n",
       " 's',\n",
       " 'you',\n",
       " 'know',\n",
       " 'was',\n",
       " 'taking',\n",
       " 'this',\n",
       " 'year',\n",
       " 'off',\n",
       " 'from',\n",
       " 'school',\n",
       " 'Later',\n",
       " 'just',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Harpers',\n",
       " 'crowd',\n",
       " 'I',\n",
       " 'did',\n",
       " 'become',\n",
       " 'a',\n",
       " 'conscientious',\n",
       " 'objector',\n",
       " 'and',\n",
       " 'so',\n",
       " 'I',\n",
       " 'got',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'military',\n",
       " 'So',\n",
       " 'and',\n",
       " 'avoided',\n",
       " 'the',\n",
       " 'Golf',\n",
       " 'War',\n",
       " 'unlike',\n",
       " 'my',\n",
       " 'fellow',\n",
       " 'ROTC',\n",
       " 'guys',\n",
       " 'which',\n",
       " 'was',\n",
       " 'a',\n",
       " 'good',\n",
       " 'thing',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'but',\n",
       " 'this',\n",
       " 'was',\n",
       " 'before',\n",
       " 'I',\n",
       " 'knew',\n",
       " 'I',\n",
       " 'was',\n",
       " 'getting',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'army',\n",
       " 'This',\n",
       " 'was',\n",
       " 'gonna',\n",
       " 'be',\n",
       " 'my',\n",
       " 'year',\n",
       " 'of',\n",
       " 'freedom',\n",
       " 'I',\n",
       " 'nee',\n",
       " 'I',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'make',\n",
       " 'money',\n",
       " 'though',\n",
       " 'during',\n",
       " 'this',\n",
       " 'year',\n",
       " 'off',\n",
       " 'And',\n",
       " 'a',\n",
       " 'friend',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'I',\n",
       " 'know',\n",
       " 'its',\n",
       " 'going',\n",
       " 'to',\n",
       " 'seem',\n",
       " 'absurd',\n",
       " 'suggested',\n",
       " 'that',\n",
       " 'I',\n",
       " 'be',\n",
       " 'a',\n",
       " 'model',\n",
       " 'And',\n",
       " 'he',\n",
       " 'took',\n",
       " 'some',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'I',\n",
       " 'know',\n",
       " 'I',\n",
       " 'have',\n",
       " 'rings',\n",
       " 'under',\n",
       " 'my',\n",
       " 'eyes',\n",
       " 'that',\n",
       " 'go',\n",
       " 'all',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'my',\n",
       " 'skull',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'no',\n",
       " 'hair',\n",
       " 'But',\n",
       " 'at',\n",
       " 'twenty',\n",
       " 'he',\n",
       " 'had',\n",
       " 'it',\n",
       " 'in',\n",
       " 'his',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'I',\n",
       " 'should',\n",
       " 'be',\n",
       " 'a',\n",
       " 'model',\n",
       " 'So',\n",
       " 'he',\n",
       " 'takes',\n",
       " 'some',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'I',\n",
       " 'must',\n",
       " 'admit',\n",
       " 'I',\n",
       " 'I',\n",
       " 'thought',\n",
       " 'I',\n",
       " 'was',\n",
       " 'ugly',\n",
       " 'but',\n",
       " 'I',\n",
       " 'was',\n",
       " 'vain',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'try',\n",
       " 'it',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'what',\n",
       " 'I',\n",
       " 'was',\n",
       " 'thinking',\n",
       " 'So',\n",
       " 'I',\n",
       " 'go',\n",
       " 'I',\n",
       " 'look',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Princeton',\n",
       " 'library',\n",
       " 'they',\n",
       " 'had',\n",
       " 'a',\n",
       " 'Manhattan',\n",
       " 'phone',\n",
       " 'book',\n",
       " 'and',\n",
       " 'I',\n",
       " 'look',\n",
       " 'up',\n",
       " 'modeling',\n",
       " 'and',\n",
       " 'I',\n",
       " 'choose',\n",
       " 'a',\n",
       " 'modeling',\n",
       " 'agency',\n",
       " 'And',\n",
       " 'I',\n",
       " 'go',\n",
       " 'into',\n",
       " 'New',\n",
       " 'York',\n",
       " 'with',\n",
       " 'these',\n",
       " 'pictures',\n",
       " 'This',\n",
       " 'fellow',\n",
       " 'takes',\n",
       " 'them',\n",
       " 'He',\n",
       " 'says',\n",
       " 'Can',\n",
       " 'I',\n",
       " 'hold',\n",
       " 'these',\n",
       " 'overnight',\n",
       " 'I',\n",
       " 'said',\n",
       " 'yes',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'the',\n",
       " 'next',\n",
       " 'morning',\n",
       " 'he',\n",
       " 'calls',\n",
       " 'me',\n",
       " 'Come',\n",
       " 'into',\n",
       " 'New',\n",
       " 'York',\n",
       " 'we',\n",
       " 'want',\n",
       " 'to',\n",
       " 'sign',\n",
       " 'you',\n",
       " 'to',\n",
       " 'a',\n",
       " 'contract',\n",
       " 'Hed',\n",
       " 'sent',\n",
       " 'my',\n",
       " 'pictures',\n",
       " 'to',\n",
       " 'the',\n",
       " 'photographer',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'who',\n",
       " 'does',\n",
       " 'the',\n",
       " 'Abercormbie',\n",
       " 'and',\n",
       " 'Fitch',\n",
       " 'and',\n",
       " 'Vanity',\n",
       " 'Fair',\n",
       " 'However',\n",
       " 'you',\n",
       " 'say',\n",
       " 'Abercrombie',\n",
       " 'whatever',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'that',\n",
       " 'this',\n",
       " 'fellow',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'photograph',\n",
       " 'me',\n",
       " 'So',\n",
       " 'suddenly',\n",
       " 'with',\n",
       " 'this',\n",
       " 'small',\n",
       " 'agency',\n",
       " 'which',\n",
       " 'existed',\n",
       " 'for',\n",
       " 'about',\n",
       " 'a',\n",
       " 'year',\n",
       " 'and',\n",
       " 'then',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'scandal',\n",
       " 'with',\n",
       " 'young',\n",
       " 'female',\n",
       " 'models',\n",
       " 'from',\n",
       " 'Minnesota',\n",
       " 'closed',\n",
       " 'down',\n",
       " 'But',\n",
       " 'so',\n",
       " 'suddenly',\n",
       " 'Im',\n",
       " 'a',\n",
       " 'model',\n",
       " 'and',\n",
       " 'Im',\n",
       " 'going',\n",
       " 'off',\n",
       " 'to',\n",
       " 'this',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'And',\n",
       " 'I',\n",
       " 'remember',\n",
       " 'we',\n",
       " 'met',\n",
       " 'early',\n",
       " 'in',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'us',\n",
       " 'a',\n",
       " 'bunch',\n",
       " 'of',\n",
       " 'male',\n",
       " 'models',\n",
       " 'and',\n",
       " 'a',\n",
       " 'van',\n",
       " 'And',\n",
       " 'as',\n",
       " 'were',\n",
       " 'going',\n",
       " 'for',\n",
       " 'to',\n",
       " 'uh',\n",
       " 'the',\n",
       " 'Hamptons',\n",
       " 'for',\n",
       " 'this',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'its',\n",
       " 'like',\n",
       " 'six',\n",
       " 'am',\n",
       " 'this',\n",
       " 'one',\n",
       " 'guy',\n",
       " 'looks',\n",
       " 'around',\n",
       " 'hes',\n",
       " 'from',\n",
       " 'Texas',\n",
       " 'he',\n",
       " 'goes',\n",
       " 'Hey',\n",
       " 'were',\n",
       " 'all',\n",
       " 'blond',\n",
       " 'you',\n",
       " 'know',\n",
       " 'just',\n",
       " 'it',\n",
       " 'dawned',\n",
       " 'on',\n",
       " 'him',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'uh',\n",
       " 'as',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'interesting',\n",
       " 'And',\n",
       " 'so',\n",
       " 'we',\n",
       " 'get',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'Hamptons',\n",
       " 'and',\n",
       " 'were',\n",
       " 'at',\n",
       " 'this',\n",
       " 'farmhouse',\n",
       " 'and',\n",
       " 'it',\n",
       " 'was',\n",
       " 'like',\n",
       " 'a',\n",
       " 'scene',\n",
       " 'out',\n",
       " 'of',\n",
       " 'Christopher',\n",
       " 'Isherwood',\n",
       " 'The',\n",
       " 'Berlin',\n",
       " 'Stories',\n",
       " 'All',\n",
       " 'these',\n",
       " 'blond',\n",
       " 'boys',\n",
       " 'about',\n",
       " 'ten',\n",
       " 'of',\n",
       " 'us',\n",
       " 'running',\n",
       " 'around',\n",
       " 'doing',\n",
       " 'pushups',\n",
       " 'so',\n",
       " 'that',\n",
       " 'our',\n",
       " 'muscles',\n",
       " 'would',\n",
       " 'swell',\n",
       " 'and',\n",
       " 'in',\n",
       " 'and',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pool',\n",
       " 'and',\n",
       " 'a',\n",
       " 'big',\n",
       " 'buffet',\n",
       " 'and',\n",
       " 'everything',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'the',\n",
       " 'light',\n",
       " 'to',\n",
       " 'change',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'so',\n",
       " 'I',\n",
       " 'get',\n",
       " 'my',\n",
       " 'picture',\n",
       " 'taken',\n",
       " 'by',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'Wonderful',\n",
       " 'guy',\n",
       " 'very',\n",
       " 'talented',\n",
       " 'obviously',\n",
       " 'And',\n",
       " 'he',\n",
       " 'wanted',\n",
       " 'me',\n",
       " 'to',\n",
       " 'uh',\n",
       " 'drop',\n",
       " 'my',\n",
       " 'shorts',\n",
       " 'and',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'embarrassed',\n",
       " 'I',\n",
       " 'didnt',\n",
       " 'think',\n",
       " 'I',\n",
       " 'could',\n",
       " 'do',\n",
       " 'that',\n",
       " 'And',\n",
       " 'so',\n",
       " 'I',\n",
       " 'anyways',\n",
       " 'I',\n",
       " 'hid',\n",
       " 'everything',\n",
       " 'I',\n",
       " 'I',\n",
       " 'wasnt',\n",
       " 'able',\n",
       " 'to',\n",
       " 'expose',\n",
       " 'myself',\n",
       " 'So',\n",
       " 'anyways',\n",
       " 'he',\n",
       " 'took',\n",
       " 'these',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'I',\n",
       " 'have',\n",
       " 'one',\n",
       " 'and',\n",
       " 'Ill',\n",
       " 'uh',\n",
       " 'Im',\n",
       " 'going',\n",
       " 'to',\n",
       " 'pass',\n",
       " 'it',\n",
       " 'out',\n",
       " 'as',\n",
       " 'a',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'gift',\n",
       " 'for',\n",
       " 'you',\n",
       " 'Uh',\n",
       " 'I',\n",
       " 'only',\n",
       " 'have',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'of',\n",
       " 'them',\n",
       " 'so',\n",
       " 'if',\n",
       " 'one',\n",
       " 'per',\n",
       " 'table',\n",
       " 'But',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'Bruce',\n",
       " 'Weber',\n",
       " 'picture',\n",
       " 'of',\n",
       " 'me',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'I',\n",
       " 'was',\n",
       " 'the',\n",
       " 'uh',\n",
       " 'anyway',\n",
       " 'the',\n",
       " 'physique',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'good',\n",
       " 'then',\n",
       " 'and',\n",
       " 'you',\n",
       " 'could',\n",
       " 'see',\n",
       " 'I',\n",
       " 'had',\n",
       " 'hair',\n",
       " 'Its',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'depressing',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'that',\n",
       " 'now',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'what',\n",
       " 'happened',\n",
       " 'but',\n",
       " 'uh',\n",
       " 'anyway',\n",
       " 'So',\n",
       " 'I',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Whitney',\n",
       " 'Biennial',\n",
       " 'I',\n",
       " 'like',\n",
       " 'to',\n",
       " 'say',\n",
       " 'to',\n",
       " 'my',\n",
       " 'artist',\n",
       " 'friends',\n",
       " 'uh',\n",
       " 'Ive',\n",
       " 'been',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Biennial',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'in',\n",
       " 'this',\n",
       " 'picture',\n",
       " 'And',\n",
       " 'uh',\n",
       " 'in',\n",
       " 'the',\n",
       " 'back',\n",
       " 'of',\n",
       " 'this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'thing',\n",
       " 'from',\n",
       " 'my',\n",
       " 'uh',\n",
       " 'nursery',\n",
       " 'school',\n",
       " 'just',\n",
       " 'so',\n",
       " 'you',\n",
       " 'can',\n",
       " 'study',\n",
       " 'that',\n",
       " 'if',\n",
       " 'you',\n",
       " 'like',\n",
       " 'So',\n",
       " 'ns',\n",
       " 'so',\n",
       " 'pass',\n",
       " 'those',\n",
       " 'around',\n",
       " 'so',\n",
       " 'maybe',\n",
       " 'one',\n",
       " 'per',\n",
       " 'table',\n",
       " 'since',\n",
       " 'theres',\n",
       " 'only',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'But',\n",
       " 'so',\n",
       " 'it',\n",
       " 'can',\n",
       " 'get',\n",
       " 'to',\n",
       " 'the',\n",
       " 'back',\n",
       " 'Alright',\n",
       " 'so',\n",
       " 'here',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'model',\n",
       " 'and',\n",
       " 'I',\n",
       " 'got',\n",
       " 'several',\n",
       " 'jobs',\n",
       " 'And',\n",
       " 'but',\n",
       " 'at',\n",
       " 'that',\n",
       " 'photo',\n",
       " 'shoot',\n",
       " 'at',\n",
       " 'at',\n",
       " 'Bruce',\n",
       " 'Webers',\n",
       " 'farmhouse',\n",
       " 'I',\n",
       " 'met',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pretty',\n",
       " 'girl',\n",
       " 'She',\n",
       " 'was',\n",
       " 'his',\n",
       " 'assistant',\n",
       " 'and',\n",
       " 'she',\n",
       " 'gave',\n",
       " 'me',\n",
       " 'her',\n",
       " 'phone',\n",
       " 'number',\n",
       " 'So',\n",
       " 'about',\n",
       " 'a',\n",
       " 'week',\n",
       " 'later',\n",
       " 'Im',\n",
       " 'sp',\n",
       " 'doing',\n",
       " 'my',\n",
       " 'modeling',\n",
       " 'appointments',\n",
       " 'which',\n",
       " 'are',\n",
       " 'called',\n",
       " 'gosees',\n",
       " 'which',\n",
       " 'was',\n",
       " 'made',\n",
       " 'very',\n",
       " 'simple',\n",
       " 'for',\n",
       " 'the',\n",
       " 'models',\n",
       " 'It',\n",
       " 'was',\n",
       " 'go',\n",
       " 'see',\n",
       " 'someone',\n",
       " 'And',\n",
       " 'so',\n",
       " 'this',\n",
       " 'was',\n",
       " 'what',\n",
       " 'they',\n",
       " 'called',\n",
       " 'appoi',\n",
       " 'uh',\n",
       " 'appointments',\n",
       " 'were',\n",
       " 'gosees',\n",
       " 'So',\n",
       " 'Im',\n",
       " 'on',\n",
       " 'a',\n",
       " 'day',\n",
       " 'of',\n",
       " 'gosees',\n",
       " 'and',\n",
       " 'I',\n",
       " 'found',\n",
       " 'myself',\n",
       " 'in',\n",
       " 'the',\n",
       " 'West',\n",
       " 'Village',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'right',\n",
       " 'by',\n",
       " 'Cornelia',\n",
       " 'Street',\n",
       " 'And',\n",
       " 'I',\n",
       " 'remembered',\n",
       " 'that',\n",
       " 'that',\n",
       " 'girl',\n",
       " 'lived',\n",
       " 'on',\n",
       " 'Cornelia',\n",
       " 'Street',\n",
       " 'And',\n",
       " 'I',\n",
       " 'my',\n",
       " 'high',\n",
       " 'school',\n",
       " 'love',\n",
       " 'was',\n",
       " 'a',\n",
       " 'girl',\n",
       " 'named',\n",
       " 'Cornelia',\n",
       " 'And',\n",
       " 'so',\n",
       " 'suddenly',\n",
       " 'it',\n",
       " 'was',\n",
       " 'all',\n",
       " 'fusing',\n",
       " 'in',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'So',\n",
       " 'I',\n",
       " 'call',\n",
       " 'her',\n",
       " 'and',\n",
       " 'uh',\n",
       " 'I',\n",
       " 'get',\n",
       " 'her',\n",
       " 'roommate',\n",
       " 'and',\n",
       " 'I',\n",
       " 'explain',\n",
       " 'that',\n",
       " 'Im',\n",
       " 'in',\n",
       " 'the',\n",
       " 'neighborhood',\n",
       " 'and',\n",
       " 'is',\n",
       " 'I',\n",
       " 'wont',\n",
       " ...]"
      ]
     },
     "execution_count": 1591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_transcribed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1589,
   "id": "fd5e007c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1589], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m tg_words \u001b[38;5;241m=\u001b[39m textgrid\u001b[38;5;241m.\u001b[39mgetTier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m contents, words_transcribed \u001b[38;5;241m=\u001b[39m load_transcription(transcript_fn)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(tg_words) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(words_transcribed))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# create the dictionary to store things in\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# put the transcript in the raw form\u001b[39;00m\n\u001b[1;32m     10\u001b[0m align \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "textgrid = load_clean_textgrid(praat_fn)\n",
    "tg_words = textgrid.getTier('word')\n",
    "\n",
    "contents, words_transcribed = load_transcription(transcript_fn)\n",
    "\n",
    "for tg_word, transcribed_word \n",
    "\n",
    "assert (len(tg_words) == len(words_transcribed))\n",
    "\n",
    "# create the dictionary to store things in\n",
    "# put the transcript in the raw form\n",
    "align = {}\n",
    "align['transcript'] = contents[0]\n",
    "align['words'] = []\n",
    "\n",
    "# Taken from Kaldi metasentence tokenizer\n",
    "# splits the transcript based on any punctuation besides for apostrophes and hyphens\n",
    "regex_split_pattern = r'(\\w|\\.\\w|\\:\\w|\\â€™\\w|\\'\\w|\\-\\w)+'\n",
    "\n",
    "iterator = list(re.finditer(regex_split_pattern, ''.join(contents), re.UNICODE))\n",
    "n_items = len(list(iterator))\n",
    "# make sure the iterator matches the length\n",
    "# assert (n_items == len(tg_words) == len(words_transcribed))\n",
    "\n",
    "\n",
    "## this block helps find what words are wrong\n",
    "for word_info, m in zip(tg_words, iterator):\n",
    "    \n",
    "    if word_info[-1].lower() != m.group().lower():\n",
    "        print (m)\n",
    "    \n",
    "sys.exit(0)\n",
    "\n",
    "# # if all matches we're good to go\n",
    "# for word_info, m in zip(tg_words, iterator):\n",
    "    \n",
    "#     # span of the word in characters relative to the overall string\n",
    "#     start_offset, end_offset = m.span()\n",
    "#     word = m.group()\n",
    "    \n",
    "#     # crop textgrid to the word\n",
    "#     cropped_grid = textgrid.crop(cropStart=word_info[0], cropEnd=word_info[1], mode='truncated', rebaseToZero=False)\n",
    "#     tg_phones = cropped_grid.getTier('phone').entries\n",
    "#     word_phones = []\n",
    "    \n",
    "#     for phone_info in tg_phones:\n",
    "#         phone = re.sub(r'\\d+', '', phone_info[-1])\n",
    "#         duration = phone_info[1] - phone_info[0]\n",
    "#         word_phones.append({'duration': duration, 'phone': phone})\n",
    "    \n",
    "#     word_align = {\n",
    "#         'alignedWord': word.lower(),\n",
    "#         \"case\": \"success\",\n",
    "#         'word': word,\n",
    "#         'start': word_info[0],\n",
    "#         'end': word_info[1],\n",
    "#         'phones': word_phones,\n",
    "#         \"startOffset\": start_offset,\n",
    "#         \"endOffset\": end_offset,\n",
    "#     }\n",
    "\n",
    "#     align['words'].append(word_align)\n",
    "\n",
    "# return align"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dark_matter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
