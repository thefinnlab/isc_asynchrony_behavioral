_target_: src.models.careful_whisper_module.CarefulWhisperModule

# Set config up as GPT2-esque default
config:
  _target_: src.models.components.careful_whisper.CarefulWhisperConfig
  n_vocab: 50257 # GPT2 Vocab size
  max_length: 128 # Number of tokens
  embed_dim: 1024 # Number of embedding dim 
  num_heads: 16
  num_layers: 12
  pad_token_id: 50256 # Padding in GPT2 space
  cross_attention: False
  use_causal_cross_attention: False
  use_text_control: False

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 0.001
  weight_decay: 0.0
  
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.1
  patience: 2