# Human-LLM behavioral comparison

This repository contains the scripts for the paper ["Sensory context as a universal principle of language in humans and LLMs"](https://osf.io/preprints/psyarxiv/nz5eg_v1?view_only=). Within this repository, one can find the scripts to run a next-word prediction behavioral experiment, train multimodal LLMs, and analyze and plot the resulting data.

## Directory structure

The following directory structure is used to organize the repository and interact with the code.

```

code
├── analysis: scripts used to analyze behavioral & LLM training data
├── modeling
│   ├── careful-whisper: training multimodal LLMs
│   ├── prediction-extraction: extracting predictions from MLMs and CLMs (behavioral comparison)
│   ├── preproc-datasets: preprocessing audiovisual datasets for LLM training
├── plot: generating plots from analyzed data 
├── preprocess: preparing data for behavioral experiment
│   ├── 01_word-selection: training multimodal LLMs
│   ├── 02_jspsych-creation: extracting predictions from MLMs and CLMs (behavioral comparison)
├── stats: generating plots from analyzed data 
└── submit_scripts: dynamically generated scripts for slurm with dSQ
└── utils: functions shared across scripts

derivatives: files generated by scripts (e.g., behavioral data, plots) 
envs: python environment .yml files
experiments: online experiment code for collecting human next-word predictions
stimuli: stimulus files used in behavioral experiments (e.g., transcripts, audio, video)

```

## Setup

### Conda environments

There are three conda environments needed for running scripts:
- **analysis:** used for running scripts in the preprocess, analysis, and plot directories. Also used for scripts within modeling/prediction-extraction
- **modeling:** used for LLM model training and dataset preparation -- remaining files within modeling directory
- **mfa:** specifically used for running the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/v3.0.7/index.html)

These environments can be installed with the command ```conda env create -n ENV_NAME -f YML_FILE```.

### Paths

You will need to edit the variable names related to paths within ```code/utils/config.py```:
- **BASE_DIR:** the path to the project directory (can be obtained by running ```pwd``` in the top-level directory)
- **CACHE_DIR**: where models downloaded from the transformers library will be saved
- **DATASETS_DIR:** location of datasets used within LLM modeling 
- **SCRATCH_DIR**: where to write temporary files

### Downloading OSF data

We upload all stimuli and data to OSF. 

## Behavioral experiment

To set up data for the behavioral experiment, we first extract predictions for all specified LLMs. The full list of LLMs possible to test are found starting on line 121 of ```code/utils/nlp_utils.py```. 



Scripts related to setting up the behavioral experiment are contained within the preprocess/ directory. 

## LLM training

### Preparing datasets

All scripts related to LLM training are found within the ```code/modeling/``` directory. To get started, activate the conda environment for modeling and move to ```preproc-datasets/download/```. Downloading a dataset requires going to the directory for the dataset name (AVSpeech, VoxCeleb2, LRS3) and running the script ```download_DATASET.py```.

Each audiovisual (AV) dataset will then need specific preprocessing steps found within ```preproc-datasets/av/```. Specifically, both AVSpeech and VoxCeleb2 require running the following scripts 1) ```extract_dataset_audio.py``` and 2) ```transcribe_audio.py```. Then run the script ```prepare_corpus.py``` for all datasets (including LRS3). Together, this will set up the directory structure required for using the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/v3.0.7/index.html) and [Wavelet Prosody Toolkit](https://github.com/asuni/wavelet_prosody_toolkit).

After running preprocessing, follow these steps to prepare the datasets for LLM training:
1) Run the Montreal Forced Aligner 

## Analysis

## Preparing 

## Alignment Cleaning Instructions

Below are general steps involved in preprocessing gentle aligned transcripts for next word/sentence/phrasal prediction experiments. While the specifics will vary by experiment, the order of scripts run will be the same.

All these steps assume that a gentle forced-aligned transcript exists for a given stimulus. 

### Setup

Follow these steps to perform setup of the alignment cleaning scripts:
1. Open a terminal window at the directory `alignment_cleaning` 
2. Set up a conda environment by running `conda env create --name alignment_cleaning --file=environment.yml`
3. Activate the conda environment by typing `conda activate alignment_cleaning`
4. Open the `config.py` located in `/code/utils/` directory
5. Change the base directory to the following `/PATHTOFOLDER/alignment_cleaning/` --> to find what the path is type `pwd`

### Creating the prediction candidates file 

The following steps describe how to create candidate words for prediction:

1. Run `select_next_word_candidates.py` with the name of a task (any of the transcript names)
	- To test this out, run the command `python select_next_word_candidates.py black`
	- This uses python to run the script `select_next_word_candidates` on the task `black`
2. Clean hyphenated words:
	- Sometimes the transcription makes mistakes in which words should be hyphenated. An example of a hyphenated word is "pre-recorded" while an example of a mistake is "am-all".
	- Select words meant to be hyphenated by typing `y` for yes
	- Flag words not meant to be hyphenated by typing `n` for no
3. Clean named entities:
	- A named entity is a real-world object (e.g., person, location, organization, product). As these words are likely not known by participants and are specific to the current transcript we remove them as possibilities for prediction. Some examples of named entities are would be "Tommy Botch", "Dartmouth College", "Hanover", "New Hampshire"
	- Select words that are named entities by typing `y` for yes
	- Flag words not meant to be hyphenated by typing `n` for no

After completing these steps, the following directory should be created `/stimuli/preprocessed/TASK/` with files required to run the audio segmentations.

### Segmenting audio and ensuring no word leakage

Next, we check all words meant to be predicted for leakage (e.g., the ability to hear the word or part of the word in advance of it being said). The following steps may require multiple iterations.

1. Run `segment_audio.py` with the name of a task (any of the transcript names)
	- To test this out, run the command `python segment_audio.py TASK` where task is the of a task (e.g., black)
	- This script will output a file named `TASK_transcript-segments.csv`
2. Open files for editing:
	- The directory `/stimuli/cut_audio/TASK/`
	- The segments file `TASK_transcript-segments.csv`
	- Within Praat open:
		- The task audio --> found as filename `/stimuli/audio/TASK_audio.wav`
		- The TextGrid --> found as filename `/stimuli/preprocessed/TASK/black_transcript-praat.TextGrid`
3. Go through each of the cut audio files, following along in Praat:
	- If the cut audio file has leakage:
		1. Adjust the audio timings in Praat
		2. Within `TASK_transcript-segments.csv`, place a `1` in the columns `checked` and `adjusted`
	- If the cut audio file does not have leakage:
		1. Within `TASK_transcript-segments.csv`, place a `1` in the columns `checked`

After checking all the cut audio files, rerun `segment_audio.py`. This will adjust the audio times. You will then need to repeat the following steps for the files that had the audio times changed.