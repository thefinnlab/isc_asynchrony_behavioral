# Human-LLM behavioral comparison

This repository contains the scripts for the paper ["Sensory context as a universal principle of language in humans and LLMs"](https://osf.io/preprints/psyarxiv/nz5eg_v1?view_only=). Within this repository, one can find the scripts to run a next-word prediction behavioral experiment, train multimodal LLMs, and analyze and plot the resulting data.

All software was implemented and run on a Red Hat Enterprise Linux release 8.10 (Ootpa) compute cluster. Environments were created with conda 24.9.1 and take ~5-10 minutes to install on the compute cluster. The scripts here were run with parallelization both within-script and at the job level on our HPC cluster using dead-simple queue.

## Directory structure

The following directory structure is used to organize the repository and interact with the code.

```

code/
├── analysis/: scripts used to analyze behavioral & LLM training data
├── modeling/
│   ├── careful-whisper/: training multimodal LLMs
│   ├── prediction-extraction/: extracting predictions from MLMs and CLMs (behavioral comparison)
│   ├── preproc-datasets/: preprocessing audiovisual datasets for LLM training
├── plot/: generating plots from analyzed data 
├── preprocess/: preparing data for behavioral experiment
│   ├── 01_word-selection/: training multimodal LLMs
│   ├── 02_jspsych-creation/: extracting predictions from MLMs and CLMs (behavioral comparison)
├── stats/: generating plots from analyzed data 
└── submit_scripts/: dynamically generated scripts for slurm with dSQ
└── utils/: functions shared across scripts

derivatives/: files generated by scripts (e.g., behavioral data, plots) 
envs/: python environment .yml files
experiments/: online experiment code for collecting human next-word predictions
stimuli/: stimulus files used in behavioral experiments (e.g., transcripts, audio, video)
data/: raw / cleaned human behavioral data sourced by analysis scripts

```

## Setup

### Download data and stimuli

All data associated with this repository can be found on OSF. Download the ```.zip``` file to this directory before running any scripts. There will be three folders that will appear upon expanding the file: 1) ```stimuli```, 2) ```data```, and 3) ```derivatives/results/```. Together, this gives access to the original stimuli used, the raw and preprocessed human behavioral data, and the compiled, analyzed results from the behavioral experiment and LLM training.

### Conda environments

There are three conda environments needed for running scripts:
- **analysis:** For preprocessing, analysis, and plotting.
- **modeling:** For LLM training and dataset preparation.
- **mfa:** For running the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/v3.0.7/index.html).

Install environments using: ```conda env create -n ENV_NAME -f YML_FILE```

### Paths

Update paths in ```code/utils/config.py```:
- **BASE_DIR:** Path to the project directory.
- **CACHE_DIR**: Directory for downloaded models.
- **DATASETS_DIR:** Location of datasets.
- **SCRATCH_DIR**: Directory for temporary files.

## Behavioral experiment

To set up data for the behavioral experiment, we first extract predictions for all specified LLMs. The full list of LLMs possible to test are found starting on line 121 of ```code/utils/nlp_utils.py```. 

Scripts related to setting up the behavioral experiment are contained within the ```preprocess/``` directory and run in the following order:
1) ```select_next_word_candidates.py```: cleaning of gentle transcripts, creating candidates for next-word prediction
2) ```batch_cut_participant_stimuli.py```: crops stimuli (text/audio/video) into separate files for each subject
3) ```make_experiment_orders.py```: creates a file to be used in the HTML code

An experiment can be run by 

## LLM training 

### Preparing audiovisual datasets

All scripts related to LLM training are found within the ```code/modeling/``` directory. To get started, activate the conda environment for modeling and move to ```preproc-datasets/download/```. Downloading a dataset requires going to the directory for the dataset name (AVSpeech, VoxCeleb2, LRS3) and running the script ```download_DATASET.py```.

Each audiovisual (AV) dataset will then need specific preprocessing steps found within ```preproc-datasets/av/```. Specifically, both AVSpeech and VoxCeleb2 require running the following scripts 1) ```extract_dataset_audio.py``` and 2) ```transcribe_audio.py```. Then run the script ```prepare_corpus.py``` for all datasets (including LRS3). Together, this will set up the directory structure required for using the [Montreal Forced Aligner](https://montreal-forced-aligner.readthedocs.io/en/v3.0.7/index.html) and [Wavelet Prosody Toolkit](https://github.com/asuni/wavelet_prosody_toolkit).

After running preprocessing, follow these steps to prepare the datasets for LLM training:
1) Create textgrid files with Montreal Forced Aligner via ```mfa/mfa_align.py```
2) Extract prosody with Wavelet Prosody Toolkit via ```prosody/batch_run_prosody.py```
3) Extract features for all videos in a dataset via ```feature-extraction/extract_dataset_features.py```

Once finished, create a symlink to the complete dataset within ```careful-whisper/``` by running ```ln -s DATASET_PATH ./careful-whisper/data/``` where DATASET_PATH is the output location of the preprocessed dataset.

### Training models

Scripts related to training models are located in ```code/modeling/careful-whisper/```. Model configurations are specified in the ```configs/``` directory. Use the file ```scripts/batch_careful-whisper_experiments.py``` to setup and train a given model (i.e., text, prosody, audio, audiovisual).

<!-- ## Analysis  -->